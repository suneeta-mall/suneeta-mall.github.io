<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href="https://suneeta-mall.github.io/blog/archive/2021/" rel="canonical"><link href=../2022/ rel=prev><link href=../2019/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../feed_rss_updated.xml><link rel=icon href=../../../resources/site/favicon.svg><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.18"><title>2021 - Random Musings - Rambling of a curious engineer & data scientist!</title><link rel=stylesheet href=../../../assets/stylesheets/main.66ac8b77.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel="stylesheet" href="../../../assets/external/fonts.googleapis.com/css.49ea35f2.css"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><meta property=og:type content=website><meta property=og:title content="2021 - Random Musings - Rambling of a curious engineer & data scientist!"><meta property=og:description content=None><meta property=og:image content=https://suneeta-mall.github.io/assets/images/social/blog/archive/2021.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://suneeta-mall.github.io/blog/archive/2021/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="2021 - Random Musings - Rambling of a curious engineer & data scientist!"><meta name=twitter:description content=None><meta name=twitter:image content=https://suneeta-mall.github.io/assets/images/social/blog/archive/2021.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#2021 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> <a href=/projects/oreilly_deep_learning_at_scale/ > <strong>ðŸŽ‰ New Book Release!</strong> Check out "Deep Learning at Scale" - An O'Reilly Book </a> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href="https://suneeta-mall.github.io/" title="Random Musings - Rambling of a curious engineer &amp; data scientist!" class="md-header__button md-logo" aria-label="Random Musings - Rambling of a curious engineer &amp; data scientist!" data-md-component="logo"> <img src=../../../resources/site/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Random Musings - Rambling of a curious engineer & data scientist! </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 2021 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3m0-4.5c5 0 9.27 3.11 11 7.5-1.73 4.39-6 7.5-11 7.5S2.73 16.39 1 12c1.73-4.39 6-7.5 11-7.5M3.18 12a9.821 9.821 0 0 0 17.64 0 9.821 9.821 0 0 0-17.64 0Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../projects/oreilly_deep_learning_at_scale/ class=md-tabs__link> Projects </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../../../tags/ class=md-tabs__link> Tags </a> </li> <li class=md-tabs__item> <a href=../../../talks/KGC_NY_2022/ class=md-tabs__link> Talks </a> </li> <li class=md-tabs__item> <a href=../../../poems/singularity/ class=md-tabs__link> Poems </a> </li> <li class=md-tabs__item> <a href=../../../about/ class=md-tabs__link> About Me </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href="https://suneeta-mall.github.io/" title="Random Musings - Rambling of a curious engineer &amp; data scientist!" class="md-nav__button md-logo" aria-label="Random Musings - Rambling of a curious engineer &amp; data scientist!" data-md-component="logo"> <img src=../../../resources/site/logo.svg alt=logo> </a> Random Musings - Rambling of a curious engineer & data scientist! </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../projects/oreilly_deep_learning_at_scale/ class=md-nav__link> <span class=md-ellipsis> Deep Learning at Scale </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/curious_cassie/ class=md-nav__link> <span class=md-ellipsis> Curious Cassie - The Children's Books </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/feature_analysis/ class=md-nav__link> <span class=md-ellipsis> Label Noise with Clean Lab </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/feature_analysis/ class=md-nav__link> <span class=md-ellipsis> Feature Analysis </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/oreilly-interactive-katacode-series-for-reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Oreilly Katacode Series </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Reproducible-ML </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/KCD/ class=md-nav__link> <span class=md-ellipsis> KCD </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2 checked> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=true> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/ai/ class=md-nav__link> <span class=md-ellipsis> AI </span> </a> </li> <li class=md-nav__item> <a href=../../category/book/ class=md-nav__link> <span class=md-ellipsis> Book </span> </a> </li> <li class=md-nav__item> <a href=../../category/childrens-books/ class=md-nav__link> <span class=md-ellipsis> Children's Books </span> </a> </li> <li class=md-nav__item> <a href=../../category/confident-learning/ class=md-nav__link> <span class=md-ellipsis> Confident-Learning </span> </a> </li> <li class=md-nav__item> <a href=../../category/curious-cassie/ class=md-nav__link> <span class=md-ellipsis> Curious Cassie </span> </a> </li> <li class=md-nav__item> <a href=../../category/data/ class=md-nav__link> <span class=md-ellipsis> Data </span> </a> </li> <li class=md-nav__item> <a href=../../category/data-centric-ai/ class=md-nav__link> <span class=md-ellipsis> Data-Centric-AI </span> </a> </li> <li class=md-nav__item> <a href=../../category/data-science/ class=md-nav__link> <span class=md-ellipsis> Data-science </span> </a> </li> <li class=md-nav__item> <a href=../../category/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../../category/generative-ai/ class=md-nav__link> <span class=md-ellipsis> Generative AI </span> </a> </li> <li class=md-nav__item> <a href=../../category/kubernetes/ class=md-nav__link> <span class=md-ellipsis> Kubernetes </span> </a> </li> <li class=md-nav__item> <a href=../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../category/machine-learning/ class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=../../category/oom/ class=md-nav__link> <span class=md-ellipsis> OOM </span> </a> </li> <li class=md-nav__item> <a href=../../category/pytorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../category/reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Reproducible-ml </span> </a> </li> <li class=md-nav__item> <a href=../../category/software/ class=md-nav__link> <span class=md-ellipsis> Software </span> </a> </li> <li class=md-nav__item> <a href=../../category/technology/ class=md-nav__link> <span class=md-ellipsis> Technology </span> </a> </li> <li class=md-nav__item> <a href=../../category/umap/ class=md-nav__link> <span class=md-ellipsis> UMAP </span> </a> </li> <li class=md-nav__item> <a href=../../category/t-sne/ class=md-nav__link> <span class=md-ellipsis> t-SNE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Talks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Talks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../talks/KGC_NY_2022/ class=md-nav__link> <span class=md-ellipsis> Knowledge Graph Conference 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubeCon_NA_2021/ class=md-nav__link> <span class=md-ellipsis> KubeCon NA 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/Kafka_Summit_APAC_2021/ class=md-nav__link> <span class=md-ellipsis> Kafka Summit APAC 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/AWS_ANZ_Commuity_day_2020/ class=md-nav__link> <span class=md-ellipsis> AWS Community Day 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/She_Builds_on_AWS_2020/ class=md-nav__link> <span class=md-ellipsis> AWS She Builds on AWS 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubeCon_US_2019/ class=md-nav__link> <span class=md-ellipsis> KubeCon US 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubernetesSydneyForum_AU_2019/ class=md-nav__link> <span class=md-ellipsis> Kubernetes Sydney 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/YOW_Data_Syd_2019/ class=md-nav__link> <span class=md-ellipsis> YOW Data 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubeCon-Europe-2018/ class=md-nav__link> <span class=md-ellipsis> KubeCon EU 2018 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/SPIE-2019/ class=md-nav__link> <span class=md-ellipsis> SPIE 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/SPIE-2018/ class=md-nav__link> <span class=md-ellipsis> SPIE 2018 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/SPIE-2015/ class=md-nav__link> <span class=md-ellipsis> SPIE 2015 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Poems </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Poems </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../poems/singularity/ class=md-nav__link> <span class=md-ellipsis> Singularity </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/life-of-ai-engineer/ class=md-nav__link> <span class=md-ellipsis> Life of AI Engineers </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/my-little-butterfly/ class=md-nav__link> <span class=md-ellipsis> My little Butterfly </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/breaking-thy-bias/ class=md-nav__link> <span class=md-ellipsis> Breaking Thy Bias </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/daminis/ class=md-nav__link> <span class=md-ellipsis> Daminis </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/one-bright-dawn/ class=md-nav__link> <span class=md-ellipsis> One Bright Dawn </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/aint-no-dr-seuss/ class=md-nav__link> <span class=md-ellipsis> Aint no Dr. Seuss </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=2021>2021<a class=headerlink href=#2021 title="Permanent link">#</a></h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2021-12-31 00:00:00">December 31, 2021</time></li> <li class=md-meta__item> in <a href=../../category/machine-learning/ class=md-meta__link>Machine Learning</a>, <a href=../../category/ai/ class=md-meta__link>AI</a>, <a href=../../category/deep-learning/ class=md-meta__link>Deep Learning</a>, <a href=../../category/data/ class=md-meta__link>Data</a>, <a href=../../category/data-centric-ai/ class=md-meta__link>Data-Centric-AI</a></li> <li class=md-meta__item> 27 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=review-of-recent-advances-in-dealing-with-data-size-challenges-in-deep-learning><a href=../../2021/12/31/data-in-deep-learning/ class=toclink>Review of recent advances in dealing with data size challenges in Deep Learning</a></h2> <p>The energy and excitement in machine learning and deep learning communities are infectious these days. So many groundbreaking advances are happening in this area but I have often found myself wondering why the only thing that makes it all shine - yes I am talking about the dark horse of deep learning <em>the</em> <em>data</em> is so underappreciated. The last few years of DL research have given me much joy and excitement and I carry hope now that going forward we can see some exciting progress in this space that explore advances in deep learning in conjunction with data! In this article, I summarise some of the recent developments in the deep learning space that I have been blown away by. </p> <p>Table of content of this article: - <a href=../../2021/12/31/data-in-deep-learning/#review-of-recent-advances-in-dealing-with-data-size-challenges-in-deep-learning>Review of recent advances in dealing with data size challenges in Deep Learning</a> - <a href=../../2021/12/31/data-in-deep-learning/#the-dark-horse-of-deep-learning-data>The dark horse of deep learning: data</a> - <a href=../../2021/12/31/data-in-deep-learning/#labelled-data-the-types-of-labels>Labelled data: the types of labels</a> - <a href=../../2021/12/31/data-in-deep-learning/#commonly-used-dl-techniques-centered-around-data>Commonly used DL techniques centered around data</a> - <a href=../../2021/12/31/data-in-deep-learning/#data-augmentation>Data Augmentation</a> - <a href=../../2021/12/31/data-in-deep-learning/#transfer-learning>Transfer Learning</a> - <a href=../../2021/12/31/data-in-deep-learning/#dimensionality-reduction>Dimensionality reduction</a> - <a href=../../2021/12/31/data-in-deep-learning/#active-learning>Active learning</a> - <a href=../../2021/12/31/data-in-deep-learning/#challenges-in-scaling-dataset-for-deep-learning>Challenges in scaling dataset for deep-learning</a> - <a href=../../2021/12/31/data-in-deep-learning/#recent-advances-in-data-related-techniques>Recent advances in data-related techniques</a> - <a href=../../2021/12/31/data-in-deep-learning/#1-regularization>1. Regularization</a> - <a href=../../2021/12/31/data-in-deep-learning/#11-mixup>1.1 Mixup</a> - <a href=../../2021/12/31/data-in-deep-learning/#12-label-smoothing>1.2 Label Smoothing</a> - <a href=../../2021/12/31/data-in-deep-learning/#2-compression>2. Compression</a> - <a href=../../2021/12/31/data-in-deep-learning/#21-x-shot-learning-how-many-are-enough>2.1. X-shot learning: How many are enough?</a> - <a href=../../2021/12/31/data-in-deep-learning/#22-pruning>2.2. Pruning</a> - <a href=../../2021/12/31/data-in-deep-learning/#221-coresets>2.2.1 Coresets</a> - <a href=../../2021/12/31/data-in-deep-learning/#222-example-forgetting>2.2.2 Example forgetting</a> - <a href=../../2021/12/31/data-in-deep-learning/#223-using-gradient-norms>2.2.3 Using Gradient norms</a> - <a href=../../2021/12/31/data-in-deep-learning/#23-distillation>2.3. Distillation</a> - <a href=../../2021/12/31/data-in-deep-learning/#3-so-what-if-you-have-noisy-data>3. So what if you have noisy data</a> - <a href=../../2021/12/31/data-in-deep-learning/#conclusion>Conclusion</a> - <a href=../../2021/12/31/data-in-deep-learning/#references>References</a></p> <h2 id=the-dark-horse-of-deep-learning-data><a class=toclink href=../../2021/12/31/data-in-deep-learning/#the-dark-horse-of-deep-learning-data>The dark horse of deep learning: data</a></h2> <p>Deep learning (DL) algorithms learn to perform a task by building a (domain) knowledge representation by looking at the training data. An early study of image models (classification and segmentation, year 2017) noted that the performance of the model increases logarithmically as the training dataset increases <a href="https://arxiv.org/abs/1707.02968">1</a>. The belief that increasing training dataset size will continue to increase model performance has been long-held. This has also been supported by another empirical study that validated this belief across machine translation, language modeling, image classification, and speech recognition <a href="https://arxiv.org/abs/1712.00409">2</a> (see figure 1). </p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/dataset_size_scale.jpeg> *Figure 1: Shows relationship between generalization error and dataset size (log scale) <a href="https://arxiv.org/abs/1712.00409">2</a> *</p> </blockquote> <p>So, the bigger dataset is better right? Almost! A theoretical foundation has been laid out in the form of power-law i.e $ \begin{equation} \label{power_law} Îµ(m) \approx Î±m^{Î²_g} \end{equation} $ wherein <em>Îµ</em> is generalization error, <em>m</em> is the number of samples in the training set, <em>Î±</em> is a constant property of the problem/DL task, and <em>Î²<sub>g</sub></em> is the scaling exponent that defines the steepness of the learning curve. Here, <em>Î²<sub>g</sub></em>, the steepness of the curve depicts how well a model can learn from adding more data to the training set <a href="https://arxiv.org/abs/1712.00409">2</a> .(see figure 2) Empirically, <em>Î²<sub>g</sub></em> was found to be between âˆ’0.07 and âˆ’0.35 despite theory suggesting <em>Î²<sub>g</sub></em> to be 0.5 or 1. Nonetheless, the logarithmic relationship holds. As shown in figure 2, the gain eventually tapers in irreducible error. </p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/power-law.jpeg> *Figure 2: Power Law curve showing model trained with a small dataset only as good as random guesses to rapidly getting better as dataset size increases to eventually settling into irreducible error region explained by a combination of factors including imperfect data that cause imperfect generalization <a href="https://arxiv.org/abs/1712.00409">2</a> *</p> </blockquote> <p>This can be attributed to several factors including imperfection in data. The importance of data quality and continually iterating over is touched on in some of the previous talks <a href="https://suneeta-mall.github.io/talks/She_Builds_on_AWS_2020.html">1</a>, <a href="https://suneeta-mall.github.io/talks/AWS_ANZ_Commuity_day_2020.html">2</a>, <a href="https://suneeta-mall.github.io/talks/Kafka_Summit_APAC_2021.html">3</a>. Data quality matters, and so does the data distribution. The better the distribution of the training dataset is, the more generalized the model can be!</p> <hr> <blockquote> <blockquote> <blockquote> <blockquote> <p>Data is certainly the new oil! <a href="https://www.wired.com/story/no-data-is-not-the-new-oil/">3</a></p> </blockquote> </blockquote> </blockquote> </blockquote> <hr> <p>So, can we scale the data size without many grievances? Keep in mind, 61% of AI practicing organizations already find data and data-related challenges as their top challenge <a href="https://pages.run.ai/hubfs/PDFs/2021-State-of-AI-Infrastructure-Survey.pdf">4</a>. If the challenges around procurement, storage, data quality, and distribution/demographic of the dataset has not subsumed you yet, this post focuses on yet another series of questioning. How can we train efficiently when data volume grows and the computation cost and turnaround time increase linearly with data growth? Then we begin asking how much of the data is superfluous, which examples are more impactful, and how do we find them? These are very important questions to ask given a recent survey <a href="https://pages.run.ai/hubfs/PDFs/2021-State-of-AI-Infrastructure-Survey.pdf">4</a> noted that about 40% of the organizations practicing AI already spend at least $1M per annum on GPUs and AI-related compute infrastructures. This should concern us all. Not every organization beyond the FAANG (and also, the one's assumed in FAANG but missed out on acronym!) and ones with big fat balance sheet will be able to leverage the gain by simply scaling the dataset. Besides, this should concern us all for environmental reasons and carbon emissions implications <a href="https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/?sh=55d621d76b43">more details</a>. </p> <blockquote> <p>The carbon footprint of training a single AI is as much as 284 tonnes of carbon dioxide equivalent â€” five times the lifetime emissions of an average car <a href="https://www.newscientist.com/article/2205779-creating-an-ai-can-be-five-times-worse-for-the-planet-than-a-car/">source</a>.</p> </blockquote> <p>The utopian state of simply scaling training datasets and counting your blessings simply does not exist. The question then is, what are we doing about it? Unfortunately not a whole lot especially if you look at the excitement in the ML research community in utilizing gazillion GPU years to gain a minuscule increase in model performance attributed to algorithmic or architectural changes. But the good news is this area is gaining much more traction now. Few pieces of research since 2020 are very promising albeit in their infancy. I have been following the literature around the use of data in AI (a.k.a data-centric AI) topic very closely as this is one of my active areas of interest. I am excited about some of the recent developments in this area. In this post, I will cover some of my learnings and excitement around this topic.</p> <p>Before, covering them in detail, let's review foundational understanding and priors first:</p> <h2 id=labelled-data-the-types-of-labels><a class=toclink href=../../2021/12/31/data-in-deep-learning/#labelled-data-the-types-of-labels>Labelled data: the types of labels</a></h2> <p>This post focuses heavily on supervised learning scenarios focussing mainly on computer vision. In this space, there are two types of labels: - Hard labels - Soft labels</p> <p>Traditional labels are hard labels where the value of ground truth is a discrete value e.g. 0 and 1, 0 for no, and 1 for yes. These discrete values can be anything depending on how the dataset was curated. It's important to note that these values are absolute and unambiguously indicate their meaning.</p> <p>There is an emerging form of labels known as soft labels where ground through represents the likelihood. By nature these labels are continuous. An example, a pixel is 40% cat 60% dog. It will make a whole lot of sense in the following sections.</p> <h2 id=commonly-used-dl-techniques-centered-around-data><a class=toclink href=../../2021/12/31/data-in-deep-learning/#commonly-used-dl-techniques-centered-around-data>Commonly used DL techniques centered around data</a></h2> <p>Data augmentation and transfer learning are two commonly used techniques in deep learning these days that focus on applying data efficiently. Both these techniques are heavily democratized now and commonly applied unless explicitly omitted. </p> <h3 id=data-augmentation><a class=toclink href=../../2021/12/31/data-in-deep-learning/#data-augmentation>Data Augmentation</a></h3> <p>Data augmentation encompasses a variety of techniques to transform a datapoint such that it adds variety to the dataset. The technique aims to keep the data distribution about the same but add richness to the dataset by adding variety. Predominantly, the transformation via this technique has been intra-sample. Affine transformations, contrast adjustment, jittering, or color balancing are some such examples of data augmentation techniques. <a href="https://github.com/aleju/imgaug">Imgaug</a> and <a href="https://github.com/kornia/kornia">kornia</a> are very good libraries for such operation even though all ML frameworks offer a limited set of data augmentation routines. </p> <p>Data augmentation technique was initially proposed to increase robustness and achieve better generalization in the model but they are also used as a technique to synthetically increase data size as well. This is especially true in cases where data procurement is really challenging. These days, data augmentation techniques have become a lot more complex and richer including scenarios where-in model-driven augmentations may also be applied. One example of this is GAN-based techniques to augment and synthesize samples. In fact, data augmentation is also one of the techniques to build robustness against adversarial attacks.</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/imgaug_heavy1.jpg> * Example of augmentation <a href="https://github.com/aleju/imgaug">src</a> *</p> </blockquote> <h3 id=transfer-learning><a class=toclink href=../../2021/12/31/data-in-deep-learning/#transfer-learning>Transfer Learning</a></h3> <p>Transfer learning is a very well democratized technique as well that stems from reusing the learned representations into a new task if the problem domain of two tasks is related. Transfer learning relaxes the assumption that the training data must be independent and identically distributed (i.i.d.) with the test data <a href="https://arxiv.org/abs/1808.01974">5</a>, allowing one to solve the problem of insufficient training data by bootstrapping model weights from another learned model trained with another dataset.</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/transfer_learning.jpeg> *Figure 3: Training with and without transfer learning <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.1515&amp;rep=rep1&amp;type=pdf">6</a> *</p> </blockquote> <p>With transfer learning, faster convergence can be achieved if there is an overlap between the tasks of the source and target model.</p> <h3 id=dimensionality-reduction><a class=toclink href=../../2021/12/31/data-in-deep-learning/#dimensionality-reduction>Dimensionality reduction</a></h3> <p>Dimension reduction techniques are also applied to large datasets:</p> <p>These techniques are categorized into two: 1. Ones that seek to preserve the pairwise distance amongst all the samples in the dataset. Principal component analysis (PCA) is a good example of this. 2. Ones that preserve the local distances over global distance. The techniques like uniform manifold approximation and projection (UMAP) <a href="https://arxiv.org/abs/1802.03426">23</a> and t-distributed stochastic neighbor embedding (t-SNE) <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">24</a> fit in this category. UMAP arguably preserves more of the global structure and is algorithmically faster than t-sne. Both T-SNE and UMAP use gradient descent for arriving at the optimal embeddings.</p> <p>These techniques in DL space however are mostly used to understand the data and also for visualization purposes. UMAP and T-SNE do better at preserving global structure than other embedding algorithms but are limited. <a href="https://towardsdatascience.com/tsne-vs-umap-global-structure-4d8045acba17">This blog</a> covers the topic more in detail.</p> <h3 id=active-learning><a class=toclink href=../../2021/12/31/data-in-deep-learning/#active-learning>Active learning</a></h3> <p>Active learning is a methodology wherein the training process proactively asks for labels on specific data. It is used more commonly in classical ML techniques, but it has not been very successful in deep learning due to back-propagation. Offline or pool-based active learning has been investigated heavily for use in deep learning but without much groundbreaking success. The use of active learning is not very straightforward either due to the negative impact of outliers on training <a href="https://arxiv.org/abs/2107.02331">25</a>. Pool-based active learning will be covered in the following section in more detail (pruning).</p> <h2 id=challenges-in-scaling-dataset-for-deep-learning><a class=toclink href=../../2021/12/31/data-in-deep-learning/#challenges-in-scaling-dataset-for-deep-learning>Challenges in scaling dataset for deep-learning</a></h2> <p>Besides the techniques discussed in <a href=../../2021/12/31/data-in-deep-learning/#commonly-used-dl-techniques-centered-around-data>previous section</a>, not a lot of investment has been done in the area focussing on data-centric AI. The momentum around data-centric AI is forming a bit recently with <a href="https://twitter.com/AndrewYNg">Andrew Ng</a> driving <a href="https://landing.ai/data-centric-ai/">data-centric AI</a> efforts through his new startup <a href="https://landing.ai">Landing.AI</a>. </p> <p>In my view, the following are some of the broad categories of questions that fall under the purview of data-centric AI:</p> <ol> <li>How to efficiently train with the rapid increase in the dataset? <a href="https://twitter.com/ylecun">Yann LeCun</a> called out in his interview with <a href="https://twitter.com/soumithchintala">Soumith Chintala</a> during <a href="https://www.youtube.com/watch?v=vXbbaEZbrOI">PyTorch developer day 2021</a> that training time of more than 1 week should not be acceptable. This is a very good baseline for practical reasons but if one does not have an enormous GPU fleet at their disposal then this is goalpost is very hard to achieve given current DL practices. So, what else can be done to train efficiently with increased dataset size?</li> <li>Are all samples equally important? How important a sample in the dataset is? Can we leverage the "importance factor" for good?</li> <li>What role does a sample play towards better generalization? Some samples carry redundant features, so how to deduplicate the dataset when features as in DL are not explicit?</li> <li>Data size matters but can we be strategic about what goes in the dataset?<ol> <li>Cleverly doing this has to do with efficient sampling and data mining techniques. These are the easily solved problem if and only if we know what our targets are. Challenge in DL, as I see it, is what to look for to mine for the best sample? This is not well understood. </li> </ol> </li> <li>How can we leverage more innate DL techniques like objective functions, backpropagation, and gradient descent to build a slick and effective dataset that provides the highest return on investment.</li> <li>Noises in datasets are seen as evil. But are they always evil? How much noise can one live with? How to quantify this?</li> <li>How much of a crime it is if data bleeds across traditional train/validate/calibrate/test splits.<ol> <li>What are the recommendations on the data split for cascade training scenarios?</li> </ol> </li> <li>How fancy can one get with data augmentation before returns start to diminish?</li> <li>How to optimize the use of data if continual learning is observed? </li> </ol> <h2 id=recent-advances-in-data-related-techniques><a class=toclink href=../../2021/12/31/data-in-deep-learning/#recent-advances-in-data-related-techniques>Recent advances in data-related techniques</a></h2> <p>If we look at humans are learning machines, they have infinite data at their disposal to learn from. Our system had evolved into efficient strategies to parse through infinite data streams to select the samples we are interested in. How our vision system performs foveal fixation utilizing saccadic eye movements to conduct efficient subsampling of interesting and useful datapoint should be a good motivation. Sure we fail sometimes, we fail to see the pen on the table even though it's right in front of us due to various reasons but we hit it right most of the time. Some concepts of <a href="http://www.gestalttheory.net/cms/uploads/pdf/archive/1934_1960/Principles_Gestalt_Psychology_koffka.pdf">Gestalt theory</a>, a principle used to explain how people perceive visual components (as organized patterns, instead of many disparate parts) are already applicable for better selection of data even if machine models are <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">stochastic parrots</a>. According to this theory, eight main factors, listed below, determine how the visual system automatically groups elements into patterns. </p> <ol> <li>Proximity: Tendency to perceive objects or shapes that are close to one another as forming a group. </li> <li>Similarity: Tendency to group objects if physical resemblance e.g. shape, pattern, color, etc. is present. </li> <li>Closure: Tendency to see complete figures/forms even if what is present in the image is incomplete. </li> <li>Symmetry: Tendency to 'see' objects as symmetrical and forming around a center point. 50 </li> <li>Common fate: Tendency to associate similar movement as part of a common motion. </li> <li>Continuity: Tendency to perceive each object as a single uninterrupted i.e. continuous object </li> <li>Good Gestalt: Tendency to group together if a regular, simple, and orderly pattern can be formed </li> <li>Past experience: Tendency to categorize objects according to past experience. </li> </ol> <p>Of these, I argue, proximity, similarity, common fate, and past experience are relevant. I even argue on the possibility of applying closure. A recent work by FAIR <a href="https://arxiv.org/abs/2111.06377">22</a>, shows that machine models can fill in the gaps and infer missing pieces correctly by applying minor changes commonly used technique autoencoders. Why I bring this up with so much excitement than GAN-based techniques of hallucination is how easy it is to build and train as compared to GAN.</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/masked_autoencoder.jpeg> <em>Masked-Autoencoders showing model inferring missing patches <a href="https://arxiv.org/abs/2111.06377">22</a></em></p> </blockquote> <p>Its been interesting to note that the recent advances towards dealing with the challenges of scaled data are largely inspired by already known deep-learning techniques except they are now applied through the lens of data. Examples like pruning, compression, sampling strategies, and leveraging from phenomena such as catastrophic forgetting, knowledge distillation.</p> <table> <thead> <tr> <th style="text-align: left;">Technique</th> <th style="text-align: center;">How it's presently utilized in model building</th> <th style="text-align: right;">Proposed data-centric view</th> </tr> </thead> <tbody> <tr> <td style="text-align: left;">Prunning</td> <td style="text-align: center;">A specialized class of model compression technique where low magnitude weights are eliminated to reduce the size and computational overhead.</td> <td style="text-align: right;">Samples that don't contribute much to generalization are omitted from the training regime.</td> </tr> <tr> <td style="text-align: left;">Compression</td> <td style="text-align: center;">A broad range of model compression techniques to reduce the size and computational overhead includes techniques like quantization wherein some amount of information loss is expected.</td> <td style="text-align: right;">A broad range of data filtering and compression techniques to reduce size without compromising much on generalization.</td> </tr> <tr> <td style="text-align: left;">Distillation</td> <td style="text-align: center;">To extract learned representation from a more complex model to a potentially smaller model.</td> <td style="text-align: right;">To extract knowledge present in the larger dataset into a smaller synthesized set.</td> </tr> <tr> <td style="text-align: left;">Loss function</td> <td style="text-align: center;">Also termed as the objective function is the one of core concepts of DL that defines the problem statement.</td> <td style="text-align: right;">As shown in <a href="https://arxiv.org/abs/2111.06377">22</a>, and also more broadly can be leveraged to fill in missing information in the data.</td> </tr> <tr> <td style="text-align: left;">Regularisation</td> <td style="text-align: center;">One of the theoretical principles of DL is applied through various techniques like BatchNorm, Dropouts to avoid overfitting.</td> <td style="text-align: right;">Variety of techniques to ensure overfitting applied with data in mind, e.g. Label Smoothing <a href="https://arxiv.org/abs/1710.09412">7</a>,<a href="https://arxiv.org/abs/1512.00567">10</a></td> </tr> </tbody> </table> <p>*Table 1: Summary of techniques that are crossbreed from core DL techniques to also into data-centric DL *</p> <p>Let's dive into the details of how these classes of techniques are applied through the lens of data:</p> <h3 id=1-regularization><a class=toclink href=../../2021/12/31/data-in-deep-learning/#1-regularization>1. Regularization</a></h3> <h4 id=11-mixup><a class=toclink href=../../2021/12/31/data-in-deep-learning/#11-mixup>1.1 Mixup</a></h4> <p>Mixup is a special form of data augmentation technique that looks beyond intra-sample modification and explores inter-sample modification. The idea with mixup is to linearly combine (through convex geometry) a pair of samples to result into one.</p> <p>$ \begin{equation} x\prime=Î»x_i + (1âˆ’Î»)x_j , \ \text{where,} Î» âˆˆ [0,1] \ \text{ drawn from beta distribution, and xi, xj are input/source vector} \end{equation} $</p> <p>$ \begin{equation} y\prime = Î»y_i + (1 âˆ’ Î»)y_j , \ \text{where y_i , yj are one-hot label encodings} \end{equation} $</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/mixup_on_oxford_pets.jpeg> *Figure 4: Sample produced by applying mixup <a href="https://arxiv.org/abs/1710.09412">7</a> on <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">Oxford Pets dataset</a> *</p> </blockquote> <p>Mixup <a href="https://arxiv.org/abs/1710.09412">7</a> in fact seeks to regularize the neural network to favor simple linear behavior in-between training examples. As shown in fig 5, mixup results in better models with fewer missed. Its been shown that mixup increases the generalization, reduces the memorization of corrupt labels, increases the robustness towards adversarial examples <a href="https://arxiv.org/abs/1710.09412">7</a>,<a href="https://arxiv.org/abs/1812.01187">8</a>.</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/mixup-gradnorm.jpeg> <em>Figure 5: Shows that using mixup <a href="https://arxiv.org/abs/1710.09412">7</a>, lower prediction error and smaller gradient norms are observed.</em></p> </blockquote> <p>I see mixup as not only an augmentation, and regularisation technique but also a data compression technique. Depending on how frequently (say Î±) the mixup is applied, the dataset compression ratio (C<sub>r</sub>) will :</p> <p>$ \begin{equation} C_r = 1 - Î±/2 \end{equation} $</p> <p>If you have not noticed already, applying mixup convert labels to soft labels. The linear combination of discreet values will result in a continuous label value that can explain the example previously discussed wherein the pixel is 40% cat 60% dog (see fig 5).</p> <h4 id=12-label-smoothing><a class=toclink href=../../2021/12/31/data-in-deep-learning/#12-label-smoothing>1.2 Label Smoothing</a></h4> <p>Label smoothing <a href="https://arxiv.org/abs/1512.00567">10</a> is a form of regularisation technique that smoothes out ground truth by a very small value epsilon <code>É›</code>. One motivation for this is of course better generalization and avoiding overfitting. While the other motivation is to discourage the model from becoming overconfident. Both <a href="https://arxiv.org/abs/1812.01187">8</a>,<a href="https://arxiv.org/abs/1512.00567">10</a> have shown that label smoothing leads to better models.</p> <p>$ \begin{equation} Q_{i} = \begin{cases} \displaystyle 1 - É› &amp; \text{if i == k,} \ <br> É›/K &amp; \text{Otherwise, where K is number of classes} \ <br> \end{cases} <br> \end{equation} $</p> <p>Label smoothing as indicated by the equation above does not lead to any visible differences in label data as <code>É›</code> is really small. However, applying mixup change visibly changes both source (x) and the label (y).</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/visible_ls.jpeg> <em>Applying label-smoothing has no noticeable difference</em></p> </blockquote> <h3 id=2-compression><a class=toclink href=../../2021/12/31/data-in-deep-learning/#2-compression>2. Compression</a></h3> <p>Compression refers to a broad range of data filtering and compression techniques to reduce size without compromising much on generalization. Following are some of the recent exciting development on this front:</p> <h4 id=21-x-shot-learning-how-many-are-enough><a class=toclink href=../../2021/12/31/data-in-deep-learning/#21-x-shot-learning-how-many-are-enough>2.1. X-shot learning: How many are enough?</a></h4> <p>The troubles of high computational cost and long training times due to an increase in the dataset have led to the development of training by a few shot strategies. The intuition behind this approach is to take a model and guide it to learn to perform a new task only by looking at a few samples <a href="https://arxiv.org/abs/1904.05046">11</a>. The concept of transfer learning is implicitly applied in this approach. This line of investigation started with training new tasks by using only a few (handful) samples and explored an extreme case of one-shot training i.e. learning new tasks from only one sample <a href="https://ieeexplore.ieee.org/document/1597116">12</a>,<a href="https://arxiv.org/abs/1606.04080">13</a>.</p> <p>Recently an interesting mega-extreme approach of shot-based learning has emerged - <code>'Less Than One'-Shot Learning</code> a.k.a LO Shot learning <a href="https://arxiv.org/abs/1904.05046">11</a>. This approach utilizes soft label concepts and seeks to merge hard label N class samples into M samples where M &lt; N and thus the name less than one! LO Shot-based techniques are a form of data compression technique and may feel very similar to the MixUp technique discussed earlier. However, LO Shot contrary to a convex combination of samples as in Mixup, exploits distance weighted k-Nearest Neighbours technique to infer the soft labels. Their algorithm termed <code>distance-weighted soft-label prototype k-Nearest Neighbours (SLaPkNN)</code> essentially takes the sum of the label vectors of the k-nearest prototypes to target point x, with each prototype weighted inversely proportional to its distance from x. The following figure shows 4 class datasets are merged into 2 samples using SLaPkNN.</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/LO-Shot.jpeg> <em>Figure LO-Shot: LO Shot splitting 4 class space into 2 points <a href="https://arxiv.org/abs/1904.05046">11</a>.</em></p> </blockquote> <p>In my understanding that is the main theoretical difference between the two techniques, with the other difference being mixup only merges two samples into one using a probability drawn from beta distribution combined using <code>Î»</code> and <code>1-Î»</code> whereas LO is more versatile and can compress greatly. I am not saying mixup cant be extended to be more multivariate but that empirical analysis of such approach is unknown; whereas with <a href="https://arxiv.org/abs/1904.05046">11</a> its been shown SLaPkNN can compress 3M âˆ’ 2 classes into M samples at least. </p> <p>The technical explanation for this along with code is available <a href="https://github.com/ilia10000/LO-Shot/tree/master/Paper1">here</a>.</p> <h4 id=22-pruning><a class=toclink href=../../2021/12/31/data-in-deep-learning/#22-pruning>2.2. Pruning</a></h4> <p>Pruning is a subclass of compression techniques wherein samples that are not really helpful or effective are dropped out whereas selected samples are kept as is without any loss in content. Following are some of the known techniques of dataset pruning:</p> <h5 id=221-coresets><a class=toclink href=../../2021/12/31/data-in-deep-learning/#221-coresets>2.2.1 Coresets</a></h5> <p>Coreset selection technique pertains to subsampling from a large dataset to a smaller set that almost approximates the given large set. This is not a new technique and has heavily been explored using hand-engineered features and simpler models like Markov models to approximate the smaller set. This is not a DL-specific technique either and has its place in classical ML as well. An example could be using naÃ¯ve Bayes to select coresets for more computationally expensive counterparts like decision trees. </p> <p>In deep learning, using a similar concept, a lighter-weight DL model can be used as a proxy to select the approximate dataset <a href="https://arxiv.org/abs/1906.11829">15</a>. This is easier achieved when continual learning is practiced otherwise it can be a very expensive technique in itself given proxy model needs to be trained with a full dataset first. This becomes especially tricky given the proxy and target models are different and also when the information in the dataset is not concentrated in a few samples but uniformly distributed over all of them. These are some of the reasons why this approach is not very successful.</p> <h5 id=222-example-forgetting><a class=toclink href=../../2021/12/31/data-in-deep-learning/#222-example-forgetting>2.2.2 Example forgetting</a></h5> <p>An investigation <a href="https://arxiv.org/abs/1812.05159">14</a> reported that some samples once learned are never forgotten and exhibit the same behavior across various training parameters and hyperparameters. There are other classes of samples that are forgotten. The forgetting event was defined as when the model prediction regresses in the subsequent epoch. Both qualitative and qualitative (see fig 6 and 7) analysis into such forgotten samples indicated noisy labels, images with "uncommon" visually complicated features were the main reasons for example forgetting.</p> <blockquote> <p><img alt=w src=../../../resources/data-centric-ai/forgetting_stats.jpeg> <em>Figure 6: Algorithm to track forgotten samples <a href="https://arxiv.org/abs/1812.05159">14</a>.</em></p> <p><img alt src=../../../resources/data-centric-ai/forgetten_samples.jpeg> <em>Figure 7: Indicating how increasing fraction of noisy samples led to increased forgetting events <a href="https://arxiv.org/abs/1812.05159">14</a>.</em></p> </blockquote> <p>An interesting observation from this study was that losing a large fraction of unforgotten samples still results in extremely competitive performance on the test set. The hypothesis formed was unforgotten samples are not very informative whereas forgotten samples are more informative and useful for training. In their case, the forgetting score stabilized after 75 epochs (using RESNET &amp; CIFAR but the value will vary as per model and data).</p> <p>Perhaps a few samples are enough to tell that a cat has 4 legs, a small face, and pointy ears, and it's more about how different varieties of cats look especially if they look different from the norm e.g. Sphynx cats.</p> <h5 id=223-using-gradient-norms><a class=toclink href=../../2021/12/31/data-in-deep-learning/#223-using-gradient-norms>2.2.3 Using Gradient norms</a></h5> <p>Loss functions are an excellent measure to find interesting samples in your dataset whether these may be poorly labeled or really outlier samples. This was highlighted by <a href="https://twitter.com/karpathy/status/1311884485676294151">Andrej Karpathy</a> as well:</p> <blockquote> <p>When you sort your dataset descending by loss you are guaranteed to find something unexpected, strange, and helpful.</p> </blockquote> <p>Personally, I have found loss a very good measure to find poorly labeled samples. So, the natural question would be "should we explore how we can use the loss as a measure to prune the dataset?". It's not until NeurIPS 2021, <a href="https://arxiv.org/abs/2107.07075">21</a> that this was properly investigated. This Standford study looked into the initial loss gradient norm of individual training examples, averaged over several weight initializations, to prune the dataset for better generalization. This work is closely related to example forgetting except that instead of performance measure the focus more is on using local information early in training to prune the dataset. </p> <p>This work proposes GraNd score of a training sample (x, y) at time t given by L2 norm of the gradient of loss computed on that sample and also expected loss L2 norm termed EL2N (equation below). The reasoning here is that samples with a small GraNd score have abounded influence on learning how to classify the rest of the training data at a given training time. Empirically, this paper found that averaging the norms over multiple weight initializations resulted in scores that correlate with forgetting scores <a href="https://arxiv.org/abs/1812.05159">14</a> and leads to pruning a significant fraction of samples early in training. They were able to prune 50% of the sample from <em>CIFAR-10 without affecting accuracy</em>, while on the more challenging CIFAR-100 dataset, they pruned 25% of examples with only a 1% drop in accuracy. <br> $ \begin{equation} Ï‡_t(x, y) = E_{w_t} || g_t(x, y)||_2 \ \tag*{GraNd eq} \ \end{equation} $</p> <p>$ \begin{equation} Ï‡_t(x, y) = E || p(w_t, x) - y)||_2 \ \tag*{EL2N eq} \ \end{equation} $</p> <p>This is a particularly interesting approach and is a big departure from other pruning strategies to date which treated samples in the dataset independently. Dropping samples based on independent statistics provides a weaker theoretical guarantee of success as DL is a non-convex problem <a href="https://arxiv.org/abs/2107.07075">21</a>. I am very curious to find out how mixup impacts the GraNd scores given it shown (see figure 5b) using mixup leads to smaller gradient norm (l1 albeit).</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/gradnd_el2n.jpeg> <em>Results of prunning with GradNd and EL2N <a href="https://arxiv.org/abs/2107.07075">21</a>.</em></p> </blockquote> <p>The results from this study are shown in the fig above. Noticeably high pruning is not fruititious even with this approach despite how well it's doing on CIFAR-10 and 100 datasets. Are we retaining the data distribution when we drop large samples? Mostly not and that is only reasoning that makes sense. And we circle back to how much pruning is enough? Is that network dependent or more a property of data and its distribution? This study <a href="https://arxiv.org/abs/2107.07075">21</a> claims that GradND and EL2N scores, when averaged over multiple initializations or training trajectories remove dependence on specific weights/networks, presenting a more compressed dataset. If this assertion holds in reality, in my view, this is a very promising finding easing the data-related challenges of DL.</p> <p>What's more fascinating about this work is that it sheds light on how the underlying data distribution shapes the training dynamics. This has been amiss until now. Of particular interest is identifying subspaces of the model's data representation that are relatively stable over the training.</p> <h4 id=23-distillation><a class=toclink href=../../2021/12/31/data-in-deep-learning/#23-distillation>2.3. Distillation</a></h4> <p>Distillation technique refers to the methodologies of distilling the knowledge of a complex or larger set into a smaller set. <a href="https://arxiv.org/abs/1503.02531">Knowledge or model distillation</a> is a popular technique that compresses the learned representation of a larger model into a much smaller model without any significant drop in performance. Using student-teacher training regime have been explored extensively even in the case of transformer networks that are even more data-hungry than more conventional network say Convolution network <a href="https://arxiv.org/abs/2012.12877">DeiT</a>. Despite being called data-efficient, this paper employs a teacher-student strategy to transform networks, and data itself is merely treated as a commodity. </p> <p>Recently, this concept is investigated for use in deep learning for dataset distillation with aim of synthesizing an optimal smaller dataset from a large dataset <a href="https://arxiv.org/abs/1811.10959">17</a>,<a href="https://arxiv.org/abs/2102.08259">16</a>. The distilled dataset are learned and synthesized but in theory, they approximate the larger dataset. Note that the synthesized data may not follow the same data distribution.</p> <p>Some dataset distillation techniques refer to their approach as compression as well. I disagree with this in principle as compression albeit lossy in this context, refers to compressing the dataset whereas with distillation the data representation is deduced/synthesized - potentially leading to entirely different samples together. Perhaps it's the compressibility factor a.k.a compression ratio that applies to both techniques. For example, see figure 13 shows the extent to which distilled images can change.</p> <p>A dataset distillation <a href="https://arxiv.org/abs/1811.10959">17</a> paper quotes:</p> <blockquote> <p>We present a new optimization algorithm for synthesizing a small number of synthetic data samples not only capturing much of the original training data but also tailored explicitly for fast model training in only a few gradient steps <a href="https://arxiv.org/abs/1811.10959">17</a>.</p> </blockquote> <p>Their problem formulation was very interesting! they derive the network weights as a differentiable function of the synthetic training data and set the training objective to optimize the pixel values of the distilled images The result from this study showed that one can go as low as one synthetic image per category while not regressing too much on the performance. More concretely, they distilled 60K training images of the MNIST digit dataset into only 10 synthetic images (one per class) yields a test-time MNIST recognition performance of 94%, compared to 99% for the original dataset). </p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/fair_mnist_distillation.jpeg> <em>Figure 8: Dataset distillation results from FAIR study <a href="https://arxiv.org/abs/1811.10959">17</a>.</em></p> </blockquote> <p>Here are some of the distilled samples for the classes labeled at the top (fig 9). It's amazing how well the MNIST trained on these sets does but CIFAR one misses the mark only being about as good as random (54%) compared to 80% on the full dataset(fig 8 &amp; 9).</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/example_of_distilled_images.jpeg> <em>Figure 9: Dataset distillation results from FAIR study <a href="https://arxiv.org/abs/1811.10959">17</a>.</em></p> </blockquote> <p>Following this work, another distillation technique was proposed utilizing kernel methods - more specifically kernel ridge regression to obtain Îµ-approximate of original datasets <a href="https://arxiv.org/abs/2011.00050">18</a>. This technique is termed Kernel Inducing Points (KIP) and follows the same principle for keeping the objective function to maximize the approximation and backpropagate the gradients to learn synthesized distilled data. The difference between <a href="https://arxiv.org/abs/2011.00050">18</a> and <a href="https://arxiv.org/abs/1811.10959">17</a> is one <a href="https://arxiv.org/abs/1811.10959">17</a> uses the same DL network while the other <a href="https://arxiv.org/abs/2011.00050">18</a> uses kernel techniques. With KIP, another advantage is that not just source samples but optionally labels can be synthesized too. In <a href="https://arxiv.org/abs/1811.10959">17</a>, the objective was purely to learn pixel values and thus the source (X). This paper <a href="https://arxiv.org/abs/2011.00050">18</a> also proposes another technique Label Solve (LS) in while X is kept constant and only label (Y) is learned.</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/KIP_LS.jpeg> <em>Figure 10: Examples of distilled samples a) with KIP and b) With LS <a href="https://arxiv.org/abs/2011.00050">18</a>.</em></p> </blockquote> <p>The CIFAR 10 result from <a href="https://arxiv.org/abs/1811.10959">17</a> (fig 9) was about 36.79% for 10 samples, with KIP there is a slight gain in performance there given the extreme compression. This raises the question of what is a good compression ratio that can guarantee good information retention. For complex tasks like CIFAR (compared to MNIST), 10 (one per sample) may not be enough given how complex this dataset is comparatively. </p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/KIP_LS_Results.jpeg> <em>Figure 11: CIFAR10 result from KIP and LS <a href="https://arxiv.org/abs/2011.00050">18</a>.</em></p> </blockquote> <p>Actually, LO shot technique <a href="https://arxiv.org/abs/1904.05046">11</a>, discussed previously, is also a specialized form of X-shot technique that does dataset distillation. Aside from that, gradient-based techniques for dataset distillation have been actively investigated in the last few years (ref <a href="https://arxiv.org/abs/2102.08259">16</a>,<a href="https://arxiv.org/abs/1811.10959">17</a>,<a href="https://arxiv.org/abs/2011.00050">18</a>,<a href="https://arxiv.org/abs/2006.05929">19</a>,<a href="https://arxiv.org/abs/2107.13034">20</a>). Another approach explored siamese augmentation method termed <code>Differentiable Siamese Augmentation (DSA)</code> that uses matching loss and synthesizes dataset through backpropagation techniques <a href="https://arxiv.org/abs/2102.08259">16</a></p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/dsa.jpeg> <em>Figure 12: Differentiable Siamese Augmentation <a href="https://arxiv.org/abs/2102.08259">16</a>.</em></p> </blockquote> <p>Bayesian and gradient-descent trained neural networks converge to Gaussian processes (GP) as the number of hidden units in intermediary layers approaches infinity <a href="https://arxiv.org/abs/2107.13034">20</a> (fig 13). This is true for convolutional networks as well as they converge to a particular gaussian processes channel limits are stretched to infinity. These networks can thus be described as kernels known as Neural Tangent Kernel (NTK). <a href="https://github.com/google/neural-tangents">Neural Tangents</a> library based on <a href="https://github.com/google/jax">JAX</a> an auto differentiation toolkit has been used in applying these kernels in defining some of the recent distillation methods. References <a href="https://arxiv.org/abs/2011.00050">18</a>,<a href="https://arxiv.org/abs/2107.13034">20</a>,<a href="https://arxiv.org/abs/2107.07075">21</a> are one such examples. </p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/1_640_50fps_FINAL_VERSION.gif> <em>Figure 13: Infinite width Convolution networks converging to infinity <a href="https://arxiv.org/abs/2107.13034">20</a>.</em></p> </blockquote> <p>The authors of KIP and LS techniques <a href="https://arxiv.org/abs/2011.00050">18</a> explore how to scale and accelerate the distillation process to apply these techniques to infinitely wide convolutional networks <a href="https://arxiv.org/abs/2107.13034">20</a>. Their results are very promising (see fig 14).</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/KIP_ConvNet.jpeg> <em>Figure 14: KIP ConvNet results <a href="https://arxiv.org/abs/2107.13034">20</a>.</em></p> </blockquote> <p>A visual inspection into the distilled dataset from the infinite width CNN-based KIP technique is shown in fig 15. The distillation results are curious, to say the least. In the example, the distilled apple image seems to represent a pile of apples whereas the bottle distilled results into visibly different bottles while still showing artifacts of the original two bottles. While other classes show high order features (with some noise). </p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/example_kip_infinite_width_kernel.jpeg> <em>Figure 15: KIP ConvNet example of distilled CIFAR set <a href="https://arxiv.org/abs/2107.13034">20</a>.</em></p> </blockquote> <p>Figure 16 shows MNIST results, they are not only very interesting but also look very much like mixup (where x and y both are mixed).</p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/KIP_Infi_MNIST.jpeg> <em>Figure 16: KIP ConvNet example of distilled MNIST set <a href="https://arxiv.org/abs/2107.13034">20</a>.</em></p> </blockquote> <h3 id=3-so-what-if-you-have-noisy-data><a class=toclink href=../../2021/12/31/data-in-deep-learning/#3-so-what-if-you-have-noisy-data>3. So what if you have noisy data</a></h3> <p>Noises in the dataset are considered a nuisance. Because models hold the compressed form of knowledge represented by the dataset, dataset curation techniques carefully look to avoid noises in datasets. </p> <p>Noises can be an incredibly powerful technique to fill in the missing information in source/images. For instance, if only part of an image is known then instead of padding the image with default (0 or 1-pixel value), filling in using random noise can be an effective technique avoiding confusion on actual real values that relate to the black or white region. This has held true in my own experience. It was very amusing to note that the forgetting event study <a href="https://arxiv.org/abs/1812.05159">14</a> in fact looked into adding label noise on the distribution of forgetting events. They added noise in pixel values and observed that adding an increasing amount of noise decreases the number of unforgettable samples (see also figure 7 for their results when noise was used). </p> <p>Noise coming from randomness is handled very well by DL networks as well. I find the result from <a href="https://arxiv.org/abs/2111.06377">22</a>, shown in figure 17 quite fascinating actually. Looking at how well the model is doing when missing patches are random and how poor it is doing when missing patches are systematic is indicative of how powerful and lame (both at the same time) machines are! </p> <blockquote> <p><img alt src=../../../resources/data-centric-ai/masked_autoencoder_noisy_patches.jpeg> <em>Figure 17: Noisy patches in Masked-AutoEncoder <a href="https://arxiv.org/abs/2111.06377">22</a>.</em></p> </blockquote> <p>GradND study <a href="https://arxiv.org/abs/2107.07075">21</a>, looked into the effect of noise on the source itself and performed a series of experiments to conclude that when there is enough data, keeping the high score examples, which are often noisy or difficult, does not hurt performance and can only help.</p> <h2 id=conclusion><a class=toclink href=../../2021/12/31/data-in-deep-learning/#conclusion>Conclusion</a></h2> <p>In summary, the last four years have been incredibly exciting for data in DL space and the year 2021 even more! There is a lot of mileage we can get out of simpler techniques like MixUp but more exciting developments are dissecting the training dynamics and exploring the importance of samples in solving a particular task using DL techniques. Distillation methods are still in the early stages where they work well for simpler datasets but honestly how many real-world problems have simple datasets? Nevertheless, some really exciting development in this space. These techniques can be groundbreaking if the compression methods hold across a wide range of architectures as indicated by <a href="https://arxiv.org/abs/2107.07075">21</a>. </p> <h2 id=references><a class=toclink href=../../2021/12/31/data-in-deep-learning/#references>References</a></h2> <ol> <li>[1707.02968] Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.â€ Accessed January 3, 2022. <a href="https://arxiv.org/abs/1707.02968">https://arxiv.org/abs/1707.02968</a>.</li> <li>Hestness, Joel, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. â€œDeep Learning Scaling Is Predictable, Empirically,â€ December 2017. <a href="https://arxiv.org/abs/1712.00409">https://arxiv.org/abs/1712.00409</a>.</li> <li><a href="https://www.wired.com/story/no-data-is-not-the-new-oil/">https://www.wired.com/story/no-data-is-not-the-new-oil/</a></li> <li><a href="https://pages.run.ai/hubfs/PDFs/2021-State-of-AI-Infrastructure-Survey.pdf">https://pages.run.ai/hubfs/PDFs/2021-State-of-AI-Infrastructure-Survey.pdf</a></li> <li>[1808.01974] A Survey on Deep Transfer Learning. Accessed January 5, 2022. <a href="https://arxiv.org/abs/1808.01974">https://arxiv.org/abs/1808.01974</a>.</li> <li>Transfer Learning. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.1515&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.1515&amp;rep=rep1&amp;type=pdf</a></li> <li>Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. â€œMixup: Beyond Empirical Risk Minimization,â€ October 2017. <a href="https://arxiv.org/abs/1710.09412">https://arxiv.org/abs/1710.09412</a>.</li> <li>[1812.01187] Bag of Tricks for Image Classification with Convolutional Neural Networks. Accessed December 30, 2021. <a href="https://arxiv.org/abs/1812.01187">https://arxiv.org/abs/1812.01187</a>.</li> <li>[2009.08449] 'Less Than One'-Shot Learning: Learning N Classes From M &lt; N Samples. Accessed January 5, 2022. <a href="https://arxiv.org/abs/2009.08449">https://arxiv.org/abs/2009.08449</a>.</li> <li>[1512.00567] Rethinking the Inception Architecture for Computer Vision. Accessed January 5, 2022. <a href="https://arxiv.org/abs/1512.00567">https://arxiv.org/abs/1512.00567</a>.</li> <li>[1904.05046] Generalizing from a Few Examples: A Survey on Few-Shot Learning. Accessed January 5, 2022. <a href="https://arxiv.org/abs/1904.05046">https://arxiv.org/abs/1904.05046</a>.</li> <li>Li Fei-Fei, R. Fergus and P. Perona, "One-shot learning of object categories," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 594â€“611, April 2006, doi: 10.1109/TPAMI.2006.79. <a href="https://ieeexplore.ieee.org/document/1597116">https://ieeexplore.ieee.org/document/1597116</a></li> <li>[1606.04080] Matching Networks for One Shot Learning. Accessed January 5, 2022. <a href="https://arxiv.org/abs/1606.04080">https://arxiv.org/abs/1606.04080</a>.</li> <li>[1812.05159] An Empirical Study of Example Forgetting during Deep Neural Network Learning. Accessed December 29, 2021. <a href="https://arxiv.org/abs/1812.05159">https://arxiv.org/abs/1812.05159</a>.</li> <li>[1906.11829] Selection via Proxy: Efficient Data Selection for Deep Learning. Accessed December 29, 2021. <a href="https://arxiv.org/abs/1906.11829">https://arxiv.org/abs/1906.11829</a>.</li> <li>[2102.08259] Dataset Condensation with Differentiable Siamese Augmentation. Accessed January 5, 2022. <a href="https://arxiv.org/abs/2102.08259">https://arxiv.org/abs/2102.08259</a>.</li> <li>[1811.10959] Dataset Distillation. Accessed January 5, 2022. <a href="https://arxiv.org/abs/1811.10959">https://arxiv.org/abs/1811.10959</a>.</li> <li>[2011.00050] Dataset Meta-Learning from Kernel Ridge-Regression. Accessed January 5, 2022. <a href="https://arxiv.org/abs/2011.00050">https://arxiv.org/abs/2011.00050</a>.</li> <li>[2006.05929] Dataset Condensation with Gradient Matching. Accessed January 5, 2022. <a href="https://arxiv.org/abs/2006.05929">https://arxiv.org/abs/2006.05929</a>.</li> <li>[2107.13034] Dataset Distillation with Infinitely Wide Convolutional Networks. Accessed January 5, 2022. <a href="https://arxiv.org/abs/2107.13034">https://arxiv.org/abs/2107.13034</a>.</li> <li>[2107.07075] Deep Learning on a Data Diet: Finding Important Examples Early in Training. Accessed December 10, 2021. <a href="https://arxiv.org/abs/2107.07075">https://arxiv.org/abs/2107.07075</a>.</li> <li>[2111.06377] Masked Autoencoders Are Scalable Vision Learners. Accessed January 5, 2022. <a href="https://arxiv.org/abs/2111.06377">https://arxiv.org/abs/2111.06377</a>.</li> <li>[1802.03426] UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. Accessed January 5, 2022. 23. <a href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</a>.</li> <li>Maaten, Laurens van der and Geoffrey E. Hinton. "Visualizing Data using t-SNE." Journal of Machine Learning Research 9 (2008): 2579â€“2605. <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf</a></li> <li>[2107.02331] Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering.â€ Accessed January 8, 2022. <a href="https://arxiv.org/abs/2107.02331">https://arxiv.org/abs/2107.02331</a></li> </ol> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2021-03-14 00:00:00">March 14, 2021</time></li> <li class=md-meta__item> in <a href=../../category/kubernetes/ class=md-meta__link>Kubernetes</a>, <a href=../../category/oom/ class=md-meta__link>OOM</a></li> <li class=md-meta__item> 19 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=wth-who-killed-my-pod-whodunit><a href=../../2021/03/14/wth-who-killed-my-pod---whodunit/ class=toclink>WTH! Who killed my pod - Whodunit?</a></h2> <p>A few days ago, I deployed a brand new application onto a self-managed Kubernetes cluster (hereafter referred to as Kube). Suffice to say, all hell broke loose. The pods were getting <code>OOMKilled</code> with error code 137 left and right! </p> <p>Now, I know a thing or two about Kubernetes<sup><a href="https://suneeta-mall.github.io/talks/KubernetesSydneyForum_AU_2019.html">1</a>,<a href="https://suneeta-mall.github.io/talks/KubeCon_US_2019.html">2</a></sup>. I am not a total Kube noob!<br> But, I could not figure out what the fudge was going on actually! Besides, this app has been thoroughly tested and profiled and ran fine on bare metal and virtual environments.</p> <p>So this was me, a few days ago!.</p> <!-- ![](../../resources/oom/4201968f94aacab1c0190d9688daba00-sticker.jpg)--> <p><img alt= src="../../../assets/external/media4.giphy.com/media/z9AUvhAEiXOqA/giphy-downsized.gif"></p> <p>This sparked a massive hunt for the culprit, and some interesting insights were discovered. Worth noting, similar investigating has also been done on by <a href="https://engineering.linecorp.com/en/blog/prometheus-container-kubernetes-cluster/">Line Corp</a> in their excellent blog however, I have a different story to tell!</p> <p>In this writeup, I am going to talk about this particular incident and the insights I have uncovered about both Kube and Linux kernels.</p> <h3 id=context-of-the-app><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#context-of-the-app>Context of the app</a></h3> <p>The app runs some intensive <a href="https://numpy.org">numpy</a> and <a href="https://tensorflow.org">Tensorflow</a> computations to produce some artifacts and associative metadata. The workloads are more memory-intensive as they operate on rich multi-media content. Other gory details besides <em>resource requirements</em> of the app is irrelevant for this discussion. </p> <p>The average resource requirement, for this app, is very fluctuating yet predictable (in a given range). At least so we thought looking at our metrics:</p> <p><img alt src=../../../resources/oom/avg-resource-requirement.jpg> <em>Figure 2: Average resource requirements of the app when run on VMs or bare metal</em></p> <p>I hear you, the resource utilization is not following a zero gradient line (fig 2)! It would be awesome to have constant non-flapping resource requirement needs - so clearly some work needs to happen on the app here. Having said that, it's an absolutely acceptable and supported workload. </p> <p>Ok, so the app was deployed and now, we will look at the line of investigation:</p> <h3 id=apps-on-kube-day-1><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#apps-on-kube-day-1>App's on Kube: day 1</a></h3> <p>The provisioned app pods started to get killed as frequently as every 20 mins or more with error code 137 and reason <code>OOMKilled</code>. </p> <p><img alt src=../../../resources/oom/Joys%20of%20being%20killed%21.jpg> <em>Figure 3: The killer is on the loose! - Whodunit?</em></p> <p>Let me explain a few things about the failure first: 1. <code>Error code 137</code> indicates that the container process received the SIGKILL and thus was killed by the OS kernel. SIGKILL on Kube can only be produced using one of the following means:</p> <div class=highlight><pre><span></span><code>1.1. Manually (human): Triggering CTRL+C or using other means of manually sending SIGKILL or even manually killing process.

1.2. Container Runtime/Interface: `Kubelet` the process running on the host machine that manages running Kube workload is `the power that be` for containers. 
It communicates through container runtime to manage the container lifecycle. It can kill and almost always kills badly behaving pods!

![](../../resources/oom/CRI.png)
*Figure 4: Container runtime interface. Image Credit: [Ian Lewis]! Borrowed from his 4 part container runtime series [container runtime] that I highly recommend reading*

1.3. OS kernel: The OS kernel is responsible for the life cycle of processes running on the host. 
It is `the mighty power that be` for all the processes on the host including the container process and its children.
It can also kill and almost always kills badly behaving processes!
</code></pre></div> <ol> <li><code>OOMKilled</code> represent a kill event (SIGKILL) triggered to a process because someone <em>in-charge</em> suspected of the process to be the culprit of a memory surge that may lead to an out of memory event. This is a safeguard mechanism to avoid system-level failure and to nip mischieve in the bud. </li> </ol> <p><code>Takeaway 1</code>: Either Container Runtime/Interface or OS Kernel killed my process because supposedly it was misbehaving and causing the out-of-memory issue! Essentially, I am ruling out the manual kill because that was simply not the case!</p> <h4 id=deep-dive-into-factors-at-play-here><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#deep-dive-into-factors-at-play-here>Deep-dive into factors at play here</a></h4> <ol> <li> <p><em>Container runtime</em> (in fig 4) is responsible for two things: </p> <p>a) Running containers: Comes from open container initiative (OCI) (about 2013) open sourced by Docker called "runc". It provides ability to run containers.</p> <p>b) Image management: How images are packed, unpacked, pushed, pulled etc comes under this umbrella. A good example for this is "containerd". </p> <p><img alt src=../../../resources/oom/docker_stack.jpeg> <em>Figure 5: Docker stack! Image credit: internet</em></p> <p>There are several other implementation for runtime than runc+containerd like rkt but for me, its <code>runc+containerd</code> in play.</p> </li> <li> <p><a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">control groups</a> are a Linux kernel feature that allows processes to be organized into hierarchical groups whose usage of various types of ../resources (memory, CPU, and so on) can then be limited and monitored. The cgroups interface is provided through a pseudo-filesystem called cgroupfs. You may have heard about <code>/sys/fs/cgroup/</code>! </p> <p><a href="https://twitter.com/lizrice">Liz Rice</a> did an excellent demonstration of <a href="https://www.youtube.com/watch?v=8fi7uSYlOdc">what it means to run a container and how they work</a> that I highly recommend going through. Don't forget playing with the <a href="https://github.com/lizrice/containers-from-scratch/blob/master/main.go">demo code</a>. It gives a foundational understanding of cgroups's role in all things containers.</p> <p><img alt= src="../../../assets/external/wizardzines.com/zines/containers/samples/cgroups.jpg"> <em>Figure 6: CGroup in picture! Image credit: <a href="https://wizardzines.com/zines/containers/">zines</a> by <a href="https://twitter.com/b0rk">Julia Evans</a></em></p> </li> <li> <p><code>Kubelet</code> (see fig 4) not only interfaces container runtime but also has <code>cAdvisor</code>(for <a href="https://github.com/google/cadvisor"><strong>C</strong>ontainer <strong>Advisor</strong></a>) integrated within. Note <code>kubelet</code> is a service running on the host and it operates at the host level, not the pod. With <code>cAdvisor</code> it captures resource utilization, statistics about <a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">control group</a> of all container processes on the host.</p> </li> <li> <p>Kubernetes manages the resource for containers using <code>cgroups</code> that guarantees resource isolation and restrictions. Kube can allocate X amount of ../resources to a container and allow the ../resources to grow until a pre-existing limit is reached or no more is left on the host to use. Kube provides these <a href="https://kubernetes.io/docs/concepts/configuration/manage-../resources-containers/">requests and limits</a> semantic on containers which are used to enforce the said limit on process hierarchy for each container via cgroups. Now, the `limit is not always a hard cut-off. As documented in google's blog of <a href="https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits">best practices resource requests and limits</a>, there are two types of ../resources:</p> <ol> <li><em>Compressible</em> ../resources: When resource limit is reached, Kube will throttle the container i.e. start to restrict the usage but won't actually terminate the container. CPU is considered as a compressible resource.</li> <li><em>Incompressible</em> ../resources: When a limit for this type of resource is reached, the highest usage process within the cgroups hierarchy will be killed. Memory is an incompressible resource. </li> </ol> <p><code>Takeaway 2</code>: It's not the CPU limit, but the memory limit that we need to focus on.</p> </li> <li> <p>Kubernetes classifies pod into three categories based on the quality of service (QoS) they provide:</p> <p>4.1 <em>Guaranteed</em> pods are those who's resource request and limit are just the same. These are the best kind of workload from Kube's viewpoint as they are easier to allocate and plan for resource-wise. These pods are guaranteed to not be killed until they exceed their limits. <img alt src=../../../resources/oom/qos-guranteed.jpg> <em>Figure 7: Guaranteed QoS pod example</em></p> <p>4.2 <em>Best-Effort</em> pods are those where no resource requirements are specified. These are the lowest priority pods and the first to get killed if the system runs out of memory. <img alt src=../../../resources/oom/qos-best%20effort.jpg> <em>Figure 8: Best-Effort QoS pod example</em></p> <p>4.3 <em>Burstable</em> pods are those whose resource request and limit are defined in a range (fig 9), with limit treated as max if undefined. These are the kind of workloads that are more likely to be killed when the host system is under load and they exceed their requests and no Best-Effort pods exist. <img alt src=../../../resources/oom/qos-bustable.jpg> <em>Figure 9: Burstable QoS pod example</em></p> <div class=highlight><pre><span></span><code>So can Kube over-commit? 
If yes, would it always be on the compressible ../resources? 
</code></pre></div> <p>Yes, Kube can overcommit. The pod limits are allowed to be higher than requests. It's possible that the sum of all limits exceeds the total node capacity. It's possible to overcommit both compressible and incompressible ../resources. This is pictorially explained <a href="https://sysdig.com/blog/troubleshoot-kubernetes-oom/">here</a>. In fact, with Kube, it's also possible to not only vertically overcommit but also horizontally (at cluster level) overcommit. Horizontal overcommits are nicer as they can trigger auto-scaling events to scale out. </p> </li> </ol> <h4 id=so-why-the-pods-are-getting-killed><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#so-why-the-pods-are-getting-killed>So why the pods are getting killed?</a></h4> <p>The app was initially deployed with <code>Burstable</code> QoS with Memory requirements set at request: 4Gi, limit: 7Gi, and CPU set at 2 for both requests, limits (see fig 2). The nodes were AWS <code>r5.2xlarge</code> type with 8 CPU, 64GB RAM, running Debian/Linux. Other than Kube system components and the app, nothing else was deployed on these nodes. </p> <p>So, Kube could have only deployed 3 app pods per <code>r5.2xlarge</code> nodes (due to CPU request). This means, 43GB (=64-7*3) of RAM was lying around singing hakuna matata! What a waste! Sure but let's not digress! So why the OOMKill? <code>Â¯\_(ãƒ„)_/Â¯</code></p> <p>Noteworthy observation: - Node monitoring tells us that is running healthy and has plenty of ../resources at its disposal. - the pod is still OOMKilled but not all app pods on the node, just one is killed.</p> <p>I am still clueless. So, caving in, I decided to use up this extra memory floating around and beef up the nodes a bit more and buy more time to do a proper investigation. Now, the apps are redeployed again with RAM request 4Gi, limit: 31Gi (leaving 4GB for other misc system components).</p> <p>Did that ameliorate the problem - no! Of course, I am being silly about this, I should be making it guaranteed to have better chance of avoiding OOMKill. </p> <h3 id=apps-on-kube-day-2><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#apps-on-kube-day-2>App's on Kube: day 2</a></h3> <p>So, my apps are running with guaranteed QoS with 31GB of RAM as required/limit. Node still seems healthy and shows no sign of duress. </p> <p>Hows the app doing with the new revised configuration: <code>still getting OOMKilled with 137 error code left and right!</code> </p> <p><img alt src=../../../resources/oom/34b8525b2cff89f7f25f2f70d62c5014-sticker.png></p> <p>Meanwhile, we uncovered random memory surges in some pods (see figure 10). These surges occurred very rarely and did not match to the duration of out-of-memory kill events. In fact, the frequency of OOM was much higher than these memory surges. </p> <p><img alt src=../../../resources/oom/memory-spike.jpg> <em>Figure 10: The notorious spike of memory use on pod</em></p> <p>While these surges are worth investigating, they are still within the request/limit range (28.x Gi suurge on 31Gi request). So they still don't justify the OOM event.</p> <h4 id=whats-log-telling-us><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#whats-log-telling-us>Whats log telling us</a></h4> <p>Based on Takeaway 1 &amp; 2, we look at who is firing the kill signal. #Whodunit</p> <h5 id=kube-events-for-pod-and-other-higher-level-abstractions><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#kube-events-for-pod-and-other-higher-level-abstractions>Kube events for pod and other higher-level abstractions</a></h5> <p>Investigating, on Kube <code>Events</code> there is no record or any OOMKill or any event signaling anything malicious. <div class=highlight><pre><span></span><code>kubectl<span class=w> </span>describe<span class=w> </span>pod<span class=w> </span>&lt;my<span class=w> </span>pod&gt;
kubectl<span class=w> </span>describe<span class=w> </span>deploy<span class=w> </span>&lt;my<span class=w> </span>pod&gt;
</code></pre></div> In fact, according to my Kube event stream <code>kubectl get events</code>, Kube is all healthy and there is nothing to see, nothing to worry about there! It shows that containers are clearly being restart but it seems to be not capturing any adverse event and bringing it back up to keep to desired declared state on attached replicaset. <div class=highlight><pre><span></span><code>26m         Normal   Created   pod/myapp   Created container planck
26m         Normal   Started   pod/myapp   Started container planck
26m         Normal   Pulled    pod/myapp   Container image &quot;app&quot; already present on machine
</code></pre></div></p> <h5 id=what-are-the-cri-and-kubelet-doing><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#what-are-the-cri-and-kubelet-doing>What are the CRI and kubelet doing?</a></h5> <p>Looking at the system journal, there is nothing noteworthy recorded for OOM. 1. Nothing is logged for <code>Out of memory</code> (command reference <code>journalctl -u kubelet | grep -i "Out of memory"</code>) 2. Only log I see for shorter term <code>oom</code> (cmd reference <code>journalctl -u kubelet | grep -i "oom"</code> is info level log of kubelet startup record.<br> <div class=highlight><pre><span></span><code>kubelet[2130]: I0309 04:52:13.990735    2130 flags.go:33] FLAG: --oom-score-adj=&quot;-999&quot;
kubelet[2130]: I0309 04:52:15.416807    2130 docker_service.go:258] Docker Info: &amp;{ID:XF74:2JFW:UOE4:QI7X:TXQU:RJLG:E7FC:K4K3:IUTM:MGFW:W2GM:Z6AC Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:false IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:23 OomKillDisable:true NGoroutines:44 SystemTime:2021-03-09T04:52:15.411198727Z LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:4.9.0-14-amd64 OperatingSystem:Debian GNU/Linux 9 (stretch) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00062c0e0 NCPU:16 MemTotal:133666107392 Generic../resources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ip-172-30-36-152 Labels:[] ExperimentalBuild:false ServerVersion:18.06.3-ce ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:&lt;nil&gt; Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:468a545b9edcd5932818eb9de8e72413e616e86e Expected:468a545b9edcd5932818eb9de8e72413e616e86e} RuncCommit:{ID:a592beb5bc4c4092b1b1bac971afed27687340c5 Expected:a592beb5bc4c4092b1b1bac971afed27687340c5} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[]}
kubelet[2130]: I0309 04:52:15.437879    2130 manager.go:1159] Started watching for new ooms in manager
</code></pre></div></p> <p>Normally, in the event of OOM triggered by Kube, we should see kubelet recording some signal for oom e.g. <code>An OOM event was triggered</code></p> <p><code>Takeaway 3</code>: As far as Kube is concerned, the pod is well behaved and it's all hakuna matata! <img alt= src="../../../assets/external/i.pinimg.com/originals/7a/18/f9/7a18f9c6efe7a954a42473cf8a5bd1fb.gif"></p> <p>So, #Whodunit? Enter day 3 - new day new investigation </p> <h3 id=apps-on-kube-day-3><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#apps-on-kube-day-3>App's on Kube: day 3</a></h3> <p>Based on the previous 3 takeaways, the only potential suspect we have is OS kernel. The pods are still crashing and metrics, events, and Kube level logs do not justify the observation. </p> <h4 id=reading-kernel-logs><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#reading-kernel-logs>Reading kernel logs</a></h4> <ol> <li> <p>System level log scan <code>grep -i -r 'out of memory' /var/log/</code> takes us somewhere. <div class=highlight><pre><span></span><code>/var/log/kern.log:Mar  9 13:17:05 ip-172-xxx-xx-xxx kernel: [30320.358563] Memory cgroup out of memory: Kill process 11190 (app) score 9 or sacrifice child
</code></pre></div> <code>Takeaway 4</code>: We do in fact have kernel thinking memory cgroups is in danger and starting to kill!</p> </li> <li> <p>Kernel logs (<code>/var/log/kern.log</code>) seem to have much more insightful info than the above one-liner <code>out of memory: Kill process</code>. </p> </li> </ol> <p>But before we look into this, let's do a bit of a deep dive into related concepts: </p> <h4 id=deep-dive-into-os-kernel><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#deep-dive-into-os-kernel>Deep-dive into OS Kernel</a></h4> <ol> <li><strong>Swap space and Kube</strong></li> </ol> <p>Docker supports setting <a href="https://docs.docker.com/config/containers/resource_constraints/">swappiness</a> however it's discouraged as it's slow and less performant. Also, providing a limit on the swap is unsupported at the docker level which can lead to resource management and overcommitment chaos. These are some of the reasons why <a href="https://github.com/kubernetes/kops/issues/3251">kops</a> and in general Kube prefer no swap on hosts. </p> <ol> <li><strong>OOMKill disable on Kubernetes</strong></li> </ol> <p>OS Kernels allow disabling OOM Kill for cgroups level (<code>/sys/fs/cgroup/memory/memory.oom_control</code>) even docker supports it using <code>--oom-kill-disable</code> flag. These are highly discouraged due to the nature of problem band-aid fixer <code>OOM Killer</code> solves. It also does not sit with Kube's declarative approach orchestration and also with cattle workload philosophy. It's also why by default oom kill is enabled on Kubernetes.</p> <p>Its possible however to configure it to disable OOMKill by starting kubelet service with <code>--cgroup-driver=cgroupfs</code> argument and then setting <code>oom_kill_disable</code> under <code>/sys/fs/cgroup/memory/memory.oom_control</code> as 1.</p> <p><code>Takeaway 5</code>: It's not something I want to enable either, but for the completeness of the discussion, it's worth mentioning :). </p> <ol> <li> <p><strong>Kernel memory management</strong></p> <p>The kernel uses virtual addressing (using paging and segmentation) to provide isolation amongst various processes running on host. It is also virtual addressing that allows for use of more memory than what's available currently in physical memory (RAM) by making use of other sources like a disk (a.k.a. swap). Virtual addressing is divided into user &amp; kernel space. Userspace is the sort of virtual address space that's reserved for user/application programs whereas kernel space is reserved for kernel-related operations. </p> <p>Now, the os kernel is designed to be greedy - greedy to be able to run as many processes as possible. This is also the reason why we need mechanisms like `out of memory'.</p> </li> <li> <p><strong>System vs memory controller (memch) OOM</strong></p> </li> </ol> <p>cgroups comprises of two components: <code>core</code> and <code>controller</code>. Core corresponds to managing the hierarchy and core capabilities whereas controllers are focused on the type of resource cgroup is controlling eg cpu, io, memory controller ('memcg'). </p> <div class=highlight><pre><span></span><code>Now, the user-space out-of-memory handling can address OOM conditions for both cgroups using the memory controller (&#39;memcg&#39;) and for the system as a whole.
`Takeaway 6`: We know, based on our takeaways, that our OOM is not stemming from system draining or system as whole. Also, log `Memory cgroup out of memory` indicating that its `memcg`
that&#39;s triggering the OOM Kill. Here, the app process hierarchy memory usage is aggregated together into its memcgs so the memory usage at group level can be accounted for. 
What our first log here is telling us is `memcg usage reached its limits and memory cannot be reclaimed i.e. the memcg is out of memory`&lt;sup&gt;[1][lwn]&lt;/sup&gt;.
</code></pre></div> <ol> <li> <p><strong>OOM kill score</strong></p> <p>How does kernel come to decide which process to kill, is based on a score. The score has two parts: main (<code>oom_score</code>) and adjustment factor (<code>oom_score_adj</code>). These scores are store against process id in process space and can be located on disk as : <div class=highlight><pre><span></span><code>/proc/&lt;pid&gt;/oom_score
/proc/&lt;pid&gt;/oom_score_adj
</code></pre></div></p> <p>The <code>oom_score</code> is given by kernel and is proportional to the amount of memory used by the process i.e. = 10 x percentage of memory used by the process. This means, the maximum <code>oom_score</code> is 100% x 10 = 1000!. Now, the higher the oom_score higher the change of the process being killed. However, user can provide an adjustment factor <code>oom_score_adj (a.k.a. oom_adj in older kernel versions)</code>. If provided, it is used to adjust the final score. The valid value for <code>oom_score_adj</code> is in the range of (-1000, +1000), where -ve score decreases and +ve increases the chances of oomkill. More details on this can be found in this very interesting article by Jonathan Corbet <a href="https://lwn.net/Articles/391222/">another OOMKill rewrite</a>, with precursory article found <a href="https://lwn.net/Articles/317814/">here</a>.</p> </li> <li> <p><strong>OOM trigger workflow</strong></p> </li> </ol> <p><code>kmsg</code> is the kernel message interface that directs kernel messages to <code>/proc/kmsg</code> &amp; <code>/dev/kmsg</code>. Now, <code>/dev/kmsg</code> is more useful for us mere mortals as it's designed to be persistent. <code>/proc/kmsg</code> is designed to be read once and treated more as event queue if you will. Messages from here also trickle through to kernel logs @ <code>/var/log/kern.log</code>.</p> <div class=highlight><pre><span></span><code>_On Kube_

Kebelet watches for `kmsg` and handles messages that will translate to OOMEvent/OOMKillEvent in Kube event stream which is then handled appropriately to trigger OOMKill. More interesting details of how this happens can be found [here][line-eng-qos] (also shown in borrowed fig 11).

![](../../resources/oom/workflow-4-1024x816.png)
*Figure 11: OOM handling workflow on Kubernetes. Image credit: [Line Corp][line-eng-qos]*

As mentioned in `takeway 3 &amp; 4`, this workflow however was not triggered in our case, we are did not record any Kube related OOM events or even kubelet receiving
any related messages.

_At Kernel Level_
When system or memory controller related OOM is suspected, based on `oom_score` (with adjustment `oom_score_adj`), `oom-killer` is invoked on the highest
score process and its children.
</code></pre></div> <h4 id=so-why-the-pods-are-getting-killed_1><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#so-why-the-pods-are-getting-killed_1>So why the pods are getting killed?</a></h4> <p>In my case, memory cgroup ran out of memory and my stack trace confirms this (see fig 12). It tells me that the application container was killed because it was consuming 1.5MB shy of memory set as limit (31457280 KB).</p> <p><img alt src=../../../resources/oom/log-part-1.jpg> <em>Figure 12: Kernel log part 1</em></p> <p>OK! this explains the OOMKill but why:</p> <p>a. My monitoring only shows 29GB as max memory surge!</p> <p>b. I never noticed beyond 9GB usage in local/testing/profiling and all the jazz!</p> <p><img alt src=../../../resources/oom/2efa70f25d30b6e591150bc7a03e76e9-sticker.jpg></p> <p>This simply does not add up! Let's hold on to this thought for a bit and look at the rest of the logs and what it says:</p> <p>Before we go into part 2 of the log, I should explain a few things:</p> <ol> <li> <p>The <em>pause container</em> is the parent container of each pod, responsible for creating and managing the environment for the group of containers that would be provisioned within the pod. For more info, I will direct you to an excellent article by <a href="https://twitter.com/IanMLewis">Ian Lewis</a>, the <a href="https://www.ianlewis.org/en/almighty-pause-container">almighty pause container</a>. I need to explain this because it will be shown in the following log.</p> </li> <li> <p>Definition of memory cgroups stats metrics as per <a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt">kernel.org</a> is listed below. </p> </li> </ol> <p>Note that, <code>anonymous memory</code> (abbreviated often as <code>anon</code>) is a memory mapping with no file or device backing it. Anon memory is used by programs to allocate memory for the stacks and heaps. Also, the standard page size on the Linux kernel is 4KB which can be really inefficient to store mapping for a large block of memory virtual memory. <code>Hugepages</code> are designed to solve this inefficiency and can hold a bigger chunk than 4KB. More details on this is available <a href="https://docs.openshift.com/container-platform/4.1/scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.html">here</a>. </p> <div class=highlight><pre><span></span><code>| Metrics of memory cgroups stats |                                                                                                        Definition                                                                                                        |
| ------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| rss                             | rss stands for resident set size. It is the portion of memory occupied by a process that is held in RAM. This metric represents the number of bytes of anonymous and swap cache memory (includes transparent hugepages). |
| rss_huge                        |                                                                                   number of bytes of anonymous transparent hugepages.                                                                                    |
| cache                           |                                                                                          number of bytes of page cache memory.                                                                                           |
| mapped_file                     |                                                                                number of bytes of the mapped file (includes tmpfs/shmem)                                                                                 |
| swap                            |                                                                                              number of bytes of swap usage                                                                                               |
| dirty                           |                                                                            number of bytes that are waiting to get written back to the disk.                                                                             |
| writeback                       |                                                                         number of bytes of file/anon cache that are queued for syncing to disk.                                                                          |
| inactive_anon                   |                                                                         number of bytes of anonymous and swap cache memory on inactive LRU list.                                                                         |
| active_anon                     |                                                                          number of bytes of anonymous and swap cache memory on active LRU list.                                                                          |
| inactive_file                   |                                                                               number of bytes of file-backed memory on inactive LRU list.                                                                                |
| active_file                     |                                                                                number of bytes of file-backed memory on active LRU list.                                                                                 |
| unevictable                     |                                                                            number of bytes of memory that cannot be reclaimed (mlocked etc).                                                                             |
</code></pre></div> <p>Now, as discussed previously, the swap is not being used in this system. See the second part of the logs in fig 13. You will note, there are two containers recorded and their memory stats is a capture - a) the pause container and b) the app container. We can ignore the pause, it's tiny and looking very healthy. But look at the stats for app pod in fig 13 (below)! At the time my app was killed, it held about 29GB in hugepages and only 1.3GB extra in RSS. That's huge and remember monitoring it not picking it for some reason! It captured 29GB but not 31GB! Perhaps its picking only <code>rss_huge</code> and presenting it as <code>rss</code> erroneously! <code>Â¯\_(ãƒ„)_/Â¯</code>! Yes, we have a problem but this monitoring issue is for another day!</p> <p><img alt src=../../../resources/oom/log-part-2.jpg> <em>Figure 13: Kernel log part 2</em></p> <p>Notice the blue arrow in fig 13, its capturing page info by both the pause container process and app container process. These are page info and not and need to be multiplied by 4KB to get actual memory stats. These are translated two lines below the blue line! </p> <p>My app has freaking <strong><em>62GB</em></strong> in total virtual memory! What's going on! <img alt src=../../../resources/oom/wth.gif></p> <p>Ok, so "total-vm" is the part of virtual memory the process uses. A part of this "total-vm" that's mapped to RAM is <code>rss</code>. Part of <code>rss</code> that's allocated on to real memory, blocks is your <code>anon-rss</code> (anonymous memory), and the other part of rss is mapped to devices and files and termed <code>file-rss</code>. If my app goes crazy and allocates a large chunk of space (say using malloc()) but never really use it then <code>total-vm</code> can be high but it won't all be used in real memory. This is made possible due to overcommit. A good sign of this happening, given swap off, is when <code>total-vm</code> is high but <code>rss</code> is actually low! This is exactly what's happening here! We have about <strong>30GB</strong> difference between <code>total-vm</code> and <code>rss</code>.</p> <p><code>Takeaway 7</code>: We have two problems here: a) Supporting over-commitment and b) Allocation of what we suspect un-needed memory! </p> <p>Let's look at solving the over-commit first and see what level of fixes it provides:</p> <h4 id=controlling-over-commits><a class=toclink href=../../2021/03/14/wth-who-killed-my-pod---whodunit/#controlling-over-commits>Controlling over-commits</a></h4> <p>So far, we have concluded over-commitment is a problem. Well, as discussed previously, it's a feature (of both kernel &amp; kube) apparently!</p> <p><img alt= src="../../../assets/external/memecreator.org/static/images/memes/4777431.jpg"></p> <p>Kernel uses the "extendability" of virtual addressing to over-commit. The kernel settings <code>vm.overcommit_memory</code> and <code>vm.overcommit_ratio</code> is specially designed to controlling this capability. For more info, see <a href="https://engineering.pivotal.io/post/virtual_memory_settings_in_linux_-_the_problem_with_overcommit/">here</a>.</p> <p>1.1 <code>vm.overcommit_memory = 0</code>: Make best guess and overcommit where possible. This is the default.</p> <p>1.2 <code>vm.overcommit_memory = 1</code>: Always overcommit </p> <p>1.3 <code>vm.overcommit_memory = 2</code>: Never overcommit, and only allocate as much memory as defined in overcommit_ratio.</p> <p><code>vm.overcommit_ratio</code> is only used when overcommit_memory=2. It defines what percent of the physical RAM plus swap space should be allocated. This is default to 50. We want this config to be 100. </p> <p>But the use of <code>sysctl</code> to set these(using the following) is not enough as the config won't persist on horizontal scaling (new node spinning due to spot instances or less important but restart): <div class=highlight><pre><span></span><code>sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_memory<span class=o>=</span><span class=m>2</span>
sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_ratio<span class=o>=</span><span class=m>100</span>
</code></pre></div> The effect of these configs is immediate and no start is needed. Talking about the restart, <code>systcl</code> cli config update do not persist, system config needs to be updated in <code>/etc/sysctl.conf</code> to persist the setting across restarts. </p> <p>On <code>Kube</code>, <a href="https://github.com/kubernetes/kops/issues/3251">kops</a> provisioned clusters, these settings need to be supplied through <a href="https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md#sysctlparameters">sysctlparameters</a> config but these are only supported from kube 1.17 and higher! Safe <a href="https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/">sysctl parameters can be set at pod level</a> however our setting is not (obviously) supported at the pod level. One can't use <a href="https://github.com/kubernetes/kops/blob/master/docs/instance_groups.md#additionaluserdata">additionaluserdata</a> for this either, as these settings are overridden when kops provision node as Kube node!</p> <p>And, to make it a helluva fun, this cluster is currently at 1.12! Heya, Mr. Murphy!</p> <p><img alt src=../../../resources/oom/mrmurphy.jpg></p> <p>So, I say our my prayers, and turn to bash: <div class=highlight><pre><span></span><code><span class=k>for</span><span class=w> </span>memip<span class=w> </span><span class=k>in</span><span class=w> </span><span class=k>$(</span>aws<span class=w> </span>ec2<span class=w> </span>describe-instances<span class=w> </span>--region<span class=w> </span>us-east-1<span class=w> </span>--instance-ids<span class=w> </span><span class=se>\</span>
<span class=k>$(</span>aws<span class=w> </span>autoscaling<span class=w> </span>describe-auto-scaling-instances<span class=w> </span>--region<span class=w> </span>us-east-1<span class=w> </span>--output<span class=w> </span>text<span class=w> </span><span class=se>\</span>
--query<span class=w> </span><span class=s2>&quot;AutoScalingInstances[?AutoScalingGroupName==&#39;myasg&#39;].InstanceId&quot;</span><span class=k>)</span><span class=w> </span><span class=se>\</span>
--query<span class=w> </span><span class=s2>&quot;Reservations[].Instances[].PrivateIpAddress&quot;</span><span class=k>)</span>
<span class=k>do</span><span class=w>     </span>
<span class=w>    </span>ssh<span class=w> </span>-o<span class=w> </span><span class=nv>StrictHostKeyChecking</span><span class=o>=</span>no<span class=w>  </span><span class=si>${</span><span class=nv>memip</span><span class=si>}</span><span class=w> </span><span class=s1>&#39;bash -s&#39;</span><span class=w> </span>&lt;<span class=w> </span>set_mem.sh<span class=w>   </span>
<span class=k>done</span><span class=w> </span>
</code></pre></div> where <code>set_mem.sh</code> is: <div class=highlight><pre><span></span><code><span class=ch>#!/usr/bin/env bash</span>
sudo<span class=w> </span>sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_memory<span class=o>=</span><span class=m>2</span>
sudo<span class=w> </span>sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_ratio<span class=o>=</span><span class=m>100</span><span class=w> </span>
</code></pre></div></p> <p>I see a massive improvement in OOMKills. Pods that were killed every 20mins and odd, are chugging along with 24hr processing and no crash still. <img alt src=../../../resources/oom/app-no-crash.jpg> <em>Figure 14: Getting somewhere! OOMKills sort of under control!</em></p> <p>So, perhaps we can upgrade Kube and make this configuration systematic! </p> <blockquote> <p>But, I am not done yet! No no no no no no no .....</p> </blockquote> <p>Remember, part <code>b</code> of our problem in <code>takeaway 7</code> i.e. <code>b) Allocation of what we suspect un-needed memory!</code>.</p> <p>Why was it happening in the first place, and why it's controlled with overcommit disabled. I won't lie, it still happens but far less infrequent! </p> <blockquote> <p>it's not fixed yet!</p> </blockquote> <p>Oh! the fun never ends! All the places we go! I will cover this later, ahem ahem, when I know the answer! Pretty sure it's some nasty behavior of Tensorflow 2, and the investigation is <em>underway</em>!</p> <p><img alt src=../../../resources/oom/4882580.jpg></p> <p>Thanks for reading. Hopefully, it was a fun insightful read!</p> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../2022/ class="md-footer__link md-footer__link--prev" aria-label="Previous: 2022"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> 2022 </div> </div> </a> <a href=../2019/ class="md-footer__link md-footer__link--next" aria-label="Next: 2019"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> 2019 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> </div> <div class=md-social> <a href="https://www.linkedin.com/in/suneeta-mall-a6a0507/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href="https://github.com/suneeta-mall" target="_blank" rel="noopener" title="github.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href="https://x.com/suneetamall/" target="_blank" rel="noopener" title="x.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9L389.2 48zm-24.8 373.8h39.1L151.1 88h-42l255.3 333.8z"/></svg> </a> <a href="https://www.medium.com/@suneetamall" target="_blank" rel="noopener" title="www.medium.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg> </a> <a href="https://scholar.google.com.au/citations?hl=en&amp;user=WD712CUAAAAJ" target="_blank" rel="noopener" title="scholar.google.com.au" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg> </a> <a href="https://www.researchgate.net/profile/Suneeta_Mall3" target="_blank" rel="noopener" title="www.researchgate.net" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6z"/></svg> </a> <a href="https://suneeta-mall.github.io/feed_rss_created.xml" target="_blank" rel="noopener" title="suneeta-mall.github.io" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64zm0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0zm32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32z"/></svg> </a> <a href=mailto:suneetamall@gmail.com target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l217.6 163.2c11.4 8.5 27 8.5 38.4 0l217.6-163.2c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48H48zM0 176v208c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V176L294.4 339.2a63.9 63.9 0 0 1-76.8 0L0 176z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "content.tabs.link", "navigation.indexes", "navigation.instant", "navigation.instant.preview", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "toc.follow", "header.autohide", "announce.dismiss", "navigation.footer", "navigation.breadcrumbs", "navigation.expand", "navigation.sections", "navigation.tracking", "navigation.top", "search.highlight", "search.share", "toc.follow", "toc.integrate", {"git-revision-date-localized": {"enable_creation_date": true, "type": "date"}}], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src=../../../assets/javascripts/bundle.3220b9d7.min.js></script> </body> </html>
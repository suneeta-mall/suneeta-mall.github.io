<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href="https://suneeta-mall.github.io/blog/page/2/" rel="canonical"><link href=../../../projects/KCD/ rel=prev><link href=../../category/ai/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../feed_rss_updated.xml><link rel=icon href=../../../resources/site/favicon.svg><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.18"><title>Blog - Random Musings - Rambling of a curious engineer & data scientist!</title><link rel=stylesheet href=../../../assets/stylesheets/main.66ac8b77.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel="stylesheet" href="../../../assets/external/fonts.googleapis.com/css.49ea35f2.css"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><meta property=og:type content=website><meta property=og:title content="Blog - Random Musings - Rambling of a curious engineer & data scientist!"><meta property=og:description content=None><meta property=og:image content=https://suneeta-mall.github.io/assets/images/social/blog/page/2/index.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://suneeta-mall.github.io/blog/page/2/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Blog - Random Musings - Rambling of a curious engineer & data scientist!"><meta name=twitter:description content=None><meta name=twitter:image content=https://suneeta-mall.github.io/assets/images/social/blog/page/2/index.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#blog class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> <a href=/projects/oreilly_deep_learning_at_scale/ > <strong>ðŸŽ‰ New Book Release!</strong> Check out "Deep Learning at Scale" - An O'Reilly Book </a> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href="https://suneeta-mall.github.io/" title="Random Musings - Rambling of a curious engineer &amp; data scientist!" class="md-header__button md-logo" aria-label="Random Musings - Rambling of a curious engineer &amp; data scientist!" data-md-component="logo"> <img src=../../../resources/site/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Random Musings - Rambling of a curious engineer & data scientist! </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Blog </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3m0-4.5c5 0 9.27 3.11 11 7.5-1.73 4.39-6 7.5-11 7.5S2.73 16.39 1 12c1.73-4.39 6-7.5 11-7.5M3.18 12a9.821 9.821 0 0 0 17.64 0 9.821 9.821 0 0 0-17.64 0Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../projects/oreilly_deep_learning_at_scale/ class=md-tabs__link> Projects </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../../../tags/ class=md-tabs__link> Tags </a> </li> <li class=md-tabs__item> <a href=../../../talks/KGC_NY_2022/ class=md-tabs__link> Talks </a> </li> <li class=md-tabs__item> <a href=../../../poems/singularity/ class=md-tabs__link> Poems </a> </li> <li class=md-tabs__item> <a href=../../../about/ class=md-tabs__link> About Me </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href="https://suneeta-mall.github.io/" title="Random Musings - Rambling of a curious engineer &amp; data scientist!" class="md-nav__button md-logo" aria-label="Random Musings - Rambling of a curious engineer &amp; data scientist!" data-md-component="logo"> <img src=../../../resources/site/logo.svg alt=logo> </a> Random Musings - Rambling of a curious engineer & data scientist! </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../projects/oreilly_deep_learning_at_scale/ class=md-nav__link> <span class=md-ellipsis> Deep Learning at Scale </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/curious_cassie/ class=md-nav__link> <span class=md-ellipsis> Curious Cassie - The Children's Books </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/feature_analysis/ class=md-nav__link> <span class=md-ellipsis> Label Noise with Clean Lab </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/feature_analysis/ class=md-nav__link> <span class=md-ellipsis> Feature Analysis </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/oreilly-interactive-katacode-series-for-reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Oreilly Katacode Series </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Reproducible-ML </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/KCD/ class=md-nav__link> <span class=md-ellipsis> KCD </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/ai/ class=md-nav__link> <span class=md-ellipsis> AI </span> </a> </li> <li class=md-nav__item> <a href=../../category/book/ class=md-nav__link> <span class=md-ellipsis> Book </span> </a> </li> <li class=md-nav__item> <a href=../../category/childrens-books/ class=md-nav__link> <span class=md-ellipsis> Children's Books </span> </a> </li> <li class=md-nav__item> <a href=../../category/confident-learning/ class=md-nav__link> <span class=md-ellipsis> Confident-Learning </span> </a> </li> <li class=md-nav__item> <a href=../../category/curious-cassie/ class=md-nav__link> <span class=md-ellipsis> Curious Cassie </span> </a> </li> <li class=md-nav__item> <a href=../../category/data/ class=md-nav__link> <span class=md-ellipsis> Data </span> </a> </li> <li class=md-nav__item> <a href=../../category/data-centric-ai/ class=md-nav__link> <span class=md-ellipsis> Data-Centric-AI </span> </a> </li> <li class=md-nav__item> <a href=../../category/data-science/ class=md-nav__link> <span class=md-ellipsis> Data-science </span> </a> </li> <li class=md-nav__item> <a href=../../category/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../../category/generative-ai/ class=md-nav__link> <span class=md-ellipsis> Generative AI </span> </a> </li> <li class=md-nav__item> <a href=../../category/kubernetes/ class=md-nav__link> <span class=md-ellipsis> Kubernetes </span> </a> </li> <li class=md-nav__item> <a href=../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../category/machine-learning/ class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=../../category/oom/ class=md-nav__link> <span class=md-ellipsis> OOM </span> </a> </li> <li class=md-nav__item> <a href=../../category/pytorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../category/reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Reproducible-ml </span> </a> </li> <li class=md-nav__item> <a href=../../category/software/ class=md-nav__link> <span class=md-ellipsis> Software </span> </a> </li> <li class=md-nav__item> <a href=../../category/technology/ class=md-nav__link> <span class=md-ellipsis> Technology </span> </a> </li> <li class=md-nav__item> <a href=../../category/umap/ class=md-nav__link> <span class=md-ellipsis> UMAP </span> </a> </li> <li class=md-nav__item> <a href=../../category/t-sne/ class=md-nav__link> <span class=md-ellipsis> t-SNE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Talks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Talks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../talks/KGC_NY_2022/ class=md-nav__link> <span class=md-ellipsis> Knowledge Graph Conference 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubeCon_NA_2021/ class=md-nav__link> <span class=md-ellipsis> KubeCon NA 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/Kafka_Summit_APAC_2021/ class=md-nav__link> <span class=md-ellipsis> Kafka Summit APAC 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/AWS_ANZ_Commuity_day_2020/ class=md-nav__link> <span class=md-ellipsis> AWS Community Day 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/She_Builds_on_AWS_2020/ class=md-nav__link> <span class=md-ellipsis> AWS She Builds on AWS 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubeCon_US_2019/ class=md-nav__link> <span class=md-ellipsis> KubeCon US 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubernetesSydneyForum_AU_2019/ class=md-nav__link> <span class=md-ellipsis> Kubernetes Sydney 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/YOW_Data_Syd_2019/ class=md-nav__link> <span class=md-ellipsis> YOW Data 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/KubeCon-Europe-2018/ class=md-nav__link> <span class=md-ellipsis> KubeCon EU 2018 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/SPIE-2019/ class=md-nav__link> <span class=md-ellipsis> SPIE 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/SPIE-2018/ class=md-nav__link> <span class=md-ellipsis> SPIE 2018 </span> </a> </li> <li class=md-nav__item> <a href=../../../talks/SPIE-2015/ class=md-nav__link> <span class=md-ellipsis> SPIE 2015 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Poems </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Poems </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../poems/singularity/ class=md-nav__link> <span class=md-ellipsis> Singularity </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/life-of-ai-engineer/ class=md-nav__link> <span class=md-ellipsis> Life of AI Engineers </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/my-little-butterfly/ class=md-nav__link> <span class=md-ellipsis> My little Butterfly </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/breaking-thy-bias/ class=md-nav__link> <span class=md-ellipsis> Breaking Thy Bias </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/daminis/ class=md-nav__link> <span class=md-ellipsis> Daminis </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/one-bright-dawn/ class=md-nav__link> <span class=md-ellipsis> One Bright Dawn </span> </a> </li> <li class=md-nav__item> <a href=../../../poems/aint-no-dr-seuss/ class=md-nav__link> <span class=md-ellipsis> Aint no Dr. Seuss </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=blog>Blog<a class=headerlink href=#blog title="Permanent link">#</a></h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-12-23 00:00:00">December 23, 2019</time></li> <li class=md-meta__item> in <a href=../../category/machine-learning/ class=md-meta__link>Machine Learning</a>, <a href=../../category/ai/ class=md-meta__link>AI</a>, <a href=../../category/kubernetes/ class=md-meta__link>Kubernetes</a>, <a href=../../category/reproducible-ml/ class=md-meta__link>Reproducible-ml</a></li> <li class=md-meta__item> 1 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=replicability-an-extension-to-reproducibility><a href=../../2019/12/23/replicability---an-extension-to-reproducibility/ class=toclink>Replicability - an extension to reproducibility</a></h2> <p>This is <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>Part 3</a> - <strong>End-to-end reproducible Machine Learning pipelines on Kubernetes</strong> of technical blog series titled [Reproducibility in Machine Learning]. <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a> &amp; <a href=/2019/12/22/Reproducible-ml-tensorflow.html>Part 2</a> can be found <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>here</a> &amp; <a href=/2019/12/22/Reproducible-ml-tensorflow.html>here</a> respectively.</p> <h3 id=part-1-reproducibility-in-machine-learning-research-and-industry><a class=toclink href=../../2019/12/23/replicability---an-extension-to-reproducibility/#part-1-reproducibility-in-machine-learning-research-and-industry>Part 1: Reproducibility in Machine Learning - Research and Industry</a></h3> <h3 id=replicability-and><a class=toclink href=../../2019/12/23/replicability---an-extension-to-reproducibility/#replicability-and>Replicability and</a></h3> <p>The research community is quite divided when it comes to defining reproducibility and often mixes it up with replicability </p> <p><img alt src=../../../resources/replicable.jpeg> <em>Figure 4: Replicability defined</em></p> <p>[Reproducibility in Machine Learning]: /2019/12/20/Reproducibility-in-Machine Learning.html</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-12-22 00:00:00">December 22, 2019</time></li> <li class=md-meta__item> in <a href=../../category/machine-learning/ class=md-meta__link>Machine Learning</a>, <a href=../../category/ai/ class=md-meta__link>AI</a>, <a href=../../category/reproducible-ml/ class=md-meta__link>Reproducible-ml</a></li> <li class=md-meta__item> 13 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=realizing-reproducible-machine-learning-with-tensorflow><a href=../../2019/12/22/reproducible-ml-with-tensorflow/ class=toclink>Realizing reproducible Machine Learning - with Tensorflow</a></h2> <p>This is <a href=/2019/12/22/Reproducible-ml-tensorflow.html>Part 2</a> - <strong>Realizing reproducible Machine Learning - with Tensorflow</strong> of technical blog series titled [Reproducibility in Machine Learning]. <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a> &amp; <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>Part 3</a> can be found <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>here</a> &amp; <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>here</a> respectively. </p> <hr> <p>As discussed in <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a>, writing reproducible machine learning is not easy with challenges arising from every direction e.g. hardware, software, algorithms, process &amp; practice, data. In this post, we will focus on what is needed to ensure ML code is reproducible.</p> <h3 id=first-things-first><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#first-things-first>First things first</a></h3> <p>There are a few very simple things that are needed to be done before <em>thinking big</em> and focussing on writing reproducible ML code. These are:</p> <ul> <li>Code is version controlled</li> </ul> <blockquote> <p>Same input (data), same process (code) resulting in the same output is the essence of reproducibility. But code keeps evolving, since ML is so iterative. Hence, it's important to version control code (including configuration). This allows obtaining the same i.e. exact code (commit/version) from the source repository. </p> </blockquote> <ul> <li>Reproducible runtime â€“ pinned libraries </li> </ul> <blockquote> <p>So we version-controlled code but what about environment/runtime? Sometimes, non-determinism is introduced not direct by user code but also dependencies. We talked about this at length in the software section of <code>Challenges in realizing reproducible ML</code> in <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a>. Taking the example of Pyproj - a geospatial transform library, that I once used to compute geo-location-based on some parameters. We changed nothing but just a version of pyproj from V1.9.6 to V2.4.0 and suddenly all our calculations were giving different results. The difference was so much that location calculation for San Diego Convention Centre was coming out to be somewhere in Miramar off golf course (see figure 1) <a href="https://github.com/pyproj4/pyproj/issues/470">issue link</a>. Now imagine ordering pizza delivery on the back of my computation snippet backed with an unpinned version of pyproj? </p> </blockquote> <p><img alt src=../../../resources/proj-bug.png> <em>Figure 1: Example of why pinned libraries are important</em></p> <blockquote> <p>Challenges like these occur quite often that we would like. That's why it's important to pin/fix dependent runtime either by pinning version of libraries or using versioned containers (like <a href="https://www.docker.com/">docker</a>). </p> </blockquote> <ul> <li>Smart randomness</li> </ul> <blockquote> <p>As discussed in <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a>, randomization plays a key role in most ML algorithms. Unseeded randomness is the simplest way to make code non-reproducible. It is also one of the easiest to manage amongst all things gotchas of non-reproducible ML. All we need to do is seed all the randomness and manage the seed via configuration (as code or external). </p> </blockquote> <ul> <li>Rounding precision, under-flows &amp; overflow</li> </ul> <blockquote> <p><a href=//docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html>Floating point</a> arithmetic is ubiquitous in ML. The complexity and intensity of floating-point operations (FLOPS) are increasing every day with current needs easily meeting <a href="https://github.com/albanie/convnet-burden">Giga-Flops</a> order of computations. To achieve efficiency in terms of speed despite the complexity, <a href="https://arxiv.org/pdf/1710.03740.pdf">mixed precision</a> floating-point operations have also been proposed. As discussed <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a>, accelerated hardware such as <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">(GPGPU)</a>, tensor processing unit <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">(TPU)</a>, etc. due to their architecture and asynchronous computing do not guarantee reproducibility. Besides, when dealing with floating points, the issues related to <a href="https://journals.sagepub.com/doi/pdf/10.1177/1536867X0800800207">overflow and underflow</a> are expected. This just adds to the complexity. </p> </blockquote> <ul> <li>Dependent library's behavior aware </li> </ul> <blockquote> <p>As discussed in the software section of <code>Challenges in realizing reproducible ML</code> in <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a>, some routines of ML libraries do not guarantee reproducibility. For instance, NVIDIA's CUDA based deep learning library <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility">cudnn</a>. Similarly, with Tensorflow, using some methods may result in non-deterministic behavior. One such example is backward pass of broadcasting on GPU<sup><a href="https://github.com/tensorflow/tensorflow/issues/2652">ref</a></sup>. Awareness about behaviors of libraries being used and approaches to overcome the non-determinism should be explored. </p> </blockquote> <h3 id=writing-reproducible-ml><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#writing-reproducible-ml>Writing reproducible ML</a></h3> <p>To demonstrate reproducible ML, I will be using <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">Oxford Pet dataset</a> that has labels for pet images. I will be doing a semantic segmentation of pets images and will be using pixel-level labeling. Each pixel of the pet image in <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">Oxford Pet dataset</a> is labeled as 1) Foreground (Pet area), 2) Background (not pet area) and 3) Unknown (edges). These labels by definition are mutually exclusive - i.e. a pixel can only be one of the above 3. </p> <blockquote> <p><img alt src=../../../resources/oxford-petset.png> <em>Figure 2: Oxford pet dataset</em></p> </blockquote> <p>I will be using a convolution neural network (ConvNet) for semantic segmentation. The network architecture is based on <a href="https://arxiv.org/abs/1505.04597">U-net</a>. This is similar to standard semantic segmentation example by <a href="https://www.tensorflow.org/tutorials../../resources/segmentation">tensorflow</a>. </p> <blockquote> <p><img alt src=../../../resources/unet.jpg> <em>Fihure 3: <a href="https://arxiv.org/abs/1505.04597">U-net</a> architecture</em></p> </blockquote> <p>The reproducible version of semantic segmentation is available in Github <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s">repository</a>. This example demonstrates reproducible ML and also performing end to end ML with provenance across process and data. </p> <p><img alt src=../../../resources/repo.jpg> <em>Figure 4: Reproducible ML sample - semantic segmentation of oxford pet</em></p> <p>In this post, however, I will be discussing only the reproducible ML aspect of it and will be referencing snippets of this example.</p> <h4 id=ml-workflow><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#ml-workflow>ML workflow</a></h4> <p>In reality, a machine learning workflow is very complex and looks somewhat similar to figure 5. In this post, however, we will only discuss the data and model training part of it. The remaining workflow i.e. the end-to-end workflow will be discussed in <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>next post</a>. </p> <blockquote> <p><img alt src=../../../resources/ai-workflow.jpg> <em>Figure 5: Machine learning workflow</em></p> </blockquote> <h4 id=data><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#data>Data</a></h4> <p>The source dataset is <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">Oxford Pet dataset</a> which contains a multitude of labels e.g. class outcome, pixel-wise label, bounding boxes, etc. The first step is to process this data to generate the trainable dataset. In the example code, this is done by <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/app/download_petset.py">download_petset.py</a> script. </p> <p><div class=highlight><pre><span></span><code>python<span class=w> </span>download_petset.py<span class=w>  </span>--output<span class=w> </span>/wks/petset
</code></pre></div> The resulting sample is shown in figure 6.</p> <blockquote> <p><img alt src=../../../resources/warehouse.png> <em>Figure 6: Pets data partitioned by Pets ID</em></p> </blockquote> <p>Post data partition, the entire dataset is divided into 4 sets: a) training, b) validation, c) calibration, and d) test We would want this set partitioning strategy to be reproducible. By doing this, we ensure that if we have to blow away the training dataset, or if accidental data loss occurs then the <em>exact</em> dataset can be created.</p> <p>In this sample, this is achieved by generating the hash of petid and partitioning the hash into 10 folds (script below) to obtain partition index of pet id. </p> <div class=highlight><pre><span></span><code><span class=n>partition_idx</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>hashlib</span><span class=o>.</span><span class=n>md5</span><span class=p>((</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>basename</span><span class=p>(</span><span class=n>petset_id</span><span class=p>))</span><span class=o>.</span><span class=n>encode</span><span class=p>())</span><span class=o>.</span><span class=n>hexdigest</span><span class=p>(),</span> <span class=mi>16</span><span class=p>)</span> <span class=o>%</span> <span class=mi>10</span>
</code></pre></div> <p>With partition_idx 0-6 assigned for training, 7 for validation, 8 for calibration, and 9 for the test, every generation will result in pets going into their respective partition. </p> <p>Besides, to set partitioning, any random data augmentation performed is seeded with seed controlled as configuration as code. See <code>tf.image.random_flip_left_right</code> used in this tensorflow data pipeline <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/pypkg/pylib/datapipeline.py#L29">method</a>. </p> <p>Script for model dataset preparation is located in <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/app/dataset_gen.py">dataset_gen.py</a> and used as following <div class=highlight><pre><span></span><code>python<span class=w> </span>dataset_gen.py<span class=w> </span>--input<span class=w> </span>/wks/petset<span class=w> </span>--output<span class=w> </span>/wks/model_dataset
</code></pre></div> with results shown below:</p> <blockquote> <p><img alt src=../../../resources/datagen.jpg> <em>Figure 7: Pets data partitioned into training, validation, calibration, and test set</em></p> </blockquote> <h4 id=modelling-semantic-segmentation><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#modelling-semantic-segmentation>Modelling semantic segmentation</a></h4> <p>The model for pet segmentation is based on <a href="https://arxiv.org/abs/1505.04597">U-net</a> with backbone of either <a href="https://arxiv.org/abs/1801.04381">MobileNet-v2</a> or <a href="https://arxiv.org/abs/1409.1556">VGG-19</a> (defaults to VGG-19). As per this model's network architecture, 5 activation layers of pre-trained backbone network are chosen. These layers are: * MobileNet</p> <blockquote> <div class=highlight><pre><span></span><code>&#39;block_1_expand_relu&#39; 
&#39;block_3_expand_relu&#39;  
&#39;block_6_expand_relu&#39; 
&#39;block_13_expand_relu&#39;
&#39;block_16_project&#39;
</code></pre></div> </blockquote> <ul> <li>VGG<blockquote> <div class=highlight><pre><span></span><code>&#39;block1_pool&#39;
&#39;block2_pool&#39;
&#39;block3_pool&#39;
&#39;block4_pool&#39;
&#39;block5_pool&#39;
</code></pre></div> </blockquote> </li> </ul> <p>Each of these layers is then concatenated with a corresponding upsampling layer comprising of <a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">Conv2DTranspose</a> layer forming what's known as skip connection. See <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/pypkg/pylib/model.py#L40">model</a> code for more info.</p> <p>The training script <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/app/train.py">train.py</a> can be used as following:</p> <div class=highlight><pre><span></span><code>python<span class=w> </span>train.py<span class=w> </span>--input<span class=w> </span>/wks/model_dataset<span class=w> </span>--output<span class=w> </span>/wks/model<span class=w> </span>--checkpoint_path<span class=w> </span>/wks/model/ckpts<span class=w> </span>--tensorboard_path<span class=w> </span>/wks/model
</code></pre></div> <h5 id=1-seeding-randomness><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#1-seeding-randomness>1. Seeding randomness</a></h5> <p>All methods exploiting randomness is used with appropriate seed.<br> - All random initialization is seeded e.g. <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/pypkg/pylib/model.py#L27">tf.random_normal_initializer</a> - All dropout layers are seeded e.g. <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/pypkg/pylib/model.py#L35">dropout</a></p> <p>Global seed is set for any hidden methods that may be using randomness by <a href="https://github.com/suneeta-mall/e2e-ml-on-k8s/blob/master/pypkg/pylib/model.py#L91">calling</a> in <code>set_seed(seed)</code> which sets seed for used libraries:</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>set_seeds</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=n>SEED</span><span class=p>):</span>
    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;PYTHONHASHSEED&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=nb>str</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
    <span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
    <span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</code></pre></div> <h5 id=2-handing-library-behaviors><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#2-handing-library-behaviors>2. Handing library behaviors</a></h5> <h6 id=21-cudnn><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#21-cudnn>2.1 CuDNN</a></h6> <p><a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility">CuDNN</a> does not guarantee reproducibility in some of its routines. Environment variable <code>TF_DETERMINISTIC_OPS</code> &amp; <code>TF_CUDNN_DETERMINISTIC</code> can be used to control this behavior as per this snippet (figure 8) from cudnn <a href="https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_19.06.html">release page</a>. </p> <blockquote> <p><img alt src=../../../resources/envvar.jpg> <em>Figure 8: NVIDIA release page snippet reproducibility</em></p> </blockquote> <h5 id=22-cpu-thread-parallelism><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#22-cpu-thread-parallelism>2.2 CPU thread parallelism</a></h5> <p>As discussed in <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a>, using high parallelism with compute intensive workflow may not be reproducible. Configurations for inter<sup><a href="https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads">ref</a></sup> and intra<sup><a href="https://www.tensorflow.org/api_docs/python/tf/config/threading/set_intra_op_parallelism_threads">ref</a></sup> operation parallelism should be set if 100% parallelism is desired.</p> <p>In this example, I have chosen <em>1</em> to avoid any non-determinism arising from inter-operation parallelism. <em>Warning</em> setting this will considerably slow down the training. </p> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>threading</span><span class=o>.</span><span class=n>set_inter_op_parallelism_threads</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>threading</span><span class=o>.</span><span class=n>set_intra_op_parallelism_threads</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</code></pre></div> <h5 id=23-tensorflow-determinism><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#23-tensorflow-determinism>2.3 Tensorflow determinism</a></h5> <p>Following are some of the application of <a href="https://tensorflow.org">Tensorflow</a> that are not reproducible: - Backward pass of broadcasting on GPU is non-deterministic<sup><a href="https://github.com/tensorflow/tensorflow/issues/2652">link</a></sup> - Mention that GPU reductions are nondeterministic in docs<sup><a href="https://github.com/tensorflow/tensorflow/issues/2732">link</a></sup> - Problems Getting TensorFlow to behave Deterministically<sup><a href="https://github.com/tensorflow/tensorflow/issues/16889">link</a></sup></p> <p><a href="https://github.com/duncanriach">Duncan Riach</a>, along with several other contributors have created <a href="https://github.com/NVIDIA/tensorflow-determinism">tensorflow_determinism</a> package that can be used to overcome non-reproducibility related challenges from TensorFlow. It should be used in addition to the above measures we have discussed so far.</p> <p>If we combine all the approaches discussed above (aside from using seeded randomness), they can be wrapped into a lightweight method like one below:<br> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>set_global_determinism</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=n>SEED</span><span class=p>,</span> <span class=n>fast_n_close</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Enable 100% reproducibility on operations related to tensor and randomness.</span>
<span class=sd>        Parameters:</span>
<span class=sd>        seed (int): seed value for global randomness</span>
<span class=sd>        fast_n_close (bool): whether to achieve efficient at the cost of determinism/reproducibility</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=n>set_seeds</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=n>seed</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>fast_n_close</span><span class=p>:</span>
        <span class=k>return</span>

    <span class=n>logging</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&quot;*******************************************************************************&quot;</span><span class=p>)</span>
    <span class=n>logging</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&quot;*** set_global_determinism is called,setting full determinism, will be slow ***&quot;</span><span class=p>)</span>
    <span class=n>logging</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&quot;*******************************************************************************&quot;</span><span class=p>)</span>

    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;TF_DETERMINISTIC_OPS&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;1&#39;</span>
    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;TF_CUDNN_DETERMINISTIC&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;1&#39;</span>
    <span class=c1># https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads</span>
    <span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>threading</span><span class=o>.</span><span class=n>set_inter_op_parallelism_threads</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>threading</span><span class=o>.</span><span class=n>set_intra_op_parallelism_threads</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
    <span class=kn>from</span><span class=w> </span><span class=nn>tfdeterminism</span><span class=w> </span><span class=kn>import</span> <span class=n>patch</span>
    <span class=n>patch</span><span class=p>()</span>
</code></pre></div> which can then be used on top of the ML algorithm/process code to generate a 100% reproducible code. </p> <h4 id=result><a class=toclink href=../../2019/12/22/reproducible-ml-with-tensorflow/#result>Result</a></h4> <p>What happens if we don't write reproducible ML? What kind of difference we are really talking about? The last two columns of figure 9 show results obtained by a model trained on the exact same dataset, exactly the same code with EXACTLY one exception. The dropout layer used in the network were unseeded. All other measures discussed above were taken into account. </p> <blockquote> <p><img alt src=../../../resources/repro-ml-result.jpg> <em>Figure 9: Effect of just forgetting to set one seed amidst many</em></p> </blockquote> <p>Looking at the result of the first pet which is a very simple case, we can see the subtle difference in the outcome of these two models. The second pet case is slightly complicated due to shadow and we can see obvious differences in the outcome. But what about the third case, this is a very hard case for pre-trained frozen backbone model we are using but we can see major differences in result between the two models.</p> <p>If we were to use all the measures discussed above then 100% reproducible ML can be obatined. This is shown in the following 2 logs obtained by running the following: <div class=highlight><pre><span></span><code>python<span class=w> </span>train.py<span class=w> </span><span class=se>\</span>
<span class=w>  </span>--input<span class=w> </span>/wks/model_dataset<span class=w> </span><span class=se>\</span>
<span class=w>  </span>--hyperparam_fn_path<span class=w> </span>best_hyper_parameters.json<span class=w> </span><span class=se>\</span>
<span class=w>  </span>--output<span class=w> </span>logs<span class=w> </span><span class=se>\</span>
<span class=w>  </span>--checkpoint_path<span class=w> </span><span class=s2>&quot;logs/ckpts&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>--tensorboard_path<span class=w> </span>logs
</code></pre></div> wherein content of <code>best_hyper_parameters.json</code> are: <div class=highlight><pre><span></span><code><span class=p>{</span>
<span class=w>   </span><span class=nt>&quot;batch_size&quot;</span><span class=p>:</span><span class=mi>60</span><span class=p>,</span>
<span class=w>   </span><span class=nt>&quot;epochs&quot;</span><span class=p>:</span><span class=mi>12</span><span class=p>,</span>
<span class=w>   </span><span class=nt>&quot;iterations&quot;</span><span class=p>:</span><span class=mi>100</span><span class=p>,</span>
<span class=w>   </span><span class=nt>&quot;learning_rate&quot;</span><span class=p>:</span><span class=mf>0.0018464290366223407</span><span class=p>,</span>
<span class=w>   </span><span class=nt>&quot;model_arch&quot;</span><span class=p>:</span><span class=s2>&quot;MobileNetV2&quot;</span><span class=p>,</span>
<span class=w>   </span><span class=nt>&quot;steps_per_epoch&quot;</span><span class=p>:</span><span class=mi>84</span>
<span class=p>}</span>
</code></pre></div> Run attempt 1: <div class=highlight><pre><span></span><code>WARNING:root:******* set_global_determinism is called, setting seeds and determinism *******
TensorFlow version 2.0.0 has been patched using tfdeterminism version 0.3.0
Input: tf-data, Model: MobileNetV2, Batch Size: 60, Epochs: 12, Learning Rate: 0.0018464290366223407, Steps Per Epoch: 84
Train for 84 steps, validate for 14 steps
Epoch 1/12
2019-11-07 12:48:27.576286: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.
84/84 [==============================] - 505s 6s/step - loss: 0.8187 - iou_score: 0.4867 - f1_score: 0.6092 - binary_accuracy: 0.8749 - val_loss: 1.0283 - val_iou_score: 0.4910 - val_f1_score: 0.6297 - val_binary_accuracy: 0.8393
Epoch 2/12
84/84 [==============================] - 533s 6s/step - loss: 0.6116 - iou_score: 0.6118 - f1_score: 0.7309 - binary_accuracy: 0.9150 - val_loss: 0.6965 - val_iou_score: 0.5817 - val_f1_score: 0.7079 - val_binary_accuracy: 0.8940
Epoch 3/12
84/84 [==============================] - 527s 6s/step - loss: 0.5829 - iou_score: 0.6301 - f1_score: 0.7466 - binary_accuracy: 0.9197 - val_loss: 0.6354 - val_iou_score: 0.6107 - val_f1_score: 0.7312 - val_binary_accuracy: 0.9038
Epoch 4/12
84/84 [==============================] - 503s 6s/step - loss: 0.5733 - iou_score: 0.6376 - f1_score: 0.7528 - binary_accuracy: 0.9213 - val_loss: 0.6192 - val_iou_score: 0.6227 - val_f1_score: 0.7411 - val_binary_accuracy: 0.9066
Epoch 5/12
84/84 [==============================] - 484s 6s/step - loss: 0.5566 - iou_score: 0.6461 - f1_score: 0.7599 - binary_accuracy: 0.9241 - val_loss: 0.5827 - val_iou_score: 0.6381 - val_f1_score: 0.7534 - val_binary_accuracy: 0.9156
Epoch 6/12
84/84 [==============================] - 509s 6s/step - loss: 0.5524 - iou_score: 0.6497 - f1_score: 0.7629 - binary_accuracy: 0.9247 - val_loss: 0.5732 - val_iou_score: 0.6477 - val_f1_score: 0.7605 - val_binary_accuracy: 0.9191
Epoch 7/12
84/84 [==============================] - 526s 6s/step - loss: 0.5439 - iou_score: 0.6544 - f1_score: 0.7669 - binary_accuracy: 0.9262 - val_loss: 0.5774 - val_iou_score: 0.6456 - val_f1_score: 0.7590 - val_binary_accuracy: 0.9170
Epoch 8/12
84/84 [==============================] - 523s 6s/step - loss: 0.5339 - iou_score: 0.6597 - f1_score: 0.7710 - binary_accuracy: 0.9279 - val_loss: 0.5533 - val_iou_score: 0.6554 - val_f1_score: 0.7672 - val_binary_accuracy: 0.9216
Epoch 9/12
84/84 [==============================] - 518s 6s/step - loss: 0.5287 - iou_score: 0.6620 - f1_score: 0.7730 - binary_accuracy: 0.9288 - val_loss: 0.5919 - val_iou_score: 0.6444 - val_f1_score: 0.7584 - val_binary_accuracy: 0.9148
Epoch 10/12
84/84 [==============================] - 506s 6s/step - loss: 0.5259 - iou_score: 0.6649 - f1_score: 0.7753 - binary_accuracy: 0.9292 - val_loss: 0.5532 - val_iou_score: 0.6554 - val_f1_score: 0.7674 - val_binary_accuracy: 0.9218
Epoch 11/12
84/84 [==============================] - 521s 6s/step - loss: 0.5146 - iou_score: 0.6695 - f1_score: 0.7789 - binary_accuracy: 0.9313 - val_loss: 0.5586 - val_iou_score: 0.6581 - val_f1_score: 0.7689 - val_binary_accuracy: 0.9221
Epoch 12/12
84/84 [==============================] - 507s 6s/step - loss: 0.5114 - iou_score: 0.6730 - f1_score: 0.7818 - binary_accuracy: 0.9317 - val_loss: 0.5732 - val_iou_score: 0.6501 - val_f1_score: 0.7626 - val_binary_accuracy: 0.9179
</code></pre></div></p> <p>Run attempt 2: <div class=highlight><pre><span></span><code>WARNING:root:******* set_global_determinism is called, setting seeds and determinism *******
TensorFlow version 2.0.0 has been patched using tfdeterminism version 0.3.0
Input: tf-data, Model: MobileNetV2, Batch Size: 60, Epochs: 12, Learning Rate: 0.0018464290366223407, Steps Per Epoch: 84
Train for 84 steps, validate for 14 steps

Epoch 1/12
2019-11-07 10:45:51.549715: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.
84/84 [==============================] - 549s 7s/step - loss: 0.8187 - iou_score: 0.4867 - f1_score: 0.6092 - binary_accuracy: 0.8749 - val_loss: 1.0283 - val_iou_score: 0.4910 - val_f1_score: 0.6297 - val_binary_accuracy: 0.8393
Epoch 2/12
84/84 [==============================] - 515s 6s/step - loss: 0.6116 - iou_score: 0.6118 - f1_score: 0.7309 - binary_accuracy: 0.9150 - val_loss: 0.6965 - val_iou_score: 0.5817 - val_f1_score: 0.7079 - val_binary_accuracy: 0.8940
Epoch 3/12
84/84 [==============================] - 492s 6s/step - loss: 0.5829 - iou_score: 0.6301 - f1_score: 0.7466 - binary_accuracy: 0.9197 - val_loss: 0.6354 - val_iou_score: 0.6107 - val_f1_score: 0.7312 - val_binary_accuracy: 0.9038
Epoch 4/12
84/84 [==============================] - 515s 6s/step - loss: 0.5733 - iou_score: 0.6376 - f1_score: 0.7528 - binary_accuracy: 0.9213 - val_loss: 0.6192 - val_iou_score: 0.6227 - val_f1_score: 0.7411 - val_binary_accuracy: 0.9066
Epoch 5/12
84/84 [==============================] - 534s 6s/step - loss: 0.5566 - iou_score: 0.6461 - f1_score: 0.7599 - binary_accuracy: 0.9241 - val_loss: 0.5827 - val_iou_score: 0.6381 - val_f1_score: 0.7534 - val_binary_accuracy: 0.9156
Epoch 6/12
84/84 [==============================] - 494s 6s/step - loss: 0.5524 - iou_score: 0.6497 - f1_score: 0.7629 - binary_accuracy: 0.9247 - val_loss: 0.5732 - val_iou_score: 0.6477 - val_f1_score: 0.7605 - val_binary_accuracy: 0.9191
Epoch 7/12
84/84 [==============================] - 506s 6s/step - loss: 0.5439 - iou_score: 0.6544 - f1_score: 0.7669 - binary_accuracy: 0.9262 - val_loss: 0.5774 - val_iou_score: 0.6456 - val_f1_score: 0.7590 - val_binary_accuracy: 0.9170
Epoch 8/12
84/84 [==============================] - 514s 6s/step - loss: 0.5339 - iou_score: 0.6597 - f1_score: 0.7710 - binary_accuracy: 0.9279 - val_loss: 0.5533 - val_iou_score: 0.6554 - val_f1_score: 0.7672 - val_binary_accuracy: 0.9216
Epoch 9/12
84/84 [==============================] - 518s 6s/step - loss: 0.5287 - iou_score: 0.6620 - f1_score: 0.7730 - binary_accuracy: 0.9288 - val_loss: 0.5919 - val_iou_score: 0.6444 - val_f1_score: 0.7584 - val_binary_accuracy: 0.9148
Epoch 10/12
84/84 [==============================] - 531s 6s/step - loss: 0.5259 - iou_score: 0.6649 - f1_score: 0.7753 - binary_accuracy: 0.9292 - val_loss: 0.5532 - val_iou_score: 0.6554 - val_f1_score: 0.7674 - val_binary_accuracy: 0.9218
Epoch 11/12
84/84 [==============================] - 495s 6s/step - loss: 0.5146 - iou_score: 0.6695 - f1_score: 0.7789 - binary_accuracy: 0.9313 - val_loss: 0.5586 - val_iou_score: 0.6581 - val_f1_score: 0.7689 - val_binary_accuracy: 0.9221
Epoch 12/12
84/84 [==============================] - 483s 6s/step - loss: 0.5114 - iou_score: 0.6730 - f1_score: 0.7818 - binary_accuracy: 0.9317 - val_loss: 0.5732 - val_iou_score: 0.6501 - val_f1_score: 0.7626 - val_binary_accuracy: 0.9179
</code></pre></div></p> <p>So we have 100% reproducible ML code now but saying <strong>training is snail-ish is an understatement</strong>. Training time has increased (CPU based measures) from 28 minutes vs 1 hr 45 minutes as we give away with inter-thread parallelism and also asynchronous computation optimization. This is not practical in reality. This is also why reproducibility in ML is more focussed around <em>having a road map to reach the same conclusions<sub>- <a href="https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/">Dodge</a></sub></em>. This is realized by maintaining a system capable of capturing full provenance over everything involved in the ML process including data, code, processes, and infrastructure/environment. This will be the focus of <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>part 3</a> of this blog series. </p> <hr> <p>The next part of the technical blog series, [Reproducibility in Machine Learning], is <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>End-to-end reproducible Machine Learning pipelines on Kubernetes</a>. </p> <p>[Reproducibility in Machine Learning]: /2019/12/20/Reproducibility-in-Machine Learning.html</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-12-21 00:00:00">December 21, 2019</time></li> <li class=md-meta__item> in <a href=../../category/machine-learning/ class=md-meta__link>Machine Learning</a>, <a href=../../category/ai/ class=md-meta__link>AI</a>, <a href=../../category/reproducible-ml/ class=md-meta__link>Reproducible-ml</a></li> <li class=md-meta__item> 12 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=reproducibility-in-machine-learning-research-and-industry><a href=../../2019/12/21/reproducible-ml-research-in-industry/ class=toclink>Reproducibility in Machine Learning - Research and Industry</a></h2> <p>This is <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a> - <strong>Reproducibility in Machine Learning - Research and Industry</strong> of technical blog series titled [Reproducibility in Machine Learning]. <a href=/2019/12/22/Reproducible-ml-tensorflow.html>Part 2</a> &amp; <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>Part 3</a> can be found <a href=/2019/12/22/Reproducible-ml-tensorflow.html>here</a> &amp; <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>here</a> respectively. </p> <hr> <p>Machine learning (ML) is an interesting field aimed at solving problems that can not be solved by applying deterministic logic. In fact, ML solves problems in logits [0, 1] with probabilities! ML is a highly iterative and fiddly field with much of its <strong><em>intelligence</em></strong> derived from data upon application of complex mathematics. Sometimes, even a slight change such as changing the order of input/data can change the outcome of ML processes drastically. Actually <a href="https://xkcd.com/1838">xkcd</a> quite aptly puts it:</p> <blockquote> <p><img alt src=../../../resources/xkcd_1838.png> <em>Figure 1: Machine Learning explained by XKCD</em></p> </blockquote> <p>This phenomenon is explained as <strong>C</strong>hange <strong>A</strong>nything <strong>C</strong>hanges <strong>E</strong>verything a.k.a. CAKE principle coined by Scully et. <em>al</em> in their NIPS 2015 paper titled ["<em>Hidden Technical Debt in Machine Learning Systems</em>"][scully_2015]. CAKE principle highlights that in ML - no input is ever really independent. </p> <h3 id=what-is-reproducibility-in-ml><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#what-is-reproducibility-in-ml>What is reproducibility in ML</a></h3> <p>Reproducibility as per the Oxford dictionary is defined as something that can be <em>produced again in the same way</em>. <img alt src=../../../resources/reproducible-oxford.jpeg> <em>Figure 2: Reproducible defined</em></p> <p>In ML context, it relates to getting the same output on the same algorithm, (hyper)parameters, and data on every run. </p> <p>To demonstrate, let's take a simple linear regression example (shown below) on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html">Scikit Diabetes Dataset</a>. Linear regression is all about fitting a line i.e. <code>Y = a + bX</code> over data-points represented as X, with b being the slope and a being the intercept. <br> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn</span><span class=w> </span><span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>linear_model</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>r2_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>

<span class=n>diabetes</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_diabetes</span><span class=p>()</span>    
<span class=n>diabetes_X</span> <span class=o>=</span> <span class=n>diabetes</span><span class=o>.</span><span class=n>data</span><span class=p>[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>,</span> <span class=mi>9</span><span class=p>]</span>
<span class=n>xtrain</span><span class=p>,</span> <span class=n>xtest</span><span class=p>,</span> <span class=n>ytrain</span><span class=p>,</span> <span class=n>ytest</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>diabetes_X</span><span class=p>,</span> <span class=n>diabetes</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.33</span><span class=p>)</span>
<span class=n>regr</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>regr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>xtrain</span><span class=p>,</span> <span class=n>ytrain</span><span class=p>)</span>
<span class=n>diabetes_y_pred</span> <span class=o>=</span> <span class=n>regr</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xtest</span><span class=p>)</span>

<span class=c1># The coefficients </span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Coefficients: </span><span class=si>{</span><span class=n>regr</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s1>&#39;</span>
      <span class=sa>f</span><span class=s1>&#39;Mean squared error: </span><span class=si>{</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>ytest</span><span class=p>,</span><span class=w> </span><span class=n>diabetes_y_pred</span><span class=p>)</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=se>\n</span><span class=s1>&#39;</span>
      <span class=sa>f</span><span class=s1>&#39;Variance score: </span><span class=si>{</span><span class=n>r2_score</span><span class=p>(</span><span class=n>ytest</span><span class=p>,</span><span class=w> </span><span class=n>diabetes_y_pred</span><span class=p>)</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=c1># Plot outputs </span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>xtest</span><span class=p>,</span> <span class=n>ytest</span><span class=p>,</span>  <span class=n>color</span><span class=o>=</span><span class=s1>&#39;green&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>xtest</span><span class=p>,</span> <span class=n>diabetes_y_pred</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Quantitative measure of diabetes progression&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;One of six blood serum measurements of patients&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <em>A linear regression example on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html">Scikit Diabetes Dataset</a></em></p> <p>Above ML code is NOT reproducible. Every run will give different results: <strong>a)</strong> The data distribution will vary and <strong>b)</strong> Obtained slop and intercept will vary. See Figure 3.</p> <p><img alt src=../../../resources/scikit-repro.jpeg> <em>Figure 3: Repeated run of above linear regression code produces different results</em> </p> <p>In the above example, we are using the same dataset, same algorithm, same hyper-parameters. So why are we getting different results? Here the method <code>train_test_split</code> splits the diabetes dataset into training and test but while doing so, it performs a random shuffle of the dataset. The seed for this random shuffle is not set here. Because of this, every run produces different training dataset distribution. Due to this, the regression line slope and intercept ends up being different. In this simple example, if we were to set a random state for method <code>train_test_split</code> e.g. <code>random_state=42</code> then we will have a reproducible regression example over the diabetes dataset. The reproducible version of the above regression example is as follows:</p> <p><div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn</span><span class=w> </span><span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>linear_model</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>r2_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>

<span class=n>diabetes</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_diabetes</span><span class=p>()</span>    
<span class=n>diabetes_X</span> <span class=o>=</span> <span class=n>diabetes</span><span class=o>.</span><span class=n>data</span><span class=p>[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>,</span> <span class=mi>9</span><span class=p>]</span>
<span class=n>xtrain</span><span class=p>,</span> <span class=n>xtest</span><span class=p>,</span> <span class=n>ytrain</span><span class=p>,</span> <span class=n>ytest</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>diabetes_X</span><span class=p>,</span> <span class=n>diabetes</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.33</span><span class=p>,</span>
                                                <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>regr</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>regr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>xtrain</span><span class=p>,</span> <span class=n>ytrain</span><span class=p>)</span>
<span class=n>diabetes_y_pred</span> <span class=o>=</span> <span class=n>regr</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xtest</span><span class=p>)</span>

<span class=c1># The coefficients </span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Coefficients: </span><span class=si>{</span><span class=n>regr</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s1>&#39;</span>
      <span class=sa>f</span><span class=s1>&#39;Mean squared error: </span><span class=si>{</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>ytest</span><span class=p>,</span><span class=w> </span><span class=n>diabetes_y_pred</span><span class=p>)</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=se>\n</span><span class=s1>&#39;</span>
      <span class=sa>f</span><span class=s1>&#39;Variance score: </span><span class=si>{</span><span class=n>r2_score</span><span class=p>(</span><span class=n>ytest</span><span class=p>,</span><span class=w> </span><span class=n>diabetes_y_pred</span><span class=p>)</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=c1># Plot outputs </span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>xtest</span><span class=p>,</span> <span class=n>ytest</span><span class=p>,</span>  <span class=n>color</span><span class=o>=</span><span class=s1>&#39;green&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>xtest</span><span class=p>,</span> <span class=n>diabetes_y_pred</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Quantitative measure of diabetes progression&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;One of six blood serum measurements of patients&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <em>A reproducible linear regression example on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html">Scikit Diabetes Dataset</a></em></p> <p>Seeding random state is not the only challenge in writing reproducible ML. In fact, there are several reasons why reproducibility in ML is so hard to achieve. But I will go into that a bit later in section <code>Challenges in realizing reproducible ML</code>. The first question should be "why reproducibility matters in ML"? </p> <h3 id=importance-of-reproducibility-in-ml><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#importance-of-reproducibility-in-ml>Importance of reproducibility in ML</a></h3> <blockquote> <p>Non-reproducible single occurrences are of no significance to science. <sub>- Popper (The Logic of Scientific Discovery)</sub></p> </blockquote> <p>The importance of reproducibility is increasingly getting recognized since <a href="https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970">Nature's Survey (2016)</a> reported a <code>reproducibility crisis</code>. As per this survey report, 70% of researchers have failed to reproduce another scientist's experiments, and more than 50% have failed to reproduce their own experiments. With more than half of participating scientist agreeing to the presence of reproducibility crisis is indeed very real. Dr. Joelle Pineau, an Associate Professor at McGill University and lead for Facebook's Artificial Intelligence Research lab, covered the reproducibility crisis in her talk at International Conference on Learning Representations (ICLR) 2018 <a href="https://www.youtube.com/watch?v=Vh4H0gOwdIg">you tube</a>. She is determined to nip<br> this crisis in bud from AI research<sup><a href="https://www.nature.com/articles/d41586-019-03895-5?utm_source=twt_nnc&amp;utm_medium=social&amp;utm_campaign=naturenews&amp;sf226569864=1">src</a></sup>. It's not just her, several AI research groups are coming up with measures to ensure reproducibility (example below): - <a href="https://arxiv.org/abs/1810.03993">Model Card</a> at Google - <a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">Reproducibility Checklist</a> at NeurIPS - <a href="https://reproducibility-challenge.github.io/iclr_2019/">ICLR Reproducibility Challenge</a> at ICLR - <a href="https://arxiv.org/abs/1909.03004">Show your work</a> at Allen Institute of Artificial Intelligence</p> <p>Aside from being of <code>no use if can't be reproduced</code>, as Popper suggested in the above quote, why does reproducibility matter?</p> <h4 id=1-understanding-explaining-debugging-and-reverse-engineering><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#1-understanding-explaining-debugging-and-reverse-engineering>1. Understanding, Explaining, Debugging, and Reverse Engineering</a></h4> <p>Reproducibility helps with <strong><em>understanding, explaining, and debugging</em></strong>. Reproducibility is also a crucial means to <strong><em>reverse engineering</em></strong>.</p> <p>Machine learning is inherently difficult to explain, understand, and also debug. Obtaining different output on the subsequent run just makes this whole understanding, explaining, debugging thing all the more challenging. How do we ever reverse engineer? As it is, understanding and explaining are hard with machine learning. It's increasingly harder with deep learning. For over a decade, researches are have been trying to understand what these deep networks learn and yet have not 100% succeeded in doing so. <img alt src=../../../resources/deep-net-understand.jpeg> </p> <p>From visualizing higher layer features of deep networks <a href="https://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/247">year 2009</a> to activation-atlases i.e. what individual neurons in deep network do <a href="https://distill.pub/2017/feature-visualization">year 2017</a> to understanding how deep networks decides <a href="https://distill.pub/2018/building-blocks/">year 2018</a> - are all ongoing progressive efforts towards understanding. Meanwhile, explainability has morphed into a dedicated field 'Explainable Artificial Intelligence <a href="https://arxiv.org/abs/1910.10045">XAI</a>. </p> <h4 id=2-correctness><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#2-correctness>2. Correctness</a></h4> <blockquote> <p>If anything can go wrong, it will <sub>-<a href="https://en.wikipedia.org/wiki/Murphy%27s_law">Murphy's law</a></sub></p> </blockquote> <p>Correctness is important as <a href="https://en.wikipedia.org/wiki/Murphy%27s_law">Murphy's law</a> rarely fails us. These are some of the examples of great AI failures of our times.<br> <img alt src=../../../resources/AI-failure.jpeg> <em>Figure 4: Example of some of the great AI failures of our times</em></p> <p>Google Photos launched AI capabilities with automatically tagging images. It was found to be tagging <a href="https://twitter.com/jackyalcine/status/615329515909156865">people of dark skin as gorillas</a>. Amazon's recruiting software exhibiting <a href="https://medium.com/syncedreview/2018-in-review-10-ai-failures-c18faadf5983">gender bias</a> or even IBM's Watson giving unsafe recommendations for <a href="https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science">cancer treatment</a>. </p> <p>ML output should be correct in addition to being explainable. Reproducibility helps to achieve correctness through understanding and debugging. </p> <h4 id=3-credibility><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#3-credibility>3. Credibility</a></h4> <p>ML output must be credible. It's not just from a fairness, ethical viewpoint but also because they sometimes impact lives (e.g. mortgage approval). Also, end-users of ML output expect answers to verifiable, reliable, unbiased, and ethical. As Lecun said in his <a href="https://twitter.com/ylecun/status/1097532314614034433">International Solid State Circuit Conference in San Francisco, 2019</a> keynote:</p> <blockquote> <p>Good results are not enough, Making them easily reproducible also makes them credible. <sub>- Lecun, ISSCC 2019</sub></p> </blockquote> <h4 id=4-extensibility><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#4-extensibility>4. Extensibility</a></h4> <p>Reproducibility in preceding layers is needed to build out and extend. Can we build a building outline model if we cant repeatedly generate roof semantics as shown in figure 5? What if we keep getting the different size for the same roof? <img alt src=../../../resources/extensibility.jpeg> <em>Figure 5: Extending ML</em></p> <p>Extensibility is essential to utilizing ML outputs for consumption. As it is, raw outputs from ML are rarely usable by end-user. Most ML outputs need to be post-processed and augmented to be consumption ready. </p> <h4 id=4-data-harvesting><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#4-data-harvesting>4. Data harvesting</a></h4> <blockquote> <p>The world's most valuable resource is no longer oil, but data! <sub>- <a href="https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data">economist.com</a></sub></p> </blockquote> <p>To train a successful ML algorithm large dataset is mostly needed - this is especially true for deep-learning. Obtaining large volumes of training data, however, is not always easy - it can be quite expensive. In some cases the occurrences of the scenario can be so rare that obtaining a large dataset will either take forever or is simply not possible. For eg. dataset for <a href="https://www.cancer.gov/types/skin/patient/merkel-cell-treatment-pdq">Merkel-cell carcinoma</a>, a type of skin cancer that's very rare, will be very challenging to procure.</p> <p>For this reason, data harvesting a.k.a. synthetic data generation is considered. Tirthajyoti Sarkar, the author of <a href="https://www.amazon.com.au/Data-Wrangling-Python-Creating-actionable-ebook/dp/B07JF26NGJ">Data Wrangling with Python: Creating actionable data from raw sources</a>, wrote an excellent post on <a href="https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae">data harvesting</a> using scikit that covers this topic in detail. However, more recently, Generative Adversarial Networks <a href="https://arxiv.org/abs/1406.2661">(GAN)</a> by <a href="https://en.wikipedia.org/wiki/Ian_Goodfellow">Ian Goodfellow</a> is being heavily used for this purpose. <a href="https://arxiv.org/abs/1909.11512">Synthetic Data for Deep Learning</a> is an excellent review article that covers this topic in detail for deep-learning.</p> <p>Give ML models e.g. <a href="https://arxiv.org/abs/1406.2661">(GAN)</a> are being used to generate training data now, it's all the more important that reproducibility in such application is ensured. Let's say, we trained a near-perfect golden goose model on data (including some synthetic). But the storage caught proverbial fire, and we lost this golden goose model along with data. Now, we have to regenerate the synthetic data and obtain the same model but the synthetic data generation process is not quite reproducible. Thus, we lost the golden goose! </p> <h3 id=challenges-in-realizing-reproducible-ml><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#challenges-in-realizing-reproducible-ml>Challenges in realizing reproducible ML</a></h3> <p>Reproducible ML does not come in easy. A wise man once said:</p> <blockquote> <p>When you want something, all the universe conspires in helping you to achieve it. <sub>- <a href="https://www.amazon.com/Alchemist-Paulo-Coelho/dp/0061122416">The Alchemist</a> by Paulo Coelho</sub></p> </blockquote> <p>But when it comes to reproducible ML it's quite the contrary. Every single resource and techniques (Hardware, Software, Algorithms, Process &amp; Practice, Data) needed to realize ML poses some kind of challenge in meeting reproducibility (see figure 6).</p> <p><img alt src=../../../resources/reproducible-challenge.jpeg> <em>Figure 6: Overview of challenges in reproducible ML</em></p> <h4 id=1-hardware><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#1-hardware>1. Hardware</a></h4> <p>ML algorithms are quite compute hungry. Complex computation needed by ML operation, now a day's runs in order of Giga/Tera floating-point operations (GFLOPS/TFLOPS). Thus needing high parallelism and multiple central processing Units <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">(CPU)</a> if not, specialized hardware such as (general purpose) graphics processing unit <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">(GPU)</a> or more specifically <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">(GPGPU)</a>, tensor processing unit <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">(TPU)</a> etc. to complete in the reasonable time frame. </p> <p>But these efficiencies in floating-point computations both at CPU &amp; GPU level comes at a cost of reproducibility. </p> <ul> <li>CPU</li> </ul> <blockquote> <p>Using Intra-ops (within an operation) and inter-ops (amongst multiple operations) parallelism on CPU can sometimes give different results on each run. One such example is using OpenMP for (intra-ops) parallelization. See this excellent talk titled "Corden's Consistency of Floating Point Results or Why doesn't my application always give" <a href="https://www.nccs.nasa.gov../../resources/FloatingPoint_consistency.pdf">Corden 2018</a> for more in-depth insight into this. Also see wandering precision <a href="https://www.nag.co.uk/content/wandering-precision">blog</a>.</p> </blockquote> <ul> <li>GPU</li> </ul> <blockquote> <p>General purpose GPUs can perform vector operations due to stream multiprocessing (SEM) unit. The asynchronous computation performed by this unit may result in different results on different runs. Floating multiple adders (FMAD) or even reductions in floating-point operands are such examples. Some algorithms e.g. vector normalization, due to reduction operations, can also be non-reproducible. See <a href="https://people.eecs.berkeley.edu/~hdnguyen/public/papers/repsum.pdf">reproducible summation paper</a> for more info. </p> <p>Changing GPU architecture may lead to different results too. The differences in SEM, or architecture-specific optimizations are a couple of reasons why the differences may arise.</p> </blockquote> <p>See <a href="https://www.nccs.nasa.gov../../resources/FloatingPoint_consistency.pdf">Corden's Consistency of Floating-Point Results or Why doesn't my application always give the same answer</a> for more details.</p> <h4 id=2-software><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#2-software>2. Software</a></h4> <p>It's not just hardware. Some software's offering high-level abstraction or APIs for performing intensive computation do not guarantee reproducibility in their routines. For instance NVIDIA's popular cuda based deep learning library <a href="https://developer.nvidia.com/cudnn">cudnn</a> do not guarantee reproducibility in some of their routines e.g. cudnnConvolutionBackwardFilter&gt;sup&gt;<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility">ref</a></sup>. Popular deep learning libraries such as tensorflow<sup><a href="https://github.com/tensorflow/tensorflow/issues/26147">ref 1</a>,<a href="https://github.com/NVIDIA/tensorflow-determinism">ref 2</a></sup>, pytorch<sup><a href="https://pytorch.org/docs/stable/notes/randomness.html">ref</a></sup> also do not guarantee 100% reproducibility. </p> <p>There is an excellent talk <a href="https://github.com/duncanriach">Duncan Riach</a>, maintainer of <a href="https://github.com/NVIDIA/tensorflow-determinism">tensorflow_determinism</a> on <a href="https://drive.google.com/file/d/18pmjeiXWqzHWB8mM2mb3kjN4JSOZBV4A/view">Determinism in deep learning</a> presented at GPU technology conference by NVIDIA 2019</p> <p>Sometimes it's not just a trade-off for efficiency but simple software bugs that lead to non-reproducibility. One such example is this <a href="https://github.com/pyproj4/pyproj/issues/470">bug</a> that I ran into resulting in different geo-location upon same computation when a certain library version was upgraded. This is a clear case of software bug but underlines the fact that reproducibility goes beyond just computation, and precision.</p> <h4 id=3-algorithm><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#3-algorithm>3. Algorithm</a></h4> <p>Several ML algorithms can be non-reproducible due to the expectation of randomness. Few examples of these algorithms are dropout layers, initialization. Some algorithms can be non-deterministic due to underlined computation complexity requiring non-reproducible measures similar to ones discussed in the software section. Some example of these are e.g. vector normalization, <a href="https://github.com/tensorflow/tensorflow/issues/2652">backward pass</a>.</p> <h4 id=4-process-practice><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#4-process-practice>4. Process &amp; Practice</a></h4> <p>ML loves randomness! </p> <p><img alt src=../../../resources/random.png> <em>Figure 7: Randomness defined by <a href="https://xkcd.com/221">xkcd</a></em></p> <p>When things don't work with ML - we randomize (pun intended). We have randomness everywhere - from algorithms to process and practices for instance: - Random initializations - Random augmentations<br> - Random noise introduction (adversarial robustness) - Data Shuffles</p> <p>To to ensure that randomness is seeded and can be reproduced (much like earlier example of scikit linear regression), with python, a long list of seed setting ritual needs to be performed: <div class=highlight><pre><span></span><code><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;PYTHONHASHSEED&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=nb>str</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
<span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
<span class=n>tensorflow</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
<span class=n>numpy</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
<span class=n>tensorflow</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=n>SEED</span><span class=p>)</span>
<span class=n>tensorflow</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>random_flip_left_right</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=n>seed</span><span class=p>)</span>
<span class=n>tensorflow</span><span class=o>.</span><span class=n>random_normal_initializer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=n>seed</span><span class=p>)</span>
<span class=c1># many such algorithmic layers as aabove </span>
</code></pre></div></p> <p>Can we ever win with this seed setting? </p> <p><img alt src=../../../resources/seed_setting.gif> <em>Figure 8: Seed setting (image credit: google)</em></p> <h4 id=5-data><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#5-data>5. Data</a></h4> <blockquote> <p>No input is ever really independent. <sub>[Scully et. al 2015][scully_2015]</sub></p> </blockquote> <p>Data is the main input to ML algorithms and these algorithms are just compute hungry but also <strong>data-hungry</strong>. So we are really talking about big data. When data volume is large, we are dealing with all sorts of challenges: - Data management - Data provenance<br> - Data poisoning - Under-represented data (inappropriate demographic) - Over-represented data (inappropriate demographic)</p> <p>One of the reasons why ML is so iterative because we need to evolve the ML algorithm with data whilst also continuously evolving data (e.g. data massaging, feature engineering, data augmentations). That's why data provenance is important but it's also important to maintain a linage with data provenance to ML processes. In short, an end to end provenance is needed with ML processes. </p> <h4 id=6-concept-drift><a class=toclink href=../../2019/12/21/reproducible-ml-research-in-industry/#6-concept-drift>6. Concept drift</a></h4> <blockquote> <p>A model is rarely deployed twice. <sub>[Talby, 2018][Talby]</sub></p> </blockquote> <p>One of the reason for why a model rarely gets deployed more than once <sup>[ref][Talby]</sup> is <code>Concept drift</code>. Our concept of <a href="https://arxiv.org/abs/1612.03716">things and stuff</a> keeps evolving. Don't believe me? figure 9 shows how we envisaged car 18's to now. Our current evolving impression of the car is solar power self-driving cars!</p> <p><img alt src=../../../resources/concept-drift.jpeg> <em>Figure 9: Our evolving concept of <code>car</code></em></p> <p>So, now we don't just have to manage reproducibility over one model but many! Because our model needs to continually keep learning in a more commonly known term in ML as <code>Continual learning</code> <a href="https://www.oreilly.com/radar/why-continuous-learning-is-key-to-ai/">more info</a>. An interesting review paper on this topic is <a href="https://arxiv.org/abs/1802.07569">here</a>.</p> <p><img alt src=../../../resources/dvs.jpg> <em>Figure 10: Top features - Dresner Advisory Services Data Science and Machine Learning <a href=//gumroad.com/l/dTfno>Market Study</a></em></p> <p>In fact, Continual learning is so recognized that support for easy iteration &amp; continuous improvement were the top two features industry voted as their main focus with ML as per Dresner Advisory Services'6<sup>th</sup> annual 2019 Data Science and Machine Learning <a href=//gumroad.com/l/dTfno>Market Study</a> (see figure 10).</p> <hr> <p>The next part of this technical blog series, [Reproducibility in Machine Learning], is <a href=/2019/12/22/Reproducible-ml-tensorflow.html>Realizing reproducible Machine Learning - with Tensorflow</a>. </p> <p>[Reproducibility in Machine Learning]: /2019/12/20/Reproducibility-in-Machine Learning.html</p> <p>[scully_2015]: https://papers.nips.cc/paper/5656-hidden-technical-debt-in-Machine Learning-systems.pdf</p> <p>[Talby]: https://www.oreilly.com/radar/lessons-learned-turning-Machine Learning-models-into-real-products-and-services/</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-12-20 00:00:00">December 20, 2019</time></li> <li class=md-meta__item> in <a href=../../category/machine-learning/ class=md-meta__link>Machine Learning</a>, <a href=../../category/ai/ class=md-meta__link>AI</a>, <a href=../../category/kubernetes/ class=md-meta__link>Kubernetes</a>, <a href=../../category/reproducible-ml/ class=md-meta__link>Reproducible-ml</a></li> <li class=md-meta__item> 1 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=reproducibility-in-machine-learning-blog-series><a href=../../2019/12/20/reproducibility-in-machine-learning-blog-series/ class=toclink>Reproducibility in Machine Learning blog series</a></h2> <p>This technical blog series titled "Reproducibility in Machine Learning" is going to be divided into three parts: 1. Reproducibility in Machine Learning - Research and Industry 2. Realizing reproducible Machine Learning - with Tensorflow 3. End-to-end reproducible Machine Learning pipelines on Kubernetes</p> <p>Some of the content of this blog series has been covered in KubeCon US 2019 - a Kubernetes conference 2019. Details of this talk can be found <a href=/talks/KubeCon_US_2019.html>here</a> with recording available <a href="https://www.youtube.com/watch?v=ZEGdSLWdrH0">here</a>. </p> <h3 id=part-1-reproducibility-in-machine-learning-research-and-industry><a class=toclink href=../../2019/12/20/reproducibility-in-machine-learning-blog-series/#part-1-reproducibility-in-machine-learning-research-and-industry>Part 1: Reproducibility in Machine Learning - Research and Industry</a></h3> <p>In <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>Part 1</a>, the objective will be to discuss the importance of reproducibility in machine learning. It will also cover where both research and industry are stand in writing reproducible ML. This blog can be accessed <a href=/2019/12/21/Reproducible-ml-research-n-industry.html>here</a>.</p> <h3 id=part-2-realizing-reproducible-machine-learning-with-tensorflow><a class=toclink href=../../2019/12/20/reproducibility-in-machine-learning-blog-series/#part-2-realizing-reproducible-machine-learning-with-tensorflow>Part 2: Realizing reproducible Machine Learning - with Tensorflow</a></h3> <p>The focus of <a href=/2019/12/22/Reproducible-ml-tensorflow.html>Part 2</a> will be writing reproducible machine learning code. <a href="https://tensorflow.org/">Tensorflow</a> is being used as a machine learning stack for demonstration purposes. This blog can be accessed <a href=/2019/12/22/Reproducible-ml-tensorflow.html>here</a>. </p> <h3 id=part-3-end-to-end-reproducible-machine-learning-pipelines-on-kubernetes><a class=toclink href=../../2019/12/20/reproducibility-in-machine-learning-blog-series/#part-3-end-to-end-reproducible-machine-learning-pipelines-on-kubernetes>Part 3: End-to-end reproducible Machine Learning pipelines on Kubernetes</a></h3> <p><a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>Part 3</a> is all about realizing end-to-end machine learning pipelines on <a href="https://kubernetes.io/">kubernetes</a>. This blog can be accessed <a href=/2019/12/23/Reproducible-ml-pipeline-k8s.html>here</a>.</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-12-01 00:00:00">December 1, 2019</time></li> <li class=md-meta__item> in <a href=../../category/machine-learning/ class=md-meta__link>Machine Learning</a>, <a href=../../category/data-science/ class=md-meta__link>Data-science</a>, <a href=../../category/book/ class=md-meta__link>Book</a>, <a href=../../category/technology/ class=md-meta__link>Technology</a></li> <li class=md-meta__item> 4 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=reading-book-list><a href=../../2019/12/01/reading-book-list/ class=toclink>Reading book list</a></h2> <p>This list includes books that a) I have truly enjoyed reading and highly admire or b) Eagerly looking forward to reading. It has six sections:</p> <ul> <li>Abstract Programming/AI</li> <li>Kubernetes</li> <li>Machine Learning, AI, Deep Learning</li> <li>Statistics</li> <li>Parenting</li> <li>Miscellaneous</li> </ul> <h3 id=1-abstract-programming><a class=toclink href=../../2019/12/01/reading-book-list/#1-abstract-programming>1. Abstract Programming</a></h3> <blockquote> <p><img alt src=../../../resources/books/Yaworski.jpg> <em><a href="https://www.amazon.com/Real-World-Bug-Hunting-Field-Hacking/dp/1593278616">Real-World Bug Hunting</a> Authored by Peter Yaworski</em></p> <p><img alt src=../../../resources/books/Petzold.jpg> <em><a href="https://www.amazon.com.au/Code-Language-Computer-Developer-Practices-ebook/dp/B00JDMPOK2">Code: The Hidden Language of Computer Hardware and Software</a> Authored by Charles Petzold</em></p> <p><img alt src=../../../resources/books/Thomas.jpg> <em><a href="https://www.amazon.com.au/Pragmatic-Programmer-journey-mastery-Anniversary-ebook/dp/B07VRS84D1">The Pragmatic Programmer: your journey to mastery</a> Authored by Andrew Hunt, David Thomas</em></p> <p><img alt src=../../../resources/books/Seibel.jpg> <em><a href="https://www.amazon.com.au/Coders-Work-Reflections-Craft-Programming/dp/1430219483">Coders at Work: Reflections on the Craft of Programming</a> Authored by Peter Seibel</em></p> <p><img alt src=../../../resources/books/Pearl.jpg> <em><a href="https://www.amazon.com.au/Book-Why-Science-Cause-Effect/dp/046509760X/">The Book of Why: The New Science of Cause and Effect</a> Authored by Judea Pearl</em></p> <p><img alt src=../../../resources/books/Martin.jpg> <em><a href="https://www.amazon.com.au/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882">Clean Code</a> Authored by Robert C. Martin</em></p> </blockquote> <h3 id=2-kubernetes><a class=toclink href=../../2019/12/01/reading-book-list/#2-kubernetes>2. Kubernetes</a></h3> <blockquote> <p><img alt src=../../../resources/books/Hightower.jpg> <em><a href="https://www.amazon.com.au/Kubernetes-Up-Running-Brendan-Burns/dp/1492046531/">Kubernetes: Up and Running</a> Authored by Kelsey Hightower, Joe Beda, Brendan Burns</em></p> <p><img alt src=../../../resources/books/LizRice.jpg> *<a href="https://learning.oreilly.com/library/view/kubernetes-security/9781492039075/">Kubernetes Security</a> Authored by Michael Hausenblas, Liz Rice *</p> <p><img alt src=../../../resources/books/LizRice2.jpg> *<a href="https://learning.oreilly.com/library/view/container-security/9781492056690/">Container Security</a> Authored by Liz Rice *</p> <p><img alt src=../../../resources/books/Heck.jpg> <em><a href="https://learning.oreilly.com/library/view/kubernetes-for-developers/9781788834759/">Kubernetes for Developers</a> Authored by Joseph Heck</em></p> </blockquote> <h3 id=3-machine-learning-ai-deep-learning><a class=toclink href=../../2019/12/01/reading-book-list/#3-machine-learning-ai-deep-learning>3. Machine Learning, AI, Deep Learning</a></h3> <blockquote> <p><img alt src=../../../resources/books/Nilsson.jpg> <a href="https://ai.stanford.edu/~nilsson/QAI/qai.pdf">The Quest for Artificial Intelligence: A History of Ideas and Achievements</a> Authored by Nils J. Nilsson*</p> <p><img alt src=../../../resources/books/Murphy.jpg> <em>[Machine Learning: A Probabilistic Perspective] Authored by Kevin P Murphy</em></p> <p><img alt src=../../../resources/books/Nielsen.png> <em><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> Authored by Michael Nielsen</em></p> <p><img alt src=../../../resources/books/Gooodfellow.jpg> <em>[Deep Learning] Authored by Ian Goodfellow et al.</em></p> <p><img alt src=../../../resources/books/AndrewNg.jpg> <em>[Machine Learning Yearning] Authored by Andrew Ng</em></p> </blockquote> <h3 id=4-statistics><a class=toclink href=../../2019/12/01/reading-book-list/#4-statistics>4. Statistics</a></h3> <blockquote> <p><img alt src=../../../resources/books/Axler.png> <em><a href="https://linear.axler.net/">Linear Algebra Done Right</a>, Authored by Sheldon Axler</em></p> <p><img alt src=../../../resources/books/Rice.jpg> <em><a href="https://www.amazon.com/Mathematical-Statistics-Data-Analysis-John/dp/0534209343">Mathematical Statistics and Data Analysis</a>, Authored by John A. Rice</em></p> <p><img alt src=../../../resources/books/Hestie.jpg> <em><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a>, Authored by Trevor Hastie et al.</em></p> <p><img alt src=../../../resources/books/Hestie2.jpg> <em><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">Introduction to Statistical Learning</a> Authored by Trevor Hastie et al.</em></p> </blockquote> <h3 id=5-parenting><a class=toclink href=../../2019/12/01/reading-book-list/#5-parenting>5. Parenting</a></h3> <blockquote> <p><img alt src=../../../resources/books/Pasek.jpg> <em><a href="https://www.amazon.com.au/Becoming-Brilliant-Science-Successful-Children/dp/1433822393/">Becoming Brilliant: What Science Tells Us About Raising Successful Children</a> Authored by Roberta Golinkoff, Kathryn Hirsh-Pasek</em></p> <p><img alt src=../../../resources/books/Shure.jpg> <em><a href="https://www.amazon.com/Thinking-Parent-Child-Everyday-Solutions/dp/0878227032">Thinking Parent, Thinking Child: Turning Everyday Problems into Solutions</a> Authored by Myrna B. Shure</em></p> <p><img alt src=../../../resources/books/Murray.jpg> <em><a href="https://www.amazon.com.au/Psychology-Babies-relationships-support-development-ebook/dp/B0070TRFIO">The Psychology of Babies: How relationships support development from birth to two</a> Authored by Lynne Murray</em> </p> </blockquote> <h3 id=6-miscellaneous><a class=toclink href=../../2019/12/01/reading-book-list/#6-miscellaneous>6. Miscellaneous</a></h3> <blockquote> <p><img alt src=../../../resources/books/Jamail.jpg> <em><a href="https://www.amazon.com/End-Ice-Bearing-Witness-Disruption-ebook/dp/B079G4NJVD">The End of Ice: Bearing Witness and Finding Meaning in the Path of Climate Disruption</a> Authored by Dahr Jamail</em></p> <p><img alt src=../../../resources/books/Steinhardt.jpg> <em><a href="https://www.amazon.com.au/Second-Kind-Impossible-Extraordinary-Matter/dp/1476729921/">The Second Kind of Impossible: The Extraordinary Quest for a New Form of Matter</a> Authored by Paul Steinhardt</em></p> </blockquote> <p>[Machine Learning: A Probabilistic Perspective]: https://www.amazon.com.au/Machine Learning-Probabilistic-Kevin-Murphy/dp/0262018020</p> <p>[Machine Learning Yearning]: https://www.deeplearning.ai/Machine Learning-yearning/</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=/resources/me.png alt="Suneeta Mall"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-09-10 00:00:00">September 10, 2019</time></li> <li class=md-meta__item> in <a href=../../category/machine-learning/ class=md-meta__link>Machine Learning</a>, <a href=../../category/data-science/ class=md-meta__link>Data-science</a></li> <li class=md-meta__item> 1 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=links-to-open-source-ml-datasets><a href=../../2019/09/10/links-to-open-source-ml-datasets/ class=toclink>Links to open source ML datasets</a></h2> <ul> <li><a href="https://datasetsearch.research.google.com">Google Dataset Search</a></li> <li>[Wikipedia ML dataset]</li> <li><a href="https://pathmind.com/wiki/open-datasets">Pathmind</a>'s aggregation</li> <li>[Computer Vision Online] aggregated source</li> <li>20 <a href="https://lionbridge.ai/datasets/20-best-image-datasets-for-computer-vision/">Multimedia dataset</a> (images &amp; videos)</li> <li>[Hackernoon Rare dataset]</li> <li><a href="https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/">Analytics vidhya</a>'s list of 25 sets</li> </ul> <p>[Wikipedia ML dataset]: https://en.wikipedia.org/wiki/List_of_datasets_for_Machine Learning_research</p> <p>[Hackernoon Rare dataset]: https://hackernoon.com/rare-datasets-for-computer-vision-every-Machine Learning-expert-must-work-with-2ddaf52ad862</p> </div> </article> <nav class=md-pagination> <a href=../../ class=md-pagination__link>1</a> <span class=md-pagination__current>2</span> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../../projects/KCD/ class="md-footer__link md-footer__link--prev" aria-label="Previous: KCD"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> KCD </div> </div> </a> <a href=../../category/ai/ class="md-footer__link md-footer__link--next" aria-label="Next: AI"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> AI </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> </div> <div class=md-social> <a href="https://www.linkedin.com/in/suneeta-mall-a6a0507/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href="https://github.com/suneeta-mall" target="_blank" rel="noopener" title="github.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href="https://x.com/suneetamall/" target="_blank" rel="noopener" title="x.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9L389.2 48zm-24.8 373.8h39.1L151.1 88h-42l255.3 333.8z"/></svg> </a> <a href="https://www.medium.com/@suneetamall" target="_blank" rel="noopener" title="www.medium.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg> </a> <a href="https://scholar.google.com.au/citations?hl=en&amp;user=WD712CUAAAAJ" target="_blank" rel="noopener" title="scholar.google.com.au" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg> </a> <a href="https://www.researchgate.net/profile/Suneeta_Mall3" target="_blank" rel="noopener" title="www.researchgate.net" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6z"/></svg> </a> <a href="https://suneeta-mall.github.io/feed_rss_created.xml" target="_blank" rel="noopener" title="suneeta-mall.github.io" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64zm0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0zm32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32z"/></svg> </a> <a href=mailto:suneetamall@gmail.com target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l217.6 163.2c11.4 8.5 27 8.5 38.4 0l217.6-163.2c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48H48zM0 176v208c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V176L294.4 339.2a63.9 63.9 0 0 1-76.8 0L0 176z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "content.tabs.link", "navigation.indexes", "navigation.instant", "navigation.instant.preview", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "toc.follow", "header.autohide", "announce.dismiss", "navigation.footer", "navigation.breadcrumbs", "navigation.expand", "navigation.sections", "navigation.tracking", "navigation.top", "search.highlight", "search.share", "toc.follow", "toc.integrate", {"git-revision-date-localized": {"enable_creation_date": true, "type": "date"}}], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src=../../../assets/javascripts/bundle.3220b9d7.min.js></script> </body> </html>
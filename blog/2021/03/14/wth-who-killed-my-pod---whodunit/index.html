<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href="https://suneeta-mall.github.io/blog/2021/03/14/wth-who-killed-my-pod---whodunit/" rel="canonical"><link href=../../../../2019/12/23/end-to-end-reproducible-machine-learning-pipelines-on-kubernetes/ rel=prev><link href=../../../12/31/data-in-deep-learning/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../../../feed_rss_updated.xml><link rel=icon href=../../../../../resources/site/favicon.svg><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.18"><title>WTH! Who killed my pod - Whodunit? - Random Musings - Rambling of a curious engineer & data scientist!</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.66ac8b77.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><link rel="stylesheet" href="../../../../../assets/external/fonts.googleapis.com/css.49ea35f2.css"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><meta property=og:type content=website><meta property=og:title content="WTH! Who killed my pod - Whodunit? - Random Musings - Rambling of a curious engineer & data scientist!"><meta property=og:description content=None><meta property=og:image content=https://suneeta-mall.github.io/assets/images/social/blog/posts/2021-03-14-wth-who-killed-my-pod.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://suneeta-mall.github.io/blog/2021/03/14/wth-who-killed-my-pod---whodunit/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="WTH! Who killed my pod - Whodunit? - Random Musings - Rambling of a curious engineer & data scientist!"><meta name=twitter:description content=None><meta name=twitter:image content=https://suneeta-mall.github.io/assets/images/social/blog/posts/2021-03-14-wth-who-killed-my-pod.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#wth-who-killed-my-pod-whodunit class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> <a href=/projects/oreilly_deep_learning_at_scale/ > <strong>ðŸŽ‰ New Book Release!</strong> Check out "Deep Learning at Scale" - An O'Reilly Book </a> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href="https://suneeta-mall.github.io/" title="Random Musings - Rambling of a curious engineer &amp; data scientist!" class="md-header__button md-logo" aria-label="Random Musings - Rambling of a curious engineer &amp; data scientist!" data-md-component="logo"> <img src=../../../../../resources/site/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Random Musings - Rambling of a curious engineer & data scientist! </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> WTH! Who killed my pod - Whodunit? </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 1 3 3 3 3 0 0 1-3 3 3 3 0 0 1-3-3 3 3 0 0 1 3-3m0-4.5c5 0 9.27 3.11 11 7.5-1.73 4.39-6 7.5-11 7.5S2.73 16.39 1 12c1.73-4.39 6-7.5 11-7.5M3.18 12a9.821 9.821 0 0 0 17.64 0 9.821 9.821 0 0 0-17.64 0Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../../projects/oreilly_deep_learning_at_scale/ class=md-tabs__link> Projects </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../../ class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../../../../../tags/ class=md-tabs__link> Tags </a> </li> <li class=md-tabs__item> <a href=../../../../../talks/KGC_NY_2022/ class=md-tabs__link> Talks </a> </li> <li class=md-tabs__item> <a href=../../../../../poems/singularity/ class=md-tabs__link> Poems </a> </li> <li class=md-tabs__item> <a href=../../../../../about/ class=md-tabs__link> About Me </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href="https://suneeta-mall.github.io/" title="Random Musings - Rambling of a curious engineer &amp; data scientist!" class="md-nav__button md-logo" aria-label="Random Musings - Rambling of a curious engineer &amp; data scientist!" data-md-component="logo"> <img src=../../../../../resources/site/logo.svg alt=logo> </a> Random Musings - Rambling of a curious engineer & data scientist! </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../projects/oreilly_deep_learning_at_scale/ class=md-nav__link> <span class=md-ellipsis> Deep Learning at Scale </span> </a> </li> <li class=md-nav__item> <a href=../../../../../projects/curious_cassie/ class=md-nav__link> <span class=md-ellipsis> Curious Cassie - The Children's Books </span> </a> </li> <li class=md-nav__item> <a href=../../../../../projects/feature_analysis/ class=md-nav__link> <span class=md-ellipsis> Label Noise with Clean Lab </span> </a> </li> <li class=md-nav__item> <a href=../../../../../projects/feature_analysis/ class=md-nav__link> <span class=md-ellipsis> Feature Analysis </span> </a> </li> <li class=md-nav__item> <a href=../../../../../projects/oreilly-interactive-katacode-series-for-reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Oreilly Katacode Series </span> </a> </li> <li class=md-nav__item> <a href=../../../../../projects/reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Reproducible-ML </span> </a> </li> <li class=md-nav__item> <a href=../../../../../projects/KCD/ class=md-nav__link> <span class=md-ellipsis> KCD </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/ai/ class=md-nav__link> <span class=md-ellipsis> AI </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/book/ class=md-nav__link> <span class=md-ellipsis> Book </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/childrens-books/ class=md-nav__link> <span class=md-ellipsis> Children's Books </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/confident-learning/ class=md-nav__link> <span class=md-ellipsis> Confident-Learning </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/curious-cassie/ class=md-nav__link> <span class=md-ellipsis> Curious Cassie </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/data/ class=md-nav__link> <span class=md-ellipsis> Data </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/data-centric-ai/ class=md-nav__link> <span class=md-ellipsis> Data-Centric-AI </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/data-science/ class=md-nav__link> <span class=md-ellipsis> Data-science </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/generative-ai/ class=md-nav__link> <span class=md-ellipsis> Generative AI </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/kubernetes/ class=md-nav__link> <span class=md-ellipsis> Kubernetes </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/machine-learning/ class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/oom/ class=md-nav__link> <span class=md-ellipsis> OOM </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/pytorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/reproducible-ml/ class=md-nav__link> <span class=md-ellipsis> Reproducible-ml </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/software/ class=md-nav__link> <span class=md-ellipsis> Software </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/technology/ class=md-nav__link> <span class=md-ellipsis> Technology </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/umap/ class=md-nav__link> <span class=md-ellipsis> UMAP </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/t-sne/ class=md-nav__link> <span class=md-ellipsis> t-SNE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../../tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Talks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Talks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../talks/KGC_NY_2022/ class=md-nav__link> <span class=md-ellipsis> Knowledge Graph Conference 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/KubeCon_NA_2021/ class=md-nav__link> <span class=md-ellipsis> KubeCon NA 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/Kafka_Summit_APAC_2021/ class=md-nav__link> <span class=md-ellipsis> Kafka Summit APAC 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/AWS_ANZ_Commuity_day_2020/ class=md-nav__link> <span class=md-ellipsis> AWS Community Day 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/She_Builds_on_AWS_2020/ class=md-nav__link> <span class=md-ellipsis> AWS She Builds on AWS 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/KubeCon_US_2019/ class=md-nav__link> <span class=md-ellipsis> KubeCon US 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/KubernetesSydneyForum_AU_2019/ class=md-nav__link> <span class=md-ellipsis> Kubernetes Sydney 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/YOW_Data_Syd_2019/ class=md-nav__link> <span class=md-ellipsis> YOW Data 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/KubeCon-Europe-2018/ class=md-nav__link> <span class=md-ellipsis> KubeCon EU 2018 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/SPIE-2019/ class=md-nav__link> <span class=md-ellipsis> SPIE 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/SPIE-2018/ class=md-nav__link> <span class=md-ellipsis> SPIE 2018 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../talks/SPIE-2015/ class=md-nav__link> <span class=md-ellipsis> SPIE 2015 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Poems </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Poems </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../poems/singularity/ class=md-nav__link> <span class=md-ellipsis> Singularity </span> </a> </li> <li class=md-nav__item> <a href=../../../../../poems/life-of-ai-engineer/ class=md-nav__link> <span class=md-ellipsis> Life of AI Engineers </span> </a> </li> <li class=md-nav__item> <a href=../../../../../poems/my-little-butterfly/ class=md-nav__link> <span class=md-ellipsis> My little Butterfly </span> </a> </li> <li class=md-nav__item> <a href=../../../../../poems/breaking-thy-bias/ class=md-nav__link> <span class=md-ellipsis> Breaking Thy Bias </span> </a> </li> <li class=md-nav__item> <a href=../../../../../poems/daminis/ class=md-nav__link> <span class=md-ellipsis> Daminis </span> </a> </li> <li class=md-nav__item> <a href=../../../../../poems/one-bright-dawn/ class=md-nav__link> <span class=md-ellipsis> One Bright Dawn </span> </a> </li> <li class=md-nav__item> <a href=../../../../../poems/aint-no-dr-seuss/ class=md-nav__link> <span class=md-ellipsis> Aint no Dr. Seuss </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../../about/ class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../../ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src=/resources/me.png alt="Suneeta Mall"> </span> <span class=md-profile__description> <strong> Suneeta Mall </strong> <br> Builder </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5v-5Z"/></svg> <time datetime="2021-03-14 00:00:00" class=md-ellipsis>March 14, 2021</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3H9m3 2 4 13 3-1-4-13-3 1M5 5v13h3V5H5M3 19v2h18v-2H3Z"/></svg> <span class=md-ellipsis> in <a href=../../../../category/kubernetes/ >Kubernetes</a>, <a href=../../../../category/oom/ >OOM</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7h1.5Z"/></svg> <span class=md-ellipsis> 19 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#context-of-the-app class=md-nav__link> <span class=md-ellipsis> Context of the app </span> </a> </li> <li class=md-nav__item> <a href=#apps-on-kube-day-1 class=md-nav__link> <span class=md-ellipsis> App's on Kube: day 1 </span> </a> <nav class=md-nav aria-label="App's on Kube: day 1"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#deep-dive-into-factors-at-play-here class=md-nav__link> <span class=md-ellipsis> Deep-dive into factors at play here </span> </a> </li> <li class=md-nav__item> <a href=#so-why-the-pods-are-getting-killed class=md-nav__link> <span class=md-ellipsis> So why the pods are getting killed? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#apps-on-kube-day-2 class=md-nav__link> <span class=md-ellipsis> App's on Kube: day 2 </span> </a> <nav class=md-nav aria-label="App's on Kube: day 2"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#whats-log-telling-us class=md-nav__link> <span class=md-ellipsis> Whats log telling us </span> </a> <nav class=md-nav aria-label="Whats log telling us"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#kube-events-for-pod-and-other-higher-level-abstractions class=md-nav__link> <span class=md-ellipsis> Kube events for pod and other higher-level abstractions </span> </a> </li> <li class=md-nav__item> <a href=#what-are-the-cri-and-kubelet-doing class=md-nav__link> <span class=md-ellipsis> What are the CRI and kubelet doing? </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#apps-on-kube-day-3 class=md-nav__link> <span class=md-ellipsis> App's on Kube: day 3 </span> </a> <nav class=md-nav aria-label="App's on Kube: day 3"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#reading-kernel-logs class=md-nav__link> <span class=md-ellipsis> Reading kernel logs </span> </a> </li> <li class=md-nav__item> <a href=#deep-dive-into-os-kernel class=md-nav__link> <span class=md-ellipsis> Deep-dive into OS Kernel </span> </a> </li> <li class=md-nav__item> <a href=#so-why-the-pods-are-getting-killed_1 class=md-nav__link> <span class=md-ellipsis> So why the pods are getting killed? </span> </a> </li> <li class=md-nav__item> <a href=#controlling-over-commits class=md-nav__link> <span class=md-ellipsis> Controlling over-commits </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <h1 id=wth-who-killed-my-pod-whodunit>WTH! Who killed my pod - Whodunit?<a class=headerlink href=#wth-who-killed-my-pod-whodunit title="Permanent link">#</a></h1> <p>A few days ago, I deployed a brand new application onto a self-managed Kubernetes cluster (hereafter referred to as Kube). Suffice to say, all hell broke loose. The pods were getting <code>OOMKilled</code> with error code 137 left and right! </p> <p>Now, I know a thing or two about Kubernetes<sup><a href="https://suneeta-mall.github.io/talks/KubernetesSydneyForum_AU_2019.html">1</a>,<a href="https://suneeta-mall.github.io/talks/KubeCon_US_2019.html">2</a></sup>. I am not a total Kube noob!<br> But, I could not figure out what the fudge was going on actually! Besides, this app has been thoroughly tested and profiled and ran fine on bare metal and virtual environments.</p> <p>So this was me, a few days ago!.</p> <!-- ![](../../resources/oom/4201968f94aacab1c0190d9688daba00-sticker.jpg)--> <p><img alt= src="../../../../../assets/external/media4.giphy.com/media/z9AUvhAEiXOqA/giphy-downsized.gif"></p> <p>This sparked a massive hunt for the culprit, and some interesting insights were discovered. Worth noting, similar investigating has also been done on by <a href="https://engineering.linecorp.com/en/blog/prometheus-container-kubernetes-cluster/">Line Corp</a> in their excellent blog however, I have a different story to tell!</p> <p>In this writeup, I am going to talk about this particular incident and the insights I have uncovered about both Kube and Linux kernels.</p> <h2 id=context-of-the-app>Context of the app<a class=headerlink href=#context-of-the-app title="Permanent link">#</a></h2> <p>The app runs some intensive <a href="https://numpy.org">numpy</a> and <a href="https://tensorflow.org">Tensorflow</a> computations to produce some artifacts and associative metadata. The workloads are more memory-intensive as they operate on rich multi-media content. Other gory details besides <em>resource requirements</em> of the app is irrelevant for this discussion. </p> <p>The average resource requirement, for this app, is very fluctuating yet predictable (in a given range). At least so we thought looking at our metrics:</p> <p><img alt src=../../../../../resources/oom/avg-resource-requirement.jpg> <em>Figure 2: Average resource requirements of the app when run on VMs or bare metal</em></p> <p>I hear you, the resource utilization is not following a zero gradient line (fig 2)! It would be awesome to have constant non-flapping resource requirement needs - so clearly some work needs to happen on the app here. Having said that, it's an absolutely acceptable and supported workload. </p> <p>Ok, so the app was deployed and now, we will look at the line of investigation:</p> <h2 id=apps-on-kube-day-1>App's on Kube: day 1<a class=headerlink href=#apps-on-kube-day-1 title="Permanent link">#</a></h2> <p>The provisioned app pods started to get killed as frequently as every 20 mins or more with error code 137 and reason <code>OOMKilled</code>. </p> <p><img alt src=../../../../../resources/oom/Joys%20of%20being%20killed%21.jpg> <em>Figure 3: The killer is on the loose! - Whodunit?</em></p> <p>Let me explain a few things about the failure first: 1. <code>Error code 137</code> indicates that the container process received the SIGKILL and thus was killed by the OS kernel. SIGKILL on Kube can only be produced using one of the following means:</p> <div class=highlight><pre><span></span><code>1.1. Manually (human): Triggering CTRL+C or using other means of manually sending SIGKILL or even manually killing process.

1.2. Container Runtime/Interface: `Kubelet` the process running on the host machine that manages running Kube workload is `the power that be` for containers. 
It communicates through container runtime to manage the container lifecycle. It can kill and almost always kills badly behaving pods!

![](../../resources/oom/CRI.png)
*Figure 4: Container runtime interface. Image Credit: [Ian Lewis]! Borrowed from his 4 part container runtime series [container runtime] that I highly recommend reading*

1.3. OS kernel: The OS kernel is responsible for the life cycle of processes running on the host. 
It is `the mighty power that be` for all the processes on the host including the container process and its children.
It can also kill and almost always kills badly behaving processes!
</code></pre></div> <ol> <li><code>OOMKilled</code> represent a kill event (SIGKILL) triggered to a process because someone <em>in-charge</em> suspected of the process to be the culprit of a memory surge that may lead to an out of memory event. This is a safeguard mechanism to avoid system-level failure and to nip mischieve in the bud. </li> </ol> <p><code>Takeaway 1</code>: Either Container Runtime/Interface or OS Kernel killed my process because supposedly it was misbehaving and causing the out-of-memory issue! Essentially, I am ruling out the manual kill because that was simply not the case!</p> <h3 id=deep-dive-into-factors-at-play-here>Deep-dive into factors at play here<a class=headerlink href=#deep-dive-into-factors-at-play-here title="Permanent link">#</a></h3> <ol> <li> <p><em>Container runtime</em> (in fig 4) is responsible for two things: </p> <p>a) Running containers: Comes from open container initiative (OCI) (about 2013) open sourced by Docker called "runc". It provides ability to run containers.</p> <p>b) Image management: How images are packed, unpacked, pushed, pulled etc comes under this umbrella. A good example for this is "containerd". </p> <p><img alt src=../../../../../resources/oom/docker_stack.jpeg> <em>Figure 5: Docker stack! Image credit: internet</em></p> <p>There are several other implementation for runtime than runc+containerd like rkt but for me, its <code>runc+containerd</code> in play.</p> </li> <li> <p><a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">control groups</a> are a Linux kernel feature that allows processes to be organized into hierarchical groups whose usage of various types of ../resources (memory, CPU, and so on) can then be limited and monitored. The cgroups interface is provided through a pseudo-filesystem called cgroupfs. You may have heard about <code>/sys/fs/cgroup/</code>! </p> <p><a href="https://twitter.com/lizrice">Liz Rice</a> did an excellent demonstration of <a href="https://www.youtube.com/watch?v=8fi7uSYlOdc">what it means to run a container and how they work</a> that I highly recommend going through. Don't forget playing with the <a href="https://github.com/lizrice/containers-from-scratch/blob/master/main.go">demo code</a>. It gives a foundational understanding of cgroups's role in all things containers.</p> <p><img alt= src="../../../../../assets/external/wizardzines.com/zines/containers/samples/cgroups.jpg"> <em>Figure 6: CGroup in picture! Image credit: <a href="https://wizardzines.com/zines/containers/">zines</a> by <a href="https://twitter.com/b0rk">Julia Evans</a></em></p> </li> <li> <p><code>Kubelet</code> (see fig 4) not only interfaces container runtime but also has <code>cAdvisor</code>(for <a href="https://github.com/google/cadvisor"><strong>C</strong>ontainer <strong>Advisor</strong></a>) integrated within. Note <code>kubelet</code> is a service running on the host and it operates at the host level, not the pod. With <code>cAdvisor</code> it captures resource utilization, statistics about <a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">control group</a> of all container processes on the host.</p> </li> <li> <p>Kubernetes manages the resource for containers using <code>cgroups</code> that guarantees resource isolation and restrictions. Kube can allocate X amount of ../resources to a container and allow the ../resources to grow until a pre-existing limit is reached or no more is left on the host to use. Kube provides these <a href="https://kubernetes.io/docs/concepts/configuration/manage-../resources-containers/">requests and limits</a> semantic on containers which are used to enforce the said limit on process hierarchy for each container via cgroups. Now, the `limit is not always a hard cut-off. As documented in google's blog of <a href="https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits">best practices resource requests and limits</a>, there are two types of ../resources:</p> <ol> <li><em>Compressible</em> ../resources: When resource limit is reached, Kube will throttle the container i.e. start to restrict the usage but won't actually terminate the container. CPU is considered as a compressible resource.</li> <li><em>Incompressible</em> ../resources: When a limit for this type of resource is reached, the highest usage process within the cgroups hierarchy will be killed. Memory is an incompressible resource. </li> </ol> <p><code>Takeaway 2</code>: It's not the CPU limit, but the memory limit that we need to focus on.</p> </li> <li> <p>Kubernetes classifies pod into three categories based on the quality of service (QoS) they provide:</p> <p>4.1 <em>Guaranteed</em> pods are those who's resource request and limit are just the same. These are the best kind of workload from Kube's viewpoint as they are easier to allocate and plan for resource-wise. These pods are guaranteed to not be killed until they exceed their limits. <img alt src=../../../../../resources/oom/qos-guranteed.jpg> <em>Figure 7: Guaranteed QoS pod example</em></p> <p>4.2 <em>Best-Effort</em> pods are those where no resource requirements are specified. These are the lowest priority pods and the first to get killed if the system runs out of memory. <img alt src=../../../../../resources/oom/qos-best%20effort.jpg> <em>Figure 8: Best-Effort QoS pod example</em></p> <p>4.3 <em>Burstable</em> pods are those whose resource request and limit are defined in a range (fig 9), with limit treated as max if undefined. These are the kind of workloads that are more likely to be killed when the host system is under load and they exceed their requests and no Best-Effort pods exist. <img alt src=../../../../../resources/oom/qos-bustable.jpg> <em>Figure 9: Burstable QoS pod example</em></p> <div class=highlight><pre><span></span><code>So can Kube over-commit? 
If yes, would it always be on the compressible ../resources? 
</code></pre></div> <p>Yes, Kube can overcommit. The pod limits are allowed to be higher than requests. It's possible that the sum of all limits exceeds the total node capacity. It's possible to overcommit both compressible and incompressible ../resources. This is pictorially explained <a href="https://sysdig.com/blog/troubleshoot-kubernetes-oom/">here</a>. In fact, with Kube, it's also possible to not only vertically overcommit but also horizontally (at cluster level) overcommit. Horizontal overcommits are nicer as they can trigger auto-scaling events to scale out. </p> </li> </ol> <h3 id=so-why-the-pods-are-getting-killed>So why the pods are getting killed?<a class=headerlink href=#so-why-the-pods-are-getting-killed title="Permanent link">#</a></h3> <p>The app was initially deployed with <code>Burstable</code> QoS with Memory requirements set at request: 4Gi, limit: 7Gi, and CPU set at 2 for both requests, limits (see fig 2). The nodes were AWS <code>r5.2xlarge</code> type with 8 CPU, 64GB RAM, running Debian/Linux. Other than Kube system components and the app, nothing else was deployed on these nodes. </p> <p>So, Kube could have only deployed 3 app pods per <code>r5.2xlarge</code> nodes (due to CPU request). This means, 43GB (=64-7*3) of RAM was lying around singing hakuna matata! What a waste! Sure but let's not digress! So why the OOMKill? <code>Â¯\_(ãƒ„)_/Â¯</code></p> <p>Noteworthy observation: - Node monitoring tells us that is running healthy and has plenty of ../resources at its disposal. - the pod is still OOMKilled but not all app pods on the node, just one is killed.</p> <p>I am still clueless. So, caving in, I decided to use up this extra memory floating around and beef up the nodes a bit more and buy more time to do a proper investigation. Now, the apps are redeployed again with RAM request 4Gi, limit: 31Gi (leaving 4GB for other misc system components).</p> <p>Did that ameliorate the problem - no! Of course, I am being silly about this, I should be making it guaranteed to have better chance of avoiding OOMKill. </p> <h2 id=apps-on-kube-day-2>App's on Kube: day 2<a class=headerlink href=#apps-on-kube-day-2 title="Permanent link">#</a></h2> <p>So, my apps are running with guaranteed QoS with 31GB of RAM as required/limit. Node still seems healthy and shows no sign of duress. </p> <p>Hows the app doing with the new revised configuration: <code>still getting OOMKilled with 137 error code left and right!</code> </p> <p><img alt src=../../../../../resources/oom/34b8525b2cff89f7f25f2f70d62c5014-sticker.png></p> <p>Meanwhile, we uncovered random memory surges in some pods (see figure 10). These surges occurred very rarely and did not match to the duration of out-of-memory kill events. In fact, the frequency of OOM was much higher than these memory surges. </p> <p><img alt src=../../../../../resources/oom/memory-spike.jpg> <em>Figure 10: The notorious spike of memory use on pod</em></p> <p>While these surges are worth investigating, they are still within the request/limit range (28.x Gi suurge on 31Gi request). So they still don't justify the OOM event.</p> <h3 id=whats-log-telling-us>Whats log telling us<a class=headerlink href=#whats-log-telling-us title="Permanent link">#</a></h3> <p>Based on Takeaway 1 &amp; 2, we look at who is firing the kill signal. #Whodunit</p> <h4 id=kube-events-for-pod-and-other-higher-level-abstractions>Kube events for pod and other higher-level abstractions<a class=headerlink href=#kube-events-for-pod-and-other-higher-level-abstractions title="Permanent link">#</a></h4> <p>Investigating, on Kube <code>Events</code> there is no record or any OOMKill or any event signaling anything malicious. <div class=highlight><pre><span></span><code>kubectl<span class=w> </span>describe<span class=w> </span>pod<span class=w> </span>&lt;my<span class=w> </span>pod&gt;
kubectl<span class=w> </span>describe<span class=w> </span>deploy<span class=w> </span>&lt;my<span class=w> </span>pod&gt;
</code></pre></div> In fact, according to my Kube event stream <code>kubectl get events</code>, Kube is all healthy and there is nothing to see, nothing to worry about there! It shows that containers are clearly being restart but it seems to be not capturing any adverse event and bringing it back up to keep to desired declared state on attached replicaset. <div class=highlight><pre><span></span><code>26m         Normal   Created   pod/myapp   Created container planck
26m         Normal   Started   pod/myapp   Started container planck
26m         Normal   Pulled    pod/myapp   Container image &quot;app&quot; already present on machine
</code></pre></div></p> <h4 id=what-are-the-cri-and-kubelet-doing>What are the CRI and kubelet doing?<a class=headerlink href=#what-are-the-cri-and-kubelet-doing title="Permanent link">#</a></h4> <p>Looking at the system journal, there is nothing noteworthy recorded for OOM. 1. Nothing is logged for <code>Out of memory</code> (command reference <code>journalctl -u kubelet | grep -i "Out of memory"</code>) 2. Only log I see for shorter term <code>oom</code> (cmd reference <code>journalctl -u kubelet | grep -i "oom"</code> is info level log of kubelet startup record.<br> <div class=highlight><pre><span></span><code>kubelet[2130]: I0309 04:52:13.990735    2130 flags.go:33] FLAG: --oom-score-adj=&quot;-999&quot;
kubelet[2130]: I0309 04:52:15.416807    2130 docker_service.go:258] Docker Info: &amp;{ID:XF74:2JFW:UOE4:QI7X:TXQU:RJLG:E7FC:K4K3:IUTM:MGFW:W2GM:Z6AC Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:false IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:23 OomKillDisable:true NGoroutines:44 SystemTime:2021-03-09T04:52:15.411198727Z LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:4.9.0-14-amd64 OperatingSystem:Debian GNU/Linux 9 (stretch) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00062c0e0 NCPU:16 MemTotal:133666107392 Generic../resources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ip-172-30-36-152 Labels:[] ExperimentalBuild:false ServerVersion:18.06.3-ce ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:&lt;nil&gt; Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:468a545b9edcd5932818eb9de8e72413e616e86e Expected:468a545b9edcd5932818eb9de8e72413e616e86e} RuncCommit:{ID:a592beb5bc4c4092b1b1bac971afed27687340c5 Expected:a592beb5bc4c4092b1b1bac971afed27687340c5} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[]}
kubelet[2130]: I0309 04:52:15.437879    2130 manager.go:1159] Started watching for new ooms in manager
</code></pre></div></p> <p>Normally, in the event of OOM triggered by Kube, we should see kubelet recording some signal for oom e.g. <code>An OOM event was triggered</code></p> <p><code>Takeaway 3</code>: As far as Kube is concerned, the pod is well behaved and it's all hakuna matata! <img alt= src="../../../../../assets/external/i.pinimg.com/originals/7a/18/f9/7a18f9c6efe7a954a42473cf8a5bd1fb.gif"></p> <p>So, #Whodunit? Enter day 3 - new day new investigation </p> <h2 id=apps-on-kube-day-3>App's on Kube: day 3<a class=headerlink href=#apps-on-kube-day-3 title="Permanent link">#</a></h2> <p>Based on the previous 3 takeaways, the only potential suspect we have is OS kernel. The pods are still crashing and metrics, events, and Kube level logs do not justify the observation. </p> <h3 id=reading-kernel-logs>Reading kernel logs<a class=headerlink href=#reading-kernel-logs title="Permanent link">#</a></h3> <ol> <li> <p>System level log scan <code>grep -i -r 'out of memory' /var/log/</code> takes us somewhere. <div class=highlight><pre><span></span><code>/var/log/kern.log:Mar  9 13:17:05 ip-172-xxx-xx-xxx kernel: [30320.358563] Memory cgroup out of memory: Kill process 11190 (app) score 9 or sacrifice child
</code></pre></div> <code>Takeaway 4</code>: We do in fact have kernel thinking memory cgroups is in danger and starting to kill!</p> </li> <li> <p>Kernel logs (<code>/var/log/kern.log</code>) seem to have much more insightful info than the above one-liner <code>out of memory: Kill process</code>. </p> </li> </ol> <p>But before we look into this, let's do a bit of a deep dive into related concepts: </p> <h3 id=deep-dive-into-os-kernel>Deep-dive into OS Kernel<a class=headerlink href=#deep-dive-into-os-kernel title="Permanent link">#</a></h3> <ol> <li><strong>Swap space and Kube</strong></li> </ol> <p>Docker supports setting <a href="https://docs.docker.com/config/containers/resource_constraints/">swappiness</a> however it's discouraged as it's slow and less performant. Also, providing a limit on the swap is unsupported at the docker level which can lead to resource management and overcommitment chaos. These are some of the reasons why <a href="https://github.com/kubernetes/kops/issues/3251">kops</a> and in general Kube prefer no swap on hosts. </p> <ol> <li><strong>OOMKill disable on Kubernetes</strong></li> </ol> <p>OS Kernels allow disabling OOM Kill for cgroups level (<code>/sys/fs/cgroup/memory/memory.oom_control</code>) even docker supports it using <code>--oom-kill-disable</code> flag. These are highly discouraged due to the nature of problem band-aid fixer <code>OOM Killer</code> solves. It also does not sit with Kube's declarative approach orchestration and also with cattle workload philosophy. It's also why by default oom kill is enabled on Kubernetes.</p> <p>Its possible however to configure it to disable OOMKill by starting kubelet service with <code>--cgroup-driver=cgroupfs</code> argument and then setting <code>oom_kill_disable</code> under <code>/sys/fs/cgroup/memory/memory.oom_control</code> as 1.</p> <p><code>Takeaway 5</code>: It's not something I want to enable either, but for the completeness of the discussion, it's worth mentioning :). </p> <ol> <li> <p><strong>Kernel memory management</strong></p> <p>The kernel uses virtual addressing (using paging and segmentation) to provide isolation amongst various processes running on host. It is also virtual addressing that allows for use of more memory than what's available currently in physical memory (RAM) by making use of other sources like a disk (a.k.a. swap). Virtual addressing is divided into user &amp; kernel space. Userspace is the sort of virtual address space that's reserved for user/application programs whereas kernel space is reserved for kernel-related operations. </p> <p>Now, the os kernel is designed to be greedy - greedy to be able to run as many processes as possible. This is also the reason why we need mechanisms like `out of memory'.</p> </li> <li> <p><strong>System vs memory controller (memch) OOM</strong></p> </li> </ol> <p>cgroups comprises of two components: <code>core</code> and <code>controller</code>. Core corresponds to managing the hierarchy and core capabilities whereas controllers are focused on the type of resource cgroup is controlling eg cpu, io, memory controller ('memcg'). </p> <div class=highlight><pre><span></span><code>Now, the user-space out-of-memory handling can address OOM conditions for both cgroups using the memory controller (&#39;memcg&#39;) and for the system as a whole.
`Takeaway 6`: We know, based on our takeaways, that our OOM is not stemming from system draining or system as whole. Also, log `Memory cgroup out of memory` indicating that its `memcg`
that&#39;s triggering the OOM Kill. Here, the app process hierarchy memory usage is aggregated together into its memcgs so the memory usage at group level can be accounted for. 
What our first log here is telling us is `memcg usage reached its limits and memory cannot be reclaimed i.e. the memcg is out of memory`&lt;sup&gt;[1][lwn]&lt;/sup&gt;.
</code></pre></div> <ol> <li> <p><strong>OOM kill score</strong></p> <p>How does kernel come to decide which process to kill, is based on a score. The score has two parts: main (<code>oom_score</code>) and adjustment factor (<code>oom_score_adj</code>). These scores are store against process id in process space and can be located on disk as : <div class=highlight><pre><span></span><code>/proc/&lt;pid&gt;/oom_score
/proc/&lt;pid&gt;/oom_score_adj
</code></pre></div></p> <p>The <code>oom_score</code> is given by kernel and is proportional to the amount of memory used by the process i.e. = 10 x percentage of memory used by the process. This means, the maximum <code>oom_score</code> is 100% x 10 = 1000!. Now, the higher the oom_score higher the change of the process being killed. However, user can provide an adjustment factor <code>oom_score_adj (a.k.a. oom_adj in older kernel versions)</code>. If provided, it is used to adjust the final score. The valid value for <code>oom_score_adj</code> is in the range of (-1000, +1000), where -ve score decreases and +ve increases the chances of oomkill. More details on this can be found in this very interesting article by Jonathan Corbet <a href="https://lwn.net/Articles/391222/">another OOMKill rewrite</a>, with precursory article found <a href="https://lwn.net/Articles/317814/">here</a>.</p> </li> <li> <p><strong>OOM trigger workflow</strong></p> </li> </ol> <p><code>kmsg</code> is the kernel message interface that directs kernel messages to <code>/proc/kmsg</code> &amp; <code>/dev/kmsg</code>. Now, <code>/dev/kmsg</code> is more useful for us mere mortals as it's designed to be persistent. <code>/proc/kmsg</code> is designed to be read once and treated more as event queue if you will. Messages from here also trickle through to kernel logs @ <code>/var/log/kern.log</code>.</p> <div class=highlight><pre><span></span><code>_On Kube_

Kebelet watches for `kmsg` and handles messages that will translate to OOMEvent/OOMKillEvent in Kube event stream which is then handled appropriately to trigger OOMKill. More interesting details of how this happens can be found [here][line-eng-qos] (also shown in borrowed fig 11).

![](../../resources/oom/workflow-4-1024x816.png)
*Figure 11: OOM handling workflow on Kubernetes. Image credit: [Line Corp][line-eng-qos]*

As mentioned in `takeway 3 &amp; 4`, this workflow however was not triggered in our case, we are did not record any Kube related OOM events or even kubelet receiving
any related messages.

_At Kernel Level_
When system or memory controller related OOM is suspected, based on `oom_score` (with adjustment `oom_score_adj`), `oom-killer` is invoked on the highest
score process and its children.
</code></pre></div> <h3 id=so-why-the-pods-are-getting-killed_1>So why the pods are getting killed?<a class=headerlink href=#so-why-the-pods-are-getting-killed_1 title="Permanent link">#</a></h3> <p>In my case, memory cgroup ran out of memory and my stack trace confirms this (see fig 12). It tells me that the application container was killed because it was consuming 1.5MB shy of memory set as limit (31457280 KB).</p> <p><img alt src=../../../../../resources/oom/log-part-1.jpg> <em>Figure 12: Kernel log part 1</em></p> <p>OK! this explains the OOMKill but why:</p> <p>a. My monitoring only shows 29GB as max memory surge!</p> <p>b. I never noticed beyond 9GB usage in local/testing/profiling and all the jazz!</p> <p><img alt src=../../../../../resources/oom/2efa70f25d30b6e591150bc7a03e76e9-sticker.jpg></p> <p>This simply does not add up! Let's hold on to this thought for a bit and look at the rest of the logs and what it says:</p> <p>Before we go into part 2 of the log, I should explain a few things:</p> <ol> <li> <p>The <em>pause container</em> is the parent container of each pod, responsible for creating and managing the environment for the group of containers that would be provisioned within the pod. For more info, I will direct you to an excellent article by <a href="https://twitter.com/IanMLewis">Ian Lewis</a>, the <a href="https://www.ianlewis.org/en/almighty-pause-container">almighty pause container</a>. I need to explain this because it will be shown in the following log.</p> </li> <li> <p>Definition of memory cgroups stats metrics as per <a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt">kernel.org</a> is listed below. </p> </li> </ol> <p>Note that, <code>anonymous memory</code> (abbreviated often as <code>anon</code>) is a memory mapping with no file or device backing it. Anon memory is used by programs to allocate memory for the stacks and heaps. Also, the standard page size on the Linux kernel is 4KB which can be really inefficient to store mapping for a large block of memory virtual memory. <code>Hugepages</code> are designed to solve this inefficiency and can hold a bigger chunk than 4KB. More details on this is available <a href="https://docs.openshift.com/container-platform/4.1/scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.html">here</a>. </p> <div class=highlight><pre><span></span><code>| Metrics of memory cgroups stats |                                                                                                        Definition                                                                                                        |
| ------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| rss                             | rss stands for resident set size. It is the portion of memory occupied by a process that is held in RAM. This metric represents the number of bytes of anonymous and swap cache memory (includes transparent hugepages). |
| rss_huge                        |                                                                                   number of bytes of anonymous transparent hugepages.                                                                                    |
| cache                           |                                                                                          number of bytes of page cache memory.                                                                                           |
| mapped_file                     |                                                                                number of bytes of the mapped file (includes tmpfs/shmem)                                                                                 |
| swap                            |                                                                                              number of bytes of swap usage                                                                                               |
| dirty                           |                                                                            number of bytes that are waiting to get written back to the disk.                                                                             |
| writeback                       |                                                                         number of bytes of file/anon cache that are queued for syncing to disk.                                                                          |
| inactive_anon                   |                                                                         number of bytes of anonymous and swap cache memory on inactive LRU list.                                                                         |
| active_anon                     |                                                                          number of bytes of anonymous and swap cache memory on active LRU list.                                                                          |
| inactive_file                   |                                                                               number of bytes of file-backed memory on inactive LRU list.                                                                                |
| active_file                     |                                                                                number of bytes of file-backed memory on active LRU list.                                                                                 |
| unevictable                     |                                                                            number of bytes of memory that cannot be reclaimed (mlocked etc).                                                                             |
</code></pre></div> <p>Now, as discussed previously, the swap is not being used in this system. See the second part of the logs in fig 13. You will note, there are two containers recorded and their memory stats is a capture - a) the pause container and b) the app container. We can ignore the pause, it's tiny and looking very healthy. But look at the stats for app pod in fig 13 (below)! At the time my app was killed, it held about 29GB in hugepages and only 1.3GB extra in RSS. That's huge and remember monitoring it not picking it for some reason! It captured 29GB but not 31GB! Perhaps its picking only <code>rss_huge</code> and presenting it as <code>rss</code> erroneously! <code>Â¯\_(ãƒ„)_/Â¯</code>! Yes, we have a problem but this monitoring issue is for another day!</p> <p><img alt src=../../../../../resources/oom/log-part-2.jpg> <em>Figure 13: Kernel log part 2</em></p> <p>Notice the blue arrow in fig 13, its capturing page info by both the pause container process and app container process. These are page info and not and need to be multiplied by 4KB to get actual memory stats. These are translated two lines below the blue line! </p> <p>My app has freaking <strong><em>62GB</em></strong> in total virtual memory! What's going on! <img alt src=../../../../../resources/oom/wth.gif></p> <p>Ok, so "total-vm" is the part of virtual memory the process uses. A part of this "total-vm" that's mapped to RAM is <code>rss</code>. Part of <code>rss</code> that's allocated on to real memory, blocks is your <code>anon-rss</code> (anonymous memory), and the other part of rss is mapped to devices and files and termed <code>file-rss</code>. If my app goes crazy and allocates a large chunk of space (say using malloc()) but never really use it then <code>total-vm</code> can be high but it won't all be used in real memory. This is made possible due to overcommit. A good sign of this happening, given swap off, is when <code>total-vm</code> is high but <code>rss</code> is actually low! This is exactly what's happening here! We have about <strong>30GB</strong> difference between <code>total-vm</code> and <code>rss</code>.</p> <p><code>Takeaway 7</code>: We have two problems here: a) Supporting over-commitment and b) Allocation of what we suspect un-needed memory! </p> <p>Let's look at solving the over-commit first and see what level of fixes it provides:</p> <h3 id=controlling-over-commits>Controlling over-commits<a class=headerlink href=#controlling-over-commits title="Permanent link">#</a></h3> <p>So far, we have concluded over-commitment is a problem. Well, as discussed previously, it's a feature (of both kernel &amp; kube) apparently!</p> <p><img alt= src="../../../../../assets/external/memecreator.org/static/images/memes/4777431.jpg"></p> <p>Kernel uses the "extendability" of virtual addressing to over-commit. The kernel settings <code>vm.overcommit_memory</code> and <code>vm.overcommit_ratio</code> is specially designed to controlling this capability. For more info, see <a href="https://engineering.pivotal.io/post/virtual_memory_settings_in_linux_-_the_problem_with_overcommit/">here</a>.</p> <p>1.1 <code>vm.overcommit_memory = 0</code>: Make best guess and overcommit where possible. This is the default.</p> <p>1.2 <code>vm.overcommit_memory = 1</code>: Always overcommit </p> <p>1.3 <code>vm.overcommit_memory = 2</code>: Never overcommit, and only allocate as much memory as defined in overcommit_ratio.</p> <p><code>vm.overcommit_ratio</code> is only used when overcommit_memory=2. It defines what percent of the physical RAM plus swap space should be allocated. This is default to 50. We want this config to be 100. </p> <p>But the use of <code>sysctl</code> to set these(using the following) is not enough as the config won't persist on horizontal scaling (new node spinning due to spot instances or less important but restart): <div class=highlight><pre><span></span><code>sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_memory<span class=o>=</span><span class=m>2</span>
sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_ratio<span class=o>=</span><span class=m>100</span>
</code></pre></div> The effect of these configs is immediate and no start is needed. Talking about the restart, <code>systcl</code> cli config update do not persist, system config needs to be updated in <code>/etc/sysctl.conf</code> to persist the setting across restarts. </p> <p>On <code>Kube</code>, <a href="https://github.com/kubernetes/kops/issues/3251">kops</a> provisioned clusters, these settings need to be supplied through <a href="https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md#sysctlparameters">sysctlparameters</a> config but these are only supported from kube 1.17 and higher! Safe <a href="https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/">sysctl parameters can be set at pod level</a> however our setting is not (obviously) supported at the pod level. One can't use <a href="https://github.com/kubernetes/kops/blob/master/docs/instance_groups.md#additionaluserdata">additionaluserdata</a> for this either, as these settings are overridden when kops provision node as Kube node!</p> <p>And, to make it a helluva fun, this cluster is currently at 1.12! Heya, Mr. Murphy!</p> <p><img alt src=../../../../../resources/oom/mrmurphy.jpg></p> <p>So, I say our my prayers, and turn to bash: <div class=highlight><pre><span></span><code><span class=k>for</span><span class=w> </span>memip<span class=w> </span><span class=k>in</span><span class=w> </span><span class=k>$(</span>aws<span class=w> </span>ec2<span class=w> </span>describe-instances<span class=w> </span>--region<span class=w> </span>us-east-1<span class=w> </span>--instance-ids<span class=w> </span><span class=se>\</span>
<span class=k>$(</span>aws<span class=w> </span>autoscaling<span class=w> </span>describe-auto-scaling-instances<span class=w> </span>--region<span class=w> </span>us-east-1<span class=w> </span>--output<span class=w> </span>text<span class=w> </span><span class=se>\</span>
--query<span class=w> </span><span class=s2>&quot;AutoScalingInstances[?AutoScalingGroupName==&#39;myasg&#39;].InstanceId&quot;</span><span class=k>)</span><span class=w> </span><span class=se>\</span>
--query<span class=w> </span><span class=s2>&quot;Reservations[].Instances[].PrivateIpAddress&quot;</span><span class=k>)</span>
<span class=k>do</span><span class=w>     </span>
<span class=w>    </span>ssh<span class=w> </span>-o<span class=w> </span><span class=nv>StrictHostKeyChecking</span><span class=o>=</span>no<span class=w>  </span><span class=si>${</span><span class=nv>memip</span><span class=si>}</span><span class=w> </span><span class=s1>&#39;bash -s&#39;</span><span class=w> </span>&lt;<span class=w> </span>set_mem.sh<span class=w>   </span>
<span class=k>done</span><span class=w> </span>
</code></pre></div> where <code>set_mem.sh</code> is: <div class=highlight><pre><span></span><code><span class=ch>#!/usr/bin/env bash</span>
sudo<span class=w> </span>sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_memory<span class=o>=</span><span class=m>2</span>
sudo<span class=w> </span>sysctl<span class=w> </span>-w<span class=w> </span>vm.overcommit_ratio<span class=o>=</span><span class=m>100</span><span class=w> </span>
</code></pre></div></p> <p>I see a massive improvement in OOMKills. Pods that were killed every 20mins and odd, are chugging along with 24hr processing and no crash still. <img alt src=../../../../../resources/oom/app-no-crash.jpg> <em>Figure 14: Getting somewhere! OOMKills sort of under control!</em></p> <p>So, perhaps we can upgrade Kube and make this configuration systematic! </p> <blockquote> <p>But, I am not done yet! No no no no no no no .....</p> </blockquote> <p>Remember, part <code>b</code> of our problem in <code>takeaway 7</code> i.e. <code>b) Allocation of what we suspect un-needed memory!</code>.</p> <p>Why was it happening in the first place, and why it's controlled with overcommit disabled. I won't lie, it still happens but far less infrequent! </p> <blockquote> <p>it's not fixed yet!</p> </blockquote> <p>Oh! the fun never ends! All the places we go! I will cover this later, ahem ahem, when I know the answer! Pretty sure it's some nasty behavior of Tensorflow 2, and the investigation is <em>underway</em>!</p> <p><img alt src=../../../../../resources/oom/4882580.jpg></p> <p>Thanks for reading. Hopefully, it was a fun insightful read!</p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="May 11, 2025 02:20:20">May 11, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3h-2Z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="May 11, 2025 02:20:20">May 11, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../../../2019/12/23/end-to-end-reproducible-machine-learning-pipelines-on-kubernetes/ class="md-footer__link md-footer__link--prev" aria-label="Previous: End-to-end reproducible Machine Learning pipelines on Kubernetes"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> End-to-end reproducible Machine Learning pipelines on Kubernetes </div> </div> </a> <a href=../../../12/31/data-in-deep-learning/ class="md-footer__link md-footer__link--next" aria-label="Next: Data in Deep Learning"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Data in Deep Learning </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> </div> <div class=md-social> <a href="https://www.linkedin.com/in/suneeta-mall-a6a0507/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href="https://github.com/suneeta-mall" target="_blank" rel="noopener" title="github.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href="https://x.com/suneetamall/" target="_blank" rel="noopener" title="x.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9L389.2 48zm-24.8 373.8h39.1L151.1 88h-42l255.3 333.8z"/></svg> </a> <a href="https://www.medium.com/@suneetamall" target="_blank" rel="noopener" title="www.medium.com" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg> </a> <a href="https://scholar.google.com.au/citations?hl=en&amp;user=WD712CUAAAAJ" target="_blank" rel="noopener" title="scholar.google.com.au" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg> </a> <a href="https://www.researchgate.net/profile/Suneeta_Mall3" target="_blank" rel="noopener" title="www.researchgate.net" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6z"/></svg> </a> <a href="https://suneeta-mall.github.io/feed_rss_created.xml" target="_blank" rel="noopener" title="suneeta-mall.github.io" class="md-social__link"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64zm0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0zm32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32z"/></svg> </a> <a href=mailto:suneetamall@gmail.com target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l217.6 163.2c11.4 8.5 27 8.5 38.4 0l217.6-163.2c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48H48zM0 176v208c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V176L294.4 339.2a63.9 63.9 0 0 1-76.8 0L0 176z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "content.tabs.link", "navigation.indexes", "navigation.instant", "navigation.instant.preview", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "toc.follow", "header.autohide", "announce.dismiss", "navigation.footer", "navigation.breadcrumbs", "navigation.expand", "navigation.sections", "navigation.tracking", "navigation.top", "search.highlight", "search.share", "toc.follow", "toc.integrate", {"git-revision-date-localized": {"enable_creation_date": true, "type": "date"}}], "search": "../../../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src=../../../../../assets/javascripts/bundle.3220b9d7.min.js></script> </body> </html>